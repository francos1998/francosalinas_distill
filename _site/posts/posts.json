[
  {
    "path": "posts/2025-12-08-predicting-the-probability-and-cost-of-accidents-in-austin/",
    "title": "Predicting the Probability and Cost of Accidents in Austin",
    "description": "A spatiotemporal machine learning model for crash prediction and severity analysis.",
    "author": [
      {
        "name": "Franco Salinas, Shyam Patel, Ishrak Wasif Udoy, Sanchal Nachappa, Muzaffar Yezdan",
        "url": {}
      }
    ],
    "date": "2025-12-08",
    "categories": [],
    "contents": "\nABSTRACT\nTraffic crashes in Austin impose a significant human and economic burden. According to the Texas Department of Transportation, in 2024 a person was killed every 2 hours and 7 minutes, a person was injured every 2 minutes and 5 seconds, and a reportable crash occurred every 57 seconds. A separate study estimated that motor-vehicle crashes cost Austin taxpayers over $35 million annually, funds that could otherwise support infrastructure, public health, and safety initiatives.\nThis project aims to develop a spatio-temporal crash prediction system for Austin using machine learning. We construct a unified pipeline that aggregates crash data into H3 hexagonal cells and 1-hour temporal bins, enriches them with lagged counts, rolling statistics, weather attributes, and contextual features, and prepares them for predictive models such as Light Gradient Boosting Machines (GBMs), XG Boost and Hurdle Model. Preliminary exploration shows strong spatial clustering along major corridors and temporal patterns driven by rush hours, weather, and weekend activity. Our processed dataset and EDA provide the foundation for downstream modeling and risk-surface generation.\n\n\nVariable of importance plot6.Conclusion:\n6.1 Project Summary:\nThis project develops a spatiotemporal risk-prediction system that estimates both the likelihood of a traffic crash and the potential severity of that crash across the City of Austin. Because crashes are extremely rare, occurring in only about 0.2% of hex–hour observations, the modeling challenge centers on handling extreme class imbalance, sparse count data, and highly skewed crash-cost outcomes. To address these issues, the team implemented two complementary approaches: a Binary LightGBM classifier that predicts crash probability, and a modeling framework combining Tweedie XGBoost with a two-stage Hurdle Model to explore expected crash counts and costs. Together, these models create a dynamic risk surface suitable for applications in insurance pricing, public-safety planning, and rideshare risk management.\nThe Binary LightGBM model estimates the probability of a crash occurring within each hexagon–timebin by leveraging thirty-two pre-crash environmental variables. These include temporal patterns such as hour, weekday, and seasonality; weather conditions such as temperature and rainfall; roadway characteristics including speed limit and roadway type; and a series of lagged crash-history indicators. The modeling pipeline uses time-ordered splits to avoid leakage and employs a balanced negative-sampling strategy during training. SHAP analysis confirms that the model captures intuitive risk patterns: probability peaks during daytime and evening hours, increases substantially on high-speed corridors, rises during rainfall, and is somewhat higher on weekends. The hotspot maps in the slides show that the model highlights historically dangerous corridors like I-35 and central Austin arterials, but it also flags locations where current conditions resemble those that have historically led to crashes. Performance results demonstrate a strong ability to rank risk, with the top five percent of predicted high-risk bins capturing crash concentrations roughly 1.76 times higher than baseline. ROC-AUC values around 0.65 are consistent with published traffic-safety benchmarks.\nCrash severity is explored through a Tweedie XGBoost model designed for non-negative, highly skewed cost data. Crash costs in the dataset range from approximately twenty thousand dollars to nearly four million, which produces a fat-tailed distribution that is impossible to predict precisely from environmental variables alone. Instead, the model is used to study severity tendencies. SHAP results identify conditions linked to higher expected severity, such as elevated speed limits, freeway environments, rainfall, and warmer temperatures. Severity hotspot maps show that conditions associated with expensive crashes cluster around major corridors like I-35 and US-183. Although the model’s RMSE of about $600k appears large, it is close to the true standard deviation of crash cost, indicating that the model captures broad patterns while avoiding data leakage. Its primary value lies in revealing environmental conditions associated with more severe outcomes rather than producing exact dollar estimates.\nTo address the near-zero crash count structure, the project also employs a Hurdle Model. The first stage predicts whether any crash occurs, using the same LightGBM classifier architecture. The second stage fits a Poisson LightGBM regressor to the subset of rows where a crash has occurred. Because almost all non-zero counts equal one, the Poisson model inevitably tends to overpredict, and the multiplicative combination of the two stages often produces higher expected counts than are observed. Visual comparisons in the slides confirm this behavior. This mismatch is expected given the limited variation in the data, and it reinforces that the crash-occurrence model, rather than the count model, carries the real predictive value by identifying where and when crashes are most likely.\n6.2 Lessons Learned:\nTaken together, these models reveal several broader insights about crash risk in Austin. Dangerous locations are not defined solely by historical crash totals; risk increases sharply when current conditions mirror those present during past hazardous periods. Infrastructure characteristics, particularly speed environments, exert a persistent influence even during otherwise low-risk moments. Temporal patterns show that nightlife areas become safer after midnight, contradicting common assumptions. More broadly, the results demonstrate that crash risk in Austin is driven by changing conditions rather than static characteristics. The system created in this project delivers meaningful predictive lift, interpretable patterns, and a practical foundation for future work such as incorporating real-time traffic data, integrating major-event information, and developing interactive risk-mapping tools.\n6.3 Future Work:\nFuture work will expand the crash-risk system with more dynamic and high-resolution modeling approaches. One promising direction is the use of Graph Neural Networks, which can represent Austin’s road network as an interconnected structure and capture how risk propagates across adjacent segments. Recent research by Wu et al. (2024) demonstrates that GNNs can effectively model zero-inflated crash data and improve spatial accident prediction. Another potential improvement involves incorporating a Hawkes-style framework to account for the self-exciting behavior of crashes. As shown by Mohler and Short (2018), traffic accidents tend to cluster in time and space, and self-exciting point-process models provide an effective way to quantify how an incident can temporarily increase the likelihood of another occurring nearby. The system may also benefit from integrating more granular weather information and traffic-flow simulations, which would support richer scenario testing and create a more adaptive platform for short-term crash-risk forecasting.\n6.4 Business Applications:\nThe crash-risk surface developed in this project has clear business and policy relevance across multiple domains. Dynamic insurance pricing could use these risk estimates to adjust premiums in real time or near real time, offering lower rates in safer conditions and applying appropriate surcharges during periods or in locations where crash likelihood increases. Rideshare platforms could also leverage this information to refine surge pricing, compensate drivers more fairly for operating in high-risk areas, and proactively reroute drivers away from hazardous conditions. From a policy perspective, the risk surface provides city planners and transportation agencies with an evidence-based tool for identifying dangerous corridors, prioritizing infrastructure investments, and evaluating the impact of interventions such as speed-limit changes, improved lighting, or targeted enforcement. Overall, the system serves as a bridge between data-driven risk modeling and practical decision-making for businesses, public agencies, and urban safety stakeholders.\n6.5 Business Applications:\nThe crash-risk surface developed in this project has clear business and policy relevance across multiple domains. Dynamic insurance pricing could use these risk estimates to adjust premiums in real time or near real time, offering lower rates in safer conditions and applying appropriate surcharges during periods or in locations where crash likelihood increases. Rideshare platforms could also leverage this information to refine surge pricing, compensate drivers more fairly for operating in high-risk areas, and proactively reroute drivers away from hazardous conditions. From a policy perspective, the risk surface provides city planners and transportation agencies with an evidence-based tool for identifying dangerous corridors, prioritizing infrastructure investments, and evaluating the impact of interventions such as speed-limit changes, improved lighting, or targeted enforcement. Overall, the system serves as a bridge between data-driven risk modeling and practical decision-making for businesses, public agencies, and urban safety stakeholders.\n6.6 Sources:\nA novel Bayesian hierarchical model for road safety: predicting crash counts in future years (Fawcett, 2017) — uses a Bayesian hierarchical model for crash prediction and shows how uncertainty can be accommodated. ScienceDirect\nA Poisson-Lognormal Conditional-Autoregressive Model for Multivariate Pedestrian Crash Counts (Wang et al., 2013) — applies a CAR spatial model (conditional autoregressive) to crash counts.\n“Application of Poisson Regression on Traffic Safety” (Strandroth et al., 2012) – via “Application of Poisson Regression on Traffic Safety – DiVA portal”. DIVA Portal\nData Science in Transportation Networks with Graph Neural Networks\nhttps://link.springer.com/article/10.1007/s42421-025-00124-6\nEstimating crash costs to Austin taxpayers. https://services.austintexas.gov/edims/document.cfm?id=440986\nGraph Neural Networks for Road Safety Modeling: Datasets and Evaluations for Accident Analysis (Nippani et al., 2023) https://proceedings.neurips.cc/paper_files/paper/2023/file/a365be0950259c9624edfb4d26eabd46-Paper-Datasets_and_Benchmarks.pdf\nMohler, G., & Short, M. B. (2018). Traffic Accident Modelling via Self-Exciting Point Processes. Reliability Engineering & System Safety, 178, 8–16.\nQuantifying Road Network Structure and its Impact on Traffic Crashes. (SSRN) – shows how road-network metrics (connectivity, intersection density) relate to crash risk. SSRN\nSpatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting (Yu et al., IJCAI-18) — introduces STGCN for traffic forecasting via graph convolution on road networks. IJCAI\nTexas Motor Vehicle Traffic Crash Facts Calendar Year 2024. https://www.txdot.gov/content/dam/docs/division/trf/crash-records/2024/01.pdf\nTraffic Accident Prediction using Graph Neural Networks – New Datasets and the TRAVEL Model (2022) — applies GNNs for traffic-accident prediction using road network structure. graph-learning-benchmarks.github.io\nWu, X., Zhou, B., Wu, Y., Han, Y., & Song, X. (2024). Uncertainty-Aware Probabilistic Graph Neural Networks for Road-Level Traffic Accident Prediction. Accident Analysis & Prevention.\nYe, X., Wang, K., Zou, Y., Lord, D. (2018). A semi-nonparametric Poisson regression model for analyzing motor vehicle crash data collected from rural multilane highway segments in California, U.S. PLoS ONE 13(5): e0197338. PLOS+1\n\n\n\n",
    "preview": {},
    "last_modified": "2025-12-08T15:10:00-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-06-16-gibbssampling/",
    "title": "Gibbs Sampling",
    "description": "Literature review and explanation of concepts and computational methods related to Gibbs Sampling.",
    "author": [
      {
        "name": "Ty Bruckner, Franco Salinas",
        "url": {}
      }
    ],
    "date": "2022-06-16",
    "categories": [],
    "contents": "\n\n\n\nIntroduction\nUsing Bayesian statistics we can incorporate prior knowledge into the\nestimation of our unknown parameters. In practice we incorporate our\nprior knowledge of the unknown parameter through a prior distribution.\nThen, we update our beliefs about \\(\\theta\\) with observed data and we end up\nwith what we call a posterior distribution. Furthermore, we use the\nposterior to estimate the parameters of interest. There are different\nmethods for when we can’t sample from the posterior. This is when we use\nMarkov Chain Monte Carlo (MCMC) techniques that approximate our target\nposterior. Gibbs sampling is a type of MCMC that uses conditional\ndistributions to approximate a joint posterior distribution with more\nthan one unknown parameter. The fact that we work with conditional\ndistributions makes this method an alternative to Metropolis-Hastings.\nIn a nutshell, Gibbs sampling is a method that samples from separate\nconditional distributions and is most useful when the joint posterior\ndistribution is unknown or hard to sample from. Like in a standard\nMarkov Chain, each event is dependent on the last event; and it is only\ndependent on the last event.\nMotivation\nGibbs sampling is a useful algorithm for Bayesian estimation. We\nthink that incorporating prior knowledge (priors) and data in the\nestimation of a parameter can be useful in different fields including\nquantitative finance and biostatistics. Applications in biostatistics\nthat will be further developed in this paper involve the inference of a\nperson’s population of origin, applications in finance, involve\nmodelling the distribution of risk for a financial project by\nincorporating all of the random variables that may affect the\nperformance of a financial project. Some people might be familiar with\nthe Metropolis-Hastings algorithm, however as we will explain explain\nlater Gibbs is a suitable alternative to Metropolis-Hastings depending\non the information one has available.\nBackground Knowledge\nBayesian Statistics\nBayesian Statistics is a different philosophy of statistics. Most\nstatistical modeling that people use are under the Frequentist school of\nthought. Frequentist methods depend only on the observed data. This\nmeans that interpretations and inference can only be from the sample\nthat was studied or collected. Bayesian methods incorporate prior\nbeliefs into the model before the data is received.\nExample 1\nWe have prior information regarding an upcoming election. This first\nexample shows how our priors affect posteriors in an election. Below we\nshow for different priors for candidates. Aaron will be represented by\nalpha and Brynn will be represented by beta. We parameterize our priors\nin a beta distribution \\(Beta(\\alpha,\\beta)\\) between [0,1]. Beta\ndistributions are used to represent probabilities. Prior: A prior is a\nprobability distribution that shows our beliefs before any data is taken\ninto account. A common distribution used for a prior is a beta model.\nPriors incorporate both the mean and the variance of your previous\nbeliefs about the data. Priors with a greater spread indicate a less\nconfident belief.\nPrior\nA prior is a probability distribution that shows our beliefs before\nany data is taken into account. A common distribution used for a prior\nis a beta model. Priors incorporate both the mean and the variance of\nyour previous beliefs about the data. Priors with a greater spread\nindicate a less confident belief.\nFirst Prior:\n\\(Beta(1,1)\\).This is the same as\nthe uniform distribution between [0,1]. We have no prior beliefs about\nhow the election will take place. This is not realistic as it is always\nunlikely that someone will receive all of the votes or none of the\nvotes.\n\n\n\nSecond Prior:\n\\(Beta(10,10)\\). In this case we\nbelieve that the election is close. The mean is centered at .5\nprobability.\n\n\n\nThird Prior:\n\\(Beta(7,4)\\). This prior favors\nAaron. The mean probability is .64, with the mode at .67. This means\nthat we have prior beliefs that Aaron will win the election.\n\n\n\nFourth Prior:\n\\(Beta(4,7)\\). This prior favors\nBrynn. The mean probability is .36, with the mode at .33. This means\nthat we have prior beliefs that Brynn will win the election.\n\n\n\nLikelihood:\nThe likelihood function is the distribution of the data. In this\nexample the outcome variable is binary on whether the person was voted\nfor or not. So we have a binomial distribution of the likelihood. The\ngraph below plots the density distribution of the binomial outcome.\nAaron received 20 out of 30 votes in the exit survey we conducted \\(Y\\mid \\pi \\sim Bin(30,\\pi)\\) where \\(\\pi = 0.67\\) given that he receive 20 out\nof 30 votes.\n\n\n\nPosterior:\nThe posterior is simply a combination of the prior and data. In this\ncase we conveniently chose conjugate distribution in which the prior and\nposterior share the same distribution. We combine our data with our\nprior model to form the posterior distribution. As more data is\nincorporated more weight shifts to the data from the prior.\nNext we will incorporate the same data into their respective\ndistribution and graph them. We will now use the exit survey to update\nour beliefs on the final result of the election.\nFirst Posterior:\nSince our prior is equivalent to a uniform distribution, our\nposterior is nearly identical to our data.\n\n\n\nSecond Posterior:\nOur data is favorable to Aaron in comparison to our priors, so the\nposterior falls between our data and our centered prior.\n\n\n\nThird Posterior:\nOur data reinforces our prior that Aaron has around a .67 probability\nof winning.\n\n\n\nFourth Posterior:\nThis represents our largest shift. Our prior favored Brynn, but with\nthe significantly different data the posterior shifted towards\nAaron.\n\n\n\nAs these graphs illustrate, a change in our prior beliefs will affect\nthe posterior, even with the same data.\nMonte Carlo\nMonte Carlo simulations randomly sample points within a region to\napproximate a distribution. The example below is a simple illustration\nof a uniform distribution for an estimate for \\(\\pi\\). This samples the proportion of\npoints within the square region that fall within the circle’s bounds.\nThe proportion would be equal to \\(\\frac{\\pi}{4}\\) since we are only\ninterested in one fourth of the circle. As we sample more, our\nestimation for \\(\\pi\\) gets closer to\nthe actual distribution. This is due to the Central Limit Theorem. Monte\nCarlo simulations work well when the posterior distribution is easy to\nsample from. However, it is not always possible to sample from the\nposterior distribution, nor is it always efficient.\nan image caption Source: From Towards\nData Science an Overview of Monte CarloThis image illustrates a Monte Carlo simulation of 1/4 of circle to\nsample for \\(\\pi\\) (Pease 2018). It is important to note\nsome of the key qualities in this simulation. As n increases, the more\naccurate our simulation becomes. It is also important that the area that\nwe are sampling from is known and easy to sample from. One reason we use\nMCMC sampling is because our distribution is hard to sample from.\nMarkov Chains\nMarkov Chains are an example of a random walk. Random walks are a\nseries of random moves through space in succession. Random walks use a\ncombination of past events in the probability to determine the next\nstep. Markov Chains are a special case in which only the previous\nstep/location is used to determine the probability distribution of the\nnext step. The following notation represents this process \\(P(X_{n+1} = x | X_n = x_n)\\) meaning the\nprobability distribution of move n+1 is only conditioned on the result\nof the previous move n. It is important to talk about the fact that\nMarkov Chains are dependent on the previous move and are not an\nindependent event.\nGibbs Sampling Overview\nGibbs Sampling is a specific type of MCMC sampling that is used when\nit is hard to sample from the joint Probability Distribtutin Function\n(PDF) or Probability Mass Function (PMF) or when the joint PDF (or PMF)\nis unknown. To perform Gibbs sampling you must know the conditional\ndistributions of both variables.\nMarkov Chains Monte Carlo\n(MCMC)\nMCMC is the application of Markov Chains to simulate probability\nmodels. Two important characteristics are that MCMC samples aren’t taken\nfrom the posterior pdf and that the samples aren’t independent. The fact\nthat the samples aren’t independent reflects the “chain” feature of the\nalgorithm. For example in the \\(N-length\\) MCMC sample ( Markov chain)\n\\(\\{\\theta^{(1)},\\theta^{(2)},...,\\theta^{(N)}\\}\\),\nwhen constructing the chain, \\(\\theta^{(2)}\\) is drawn from some model\nthat depends upon \\(\\theta^{(1)}\\),\n\\(\\theta^{(3)}\\) is drawn from some\nmodel that depends on \\(\\theta^{(2)}\\)\nand so on.\nWe can say that the (i+1)st chain value \\(\\theta^{(i+1)}\\) has a conditional PDF\n\\(f(\\theta^{(i+1)}|\\theta^{(i)},y)\\) is\ndrawn from a model that depends on data y and the previous chain value\n\\(\\theta^{(i)}\\). It’s important to\nnote that by the Markov property, \\(\\theta^{(i+1)}\\) depends on the preceding\nchain values only through \\(\\theta^{(i)}\\), the most recent value. The\nonly information we need to simulate \\(\\theta^{(i+1)}\\) is the value of \\(\\theta^{(i)}\\). Therefore, each value can\nbe sampled from a different model, and none of these models are the\ntarget posterior. The pdf from which a Markov Chain value is simulated\nis not equivalent to the posterior pdf.\n\\[f(\\theta^{(i+1)}|\\theta^{(i)}, y)\\ne\nf(\\theta^{(i+1)}| y)\\]\nWe will conduct the MCMC simulation using the rstan package (Guo and Weber 2020). There are two\nessential steps to all RSTAN analyses, first we define the Bayesian\nmodel structure and then simulate the posterior. It’s important to note\nthat RSTAN doesn’t use Gibbs sampling, it uses a Hamiltonian algorithm,\nwe are using this package to show how MCMC computation works. We will\nuse a generic Beta-Binomial example:\n\\[Y\\mid \\pi \\sim Bin(10,\\pi)\\]\n\\[\\pi \\sim Beta(2,2)\\]\nWhere Y is the number of successes in 10 independent trials. Each\ntrial has a probability of success \\(\\pi\\) where our prior for \\(\\pi\\) is captured by a \\(Beta(2,2)\\) model. If we observe 9\nsuccesses we have an updated posterior model of \\(\\pi\\) with distribution \\(Beta(11,3)\\). Don’t worry about how we\nfound this answer for the moment being. Our goal is to run an MCMC\nalgorithm to produce an approximate sample from the Beta-binomial\nposterior.\nSTEP 1: DEFINE the model\nData: Y is the observed number of success trials. We specify\nthat Y is between 10 and 0. Parameters: The model depends on\n\\(\\pi\\), therefore we must specify that\n\\(\\pi\\) can be any real number from 0\nto 1. Model: We need to specify the model for the data and the\nmodel for the prior.\n\n\n# STEP 1: DEFINE the model\nbb_model <- \"\n  data {\n    int<lower = 0, upper = 10> Y;\n  }\n  parameters {\n    real<lower = 0, upper = 1> pi;\n  }\n  model {\n    Y ~ binomial(10, pi);\n    pi ~ beta(2, 2);\n  }\n\"\n\n\n\nSTEP 2: Simulate the\nposterior\nWe simulate the posterior using the stan() function. This function\ndesigns and runs an MCMC algorithm to produce an approximate sample from\nthe Beta-Binomial posterior. The model code argument requires a string\nthat defines the model. The data argument requires a list of observed\ndata. The chains argument specifies how many parallel Markov Chains we\nare running. Since we are running four chains we will have four \\(\\pi\\) values. The “iter’ argument specifies\nthe number of iterations or length for each chain. The first half of\nthis iterations are thrown out as”burn in” samples (samples that we use\nto calibrate our model). To keep our random results constant we utilize\nthe seed argument within the stan() function.\n\n\n\nAs you can see in the figure below, when observing the distribution\nof the sampled \\(\\pi\\) values we\napproximate the target Beta(11,3) posterior model of \\(\\pi\\). The target pdf is superimposed to it\n(Alicia A. Johnson 2022).\n\n\n\nMetropolis-Hastings\nalgorithm\nIf we weren’t able to recognize the posterior model of \\(\\mu\\) in a Normal-Normal model, we could\napproximate it using the MCMC simulation. Metropolis-Hastings algorithm\nhelps automate the decision of what values of \\(\\mu\\) to sample and with what frequency.\nThis algorithm iterates through a two step process. If we are in the\nlocation \\(\\mu^{(i)} = \\mu\\) we select\nthe next value to sample first by proposing a random location \\(\\mu^{\\prime}\\) and then we decide whether\nto stay at the current location or to stay at the current location \\(\\mu^{(i+1)} = \\mu\\).\nThere are special cases of the Metropolis-Hastings that involve a\ndifferent sampling decision criteria such as Gibbs sampling,\nHamiltonian, the Monte Carlo and the Metropolis algorithms. In this\nreport we will be focusing on the Gibbs Sampling algorithm.\nGibbs Sampling\nExample 1: Bernoulli\nDistribution:\nWe use Gibbs sampling to approximate a posterior for a joint\ndistribution. As we will demonstrate later, we reduce our problem to\ncalculations that only involve one parameter at the time. This is more\nefficient than trying to find the real posterior. We start with an\nexample of two random variables \\(x,\ny\\) with a Bernoulli distribution (Lambert 2018). We now find their\nconditional distributions. This process is a simplified version of what\nis done in all forms of Gibbs Sampling. By understanding the steps\nbelow, one will be able to understand the concepts behind more complex\ndistributions.\n\\[P(x|y = 0) \\in P(x=1) = \\frac{4}{5}\n\\space , P(x = 0) = \\frac{1}{5} \\]\n\\[P(x|y = 1) \\in P(x=1) = \\frac{2}{5}\n\\space , P(x = 0) = \\frac{3}{5} \\] \\[P(y|x = 0) \\in P(y=1) = \\frac{3}{4} \\space , P(y\n= 0) = \\frac{1}{4} \\]\n\\[P(y|x = 1) \\in P(y=1) = \\frac{1}{3}\n\\space , P(y = 0) = \\frac{2}{3} \\]\nPick specific starting value of \\((x_0,y_0)\\) Here we pick \\((x=0),(y=0)\\)\nCondition on \\(y_0\\)\nYour distribution is now \\(P(x=0) =\n1/5\\) and $ P(x= 1) = 4/5$. This is based on the conditional\ndistribution for \\(x=0\\).\nRandomly sample\nYour random sample leads to \\(x=1\\)\nCondition on \\(x_1\\)\nYour distribution is now \\(P(y=0) =\n2/3\\) and $ P(y=1) = 1/3$. This is based on the conditional\ndistribution of \\(x=1\\).\nRandomly sample\nYour random sample leads to $y=1 $\nCondition on \\(y_1\\) Now the\nprocess repeats thousands of times until and each move is recorded. This\nalgorithm then approximates well the true probability distribution after\nthousand of trials. More trials will lead to a better\napproximation.\nExample 2: Normal\nDistribution\nNow suppose we have data from a normal distribution where both the\nmean and variance are unknown. For convenience, we’ll\nparameterize this model in terms of the precision \\(\\gamma = \\frac{1}{\\sigma^2}\\) instead of\nthe variance \\(\\sigma^2\\).\n\\[Y \\mid \\mu, \\gamma \\sim N\\left(\\mu,\n\\frac{1}{\\gamma}\\right)\\]\nSuppose we put the following independent priors on the mean\n\\(\\mu\\) and precision \\(\\gamma\\):\n\\[\\mu \\sim N(m, v)\\]\n\\[\\gamma \\sim\n\\text{Gamma}(a,b)\\]\nWe can start writing down the joint posterior distribution for \\(\\mu, \\gamma\\).\n\\[\n\\begin{aligned}\ng(\\mu,\\gamma \\mid y) & \\propto f(y \\mid \\mu, \\gamma) f(\\mu, \\gamma)\n\\\\\n& = f(y \\mid \\mu, \\gamma) f(\\mu) f(\\gamma), \\text{ since } \\mu,\n\\gamma \\text{ independent} \\\\\n& =\n\\left[(2\\pi)^{-\\frac{1}{2}}\\gamma^{\\frac{1}{2}}e^{-\\frac{1}{2}\\gamma(y -\n\\mu)^2} \\right] \\left[(2\\pi v)^{-\\frac{1}{2}}e^{-\\frac{1}{2v}(\\mu -\nm)^2} \\right]\\left[\\frac{b^a}{\\Gamma(a)}\n\\gamma^{a-1}e^{-b\\gamma}\\right]\\\\\n& \\propto \\gamma^{\\frac{1}{2}}e^{-\\frac{1}{2}\\gamma(y -\n\\mu)^2}e^{-\\frac{1}{2v}(\\mu - m)^2}\\gamma^{a-1}e^{-b\\gamma}\\\\\n& =\\gamma^{\\frac{1}{2} + a - 1}e^{-\\frac{1}{2}\\gamma(y - \\mu)^2 +\n-\\frac{1}{2v}(\\mu - m)^2 -b\\gamma} \\\\\n& = \\gamma^{\\frac{1}{2} + a - 1}e^{-\\frac{1}{2}\\left[\\gamma(y -\n\\mu)^2 + \\frac{1}{v}(\\mu - m)^2 + 2b\\gamma\\right]} \\\\\n& = \\gamma^{\\frac{1}{2} + a - 1}e^{-\\frac{1}{2}\\left[\\gamma y^2 -\n2\\mu y \\gamma + \\gamma \\mu^2 + \\mu^2 / v - 2m \\mu / v + m^2 / v + 2 b\n\\gamma\\right]} \\\\\n& = \\gamma^{\\frac{1}{2} + a - 1}e^{-\\frac{1}{2}\\left[\\gamma (y^2 +\n2b) - 2\\mu (y \\gamma + m/v) + \\mu^2(\\gamma  + 1 / v)  + m^2 / v \\right]}\n\\\\\n& \\propto \\gamma^{\\frac{1}{2}+a-1}e^{-\\frac{1}{2}\\left[\\gamma(y^2 +\n2b) - 2\\mu(y\\gamma + \\frac{m}{v}) + \\mu^2(\\frac{1}{v}+ \\gamma) \\right]}\n\\\\\n\\end{aligned}\n\\]\nWe can see this doesn’t look like a recognizable probability\ndistribution. Therefore, we can’t use our usual techniques here to find\nBayes estimators for \\(\\mu\\) or \\(\\gamma\\) since we don’t have a recognizable\nposterior distribution. Instead, we’ll use Gibbs Sampling to\ngenerate samples from this posterior distribution. In order to perform\nGibbs Sampling, we need to find the conditional distributions \\[g(\\mu \\mid y, \\gamma) \\propto f(y \\mid \\mu,\n\\gamma)f(\\mu)\\] \\[g(\\gamma \\mid y,\n\\mu) \\propto f(y \\mid \\mu,\\gamma)f(\\gamma)\\].\nWe will use these conditional distributions to sample from the joint\nposterior \\(g(\\mu, \\gamma \\mid y)\\)\naccording to the following algorithm:\n\nStart with initial values \\(\\mu^{(0)},\n\\gamma^{(0)}\\).\nSample \\(\\mu^{(t+1)} \\sim g(\\mu \\mid y,\n\\gamma = \\gamma^{(t)})\\).\nSample \\(\\gamma^{(t+1)} \\sim g(\\gamma \\mid\ny, \\mu = \\mu^{(t+1)})\\).\nRepeat many times. It turns out that the resulting \\(\\mu^{(0)}, \\mu^{(1)}, \\dots, \\mu^{(N)}\\)\nand \\(\\gamma^{(0)}, \\gamma^{(1)}, \\dots,\n\\gamma^{(N)}\\) are samples from the joint posterior distribution\n\\(g(\\mu, \\gamma \\mid Y)\\), and we can\nuse these sampled values to estimate quantities such as the posterior\nmean of each parameter \\(\\hat{E}(\\mu \\mid y) =\n\\frac{1}{N}\\sum_{i=1}^N \\mu^{(i)}, \\ \\ \\hat{E}(\\gamma \\mid y) =\n\\frac{1}{N}\\sum_{i=1}^N \\gamma^{(i)}\\). Note that in practice we\ntypically remove the initial iterations, known as the “burn-in” period:\ne.g., \\(\\hat{E}(\\mu \\mid y) =\n\\frac{1}{N-B}\\sum_{i=B}^N \\mu^{(i)}\\).\n\nTo use this conditional distributions, first we need to show that the\nconditional distributions \\(g(\\mu \\mid y,\n\\gamma), g(\\gamma \\mid y, \\mu)\\) are proportional to \\(f(y \\mid \\mu, \\gamma)f(\\mu), f(y \\mid\n\\mu,\\gamma)f(\\gamma)\\), respectively, as stated above.\n\\[\n\\begin{aligned}\ng(\\mu \\mid y, \\gamma) &= \\frac{f(\\mu, y, \\gamma)}{f(y, \\gamma)} \\\\\n& \\propto f(\\mu, y, \\gamma), \\text{ since } f(y, \\gamma) \\text{\ndoesn't depend on } \\mu \\\\\n& = f(y \\mid \\mu, \\gamma) f(\\mu, \\gamma) \\\\\n& = f(y \\mid \\mu, \\gamma) f(\\mu) f(\\gamma), \\text{ since } \\mu,\n\\gamma \\text{ independent} \\\\\n& \\propto f(y \\mid \\mu, \\gamma) f(\\mu), \\text{ since} f(\\gamma)\n\\text{ doesn't depend on } \\mu\n\\end{aligned}\n\\]\nA similar argument can be used to show \\(g(\\gamma \\mid y, \\mu) \\propto f(y | \\mu, \\gamma)\nf(\\gamma)\\).\nNext we can use this result to show that \\(\\mu \\mid y, \\gamma \\sim N\\left(\\frac{y\\gamma +\n\\frac{m}{v}}{\\gamma + \\frac{1}{v}}, \\left[\\gamma + \\frac{1}{v}\n\\right]^{-1}\\right)\\) and \\(\\gamma \\mid\ny, \\mu \\sim \\text{Gamma}\\left(\\frac{1}{2} + a, \\frac{1}{2}(y-\\mu)^2 +\nb\\right)\\).\n\\[g(\\mu \\mid y, \\gamma) \\propto f(y \\mid\n\\mu, \\gamma)f(\\mu)\\]\n\\[=\n\\left[(2\\pi)^{-\\frac{1}{2}}\\gamma^{\\frac{1}{2}}e^{-\\frac{1}{2}\\gamma(y -\n\\mu)^2} \\right] \\left[(2\\pi v)^{-\\frac{1}{2}}e^{-\\frac{1}{2v}(\\mu -\nm)^2} \\right]\\] \\[ \\propto\ne^{-\\frac{1}{2}\\gamma(y - \\mu)^2 -\\frac{1}{2v}(\\mu - m)^2}\\]\n\\[ = e^{-\\frac{1}{2}\\gamma(y^2 - 2\\mu y +\n\\mu^2) -\\frac{1}{2v}(\\mu^2 - 2\\mu m + m^2)}\\] \\[ \\propto e^{-\\frac{1}{2}\\gamma(- 2\\mu y + \\mu^2)\n-\\frac{1}{2v}(\\mu^2 - 2\\mu m)} \\] \\[\n\\propto e^{-\\frac{1}{2}[\\gamma(- 2\\mu y + \\mu^2) +\\frac{1}{v}(\\mu^2 -\n2\\mu m)]}\\] \\[ = e^{-\\frac{1}{2}[-\n2\\gamma\\mu y + \\gamma\\mu^2 +\\frac{1}{v}\\mu^2 - \\frac{1}{v}2\\mu\nm)]}\\] \\[=\ne^{-\\frac{1}{2}\\left[\\mu^2(\\gamma + \\frac{1}{v}) - 2\\mu(y \\gamma +\n\\frac{m}{v})  \\right]} \\] \\[=\ne^{-\\frac{1}{2}(\\gamma + \\frac{1}{v})\\left[\\mu^2 - 2\\mu \\left(\\frac{y\n\\gamma + \\frac{m}{v}}{\\gamma + \\frac{1}{v}}\\right)  \\right]} \\]\n\\[\\propto e^{-\\frac{1}{2}(\\gamma +\n\\frac{1}{v})\\left[\\mu^2 - 2\\mu\\left(\\frac{y \\gamma + \\frac{m}{v}}{\\gamma\n+ \\frac{1}{v}}\\right) + \\left(\\frac{y \\gamma + \\frac{m}{v}}{\\gamma +\n\\frac{1}{v}}\\right)^2 \\right]} \\] \\[\\text{because of inverse proportionality} \\propto\ne^{-\\frac{1}{2\\left(\\gamma + \\frac{1}{v}\\right)^{-1}}\\left[ \\mu -\n\\left(\\frac{y \\gamma + \\frac{m}{v}}{\\gamma + \\frac{1}{v}}\\right)\n\\right]^2}\\]\n\\[\\implies \\mu \\mid y, \\gamma \\sim\nN\\left(\\frac{y\\gamma + \\frac{m}{v}}{\\gamma + \\frac{1}{v}}, \\left[\\gamma\n+ \\frac{1}{v} \\right]^{-1}\\right)\\]\n\\[\n\\begin{aligned}\ng(\\gamma \\mid y, \\mu) &\\propto f(y \\mid \\mu, \\gamma)f(\\gamma) \\\\\n& =\n\\left[(2\\pi)^{-\\frac{1}{2}}\\gamma^{\\frac{1}{2}}e^{-\\frac{1}{2}\\gamma(y -\n\\mu)^2} \\right] \\left[\\frac{b^a}{\\Gamma(a)}\n\\gamma^{a-1}e^{-b\\gamma}\\right]\\\\\n& \\propto \\gamma^{\\frac{1}{2}}\\gamma^{a-1}e^{-\\frac{1}{2}\\gamma(y -\n\\mu)^2}e^{-b\\gamma} \\\\\n& = \\gamma^{\\frac{1}{2} + a - 1}e^{-\\frac{1}{2}\\gamma(y - \\mu)^2\n-b\\gamma} \\\\\n& = \\gamma^{\\frac{1}{2} + a - 1}e^{-\\gamma\\left(\\frac{1}{2}(y -\n\\mu)^2 +b\\right)}\n\\end{aligned}\\]\n\\[\\implies \\gamma \\mid y, \\mu \\sim\n\\text{Gamma}\\left(\\frac{1}{2} + a, \\frac{1}{2}(y-\\mu)^2 +\nb\\right)\\]\nSuppose that we choose the following hyperparameters for our prior\ndistributions—\\(m = 0, v = 1, a = 1, b =\n1\\)—and that we observe \\(y =\n2\\). We can implement this Gibbs Sampler using the following\ncode.\n\n\n# set up priors\nm <- 0\nv <- 1\na <- 1\nb <- 1\n# set up data\ny <- 2\n# choose starting values by randomly sampling from our priors\n# (this is just one possible way to choose starting values)\n# (it's also useful to try out a few different starting values)\nset.seed(1)\nmu <- rnorm(1, mean = m, sd = sqrt(v))\ngam <- rgamma(1, shape = a, rate = b)\n# set up empty vectors to store samples\nmus <- c()\ngams <- c()\n# store starting values in vectors of samples\nmus[1] <- mu\ngams[1] <- gam\n\n\n\n\n\n# choose number of iterations\n# (we'll start with 1000, but in practice you'd choose something much bigger)\nN <- 1000\n# run through Gibbs Sampling for a total of N iterations\nfor(i in 2:N){\n  # update mu\n  numerator_for_mu <- y*gam + m/v\n  denominator_for_mu <- gam + 1/v\n  mu <- rnorm(n = 1, mean = (numerator_for_mu)/(denominator_for_mu), sd = sqrt(1/denominator_for_mu))\n  \n  # update gamma\n  alpha <- 0.5 + a\n  beta <- 0.5*(y-mu)^2 + b\n  gam <- rgamma(n = 1, shape = alpha, rate = beta)\n  \n  # store new samples\n  mus[i] <- mu\n  gams[i] <- gam\n}\n\n\n\nNext we can look at a histogram of our posterior samples for \\(\\mu, \\gamma\\) and \\(\\sigma^2 = \\frac{1}{\\gamma}\\). This shows\nwhat values of our parameters we sampled the most in our chain, and if\nwe superimposed the posterior distribution, we would see the\nplausibility represented on these histograms are proportional to the\nplausibility of the real posteriors.\n\n\n\nNext we create a trace plot to show the behavior of the\nsamples over the \\(N\\) iterations.\n\n\n\nTrace plots show the values from which the algorithm sampled from by\niteration. It’s like a tour around the different values of our\nparameters where we spend most time visiting values that are more\nplausible in our posterior. Successful trace plots cover the range of\nvalues in our distributions. It is important that trace plots are not\nstuck on one value, otherwise the distribution will not be accurate.\nTrace plots can best be described as a tour of a neighborhood. One does\nnot want to be stuck in one location. To get the correct distribution,\nthe markov chain should spend the most time in the locations that are\nmost plausible.\nNext, once we have evaluated whether our approximate distribution is\nproportional to our posterior we can estimate different measurements to\nunderstand better our target posterior.\nAs mentioned above, in practice we usually pick a burn-in period of\ninitial iterations to remove. This decision is often motivated by the\nfact that, depending on your choice of starting value, it may take\nawhile for your chain of samples to look like it is “mixing” well. Play\naround with your choice of starting value above to see if you can find\nsituations in which a burn-in period might be helpful.\nDiscussion\nGibbs Sampling vs\nHamiltonian Sampling\nOne of the most difficult aspects in Gibbs sampling is finding the\nconditional distributions for our parameters. As shown in the\nNormal-Normal example previously explained, it was difficult to work\nwith 2 unknown parameters, it’s clear that working with more than two\nunknown parameters would be even more troublesome. Additionally,\nsampling from more than 2 different conditional distributions and\nalternating between them makes the computation for this algorithm highly\nexpensive. Because of efficiency, people have started opting for\nHamiltonian sampling instead. This is reflected on the discussion on\nwhether to use the RSTAN or RJAGS packages, where RSTAN employs a\nHamiltonian algorithm and RJAGS uses a Gibbs algorithm. When comparing\nboth algorithms model specification matters. Furthermore, RJAGS performs\nbetter when we use conjugate priors while RSTAN performs better when we\nhave a non-centered specification and partly and non-conjugate models.\nRSTAN tends to be better at exploring complicated posterior\ndistributions while RJAGS can be very fast for problems with specific\ncharacteristics. Overall, RSTAN provides the best performance (Bølstad 2019).\nReal World Application\nAs mentioned earlier, Gibbs sampling is particularly useful when\ndealing with a multivariate posterior distribution and when we work with\nconjugate priors. This example shows how Gibbs sampling is effective\nwhen dealing with multivariate genetic models, where it’s particularly\ndifficult to find a posterior and where reducing our problem to\ncalculations that involve only one parameter at the time simplifies our\nwork significantly.\nGibbs sampling has been used in the inference of population structure\nusing multilocus genotype data (Jonathan K. Pritchard and Donnelly\n2000). In other words, to infer the population of an\nindividual using their genetic information. I will start defining some\nof the most important terms in the paper. A locus is the specific\nphysical location of a gene or other DNA sequence on a chromosome, like\na genetic street address. A genotype is the pair of alleles inherited\nfrom each parent for a particular gene, where an allele is a variation\nof a gene and a gene is the functional unit of heredity. For example, if\na gene contains information on hair color one allele might code for\nbrown and other for blond, a genotype would be the pair of alleles one\ncoded for brown and the other for blond hair located at a specific\nlocus.\nAssuming that each population is modeled by a characteristic set of\nallele frequencies, let X represent the genotypes of the sampled\nindividuals, Z the populations of origin of individuals, and P the\nallele frequencies in all populations. Each allele at each locus in each\ngenotype is an independent draw from the appropriate frequency\ndistribution. This specifies the probability distribution \\(Pr(X\\mid Z,P)\\). Which is the probability\nthat we draw a specific genotype given the population of origin of an\nindividual and the allele frequency in a population.\nJonathan K. Pritchard, et. al used a Dirichlet distribution to model\nthe probability that we observe specific set of allele frequencies for a\nspecific population and locus.\n\\[D \\sim Dir(\\alpha) =\n\\frac{1}{Beta(\\alpha)}\\prod_{i=1}^J\\theta_i^{\\alpha_i-1}, \\text{\nwhere  }Beta(\\alpha)=\n\\frac{\\prod_{i=1}^K\\Gamma(\\alpha_i)}{\\Gamma(\\sum_{i=1}^J\\alpha_i)}\\space\n\\alpha= (\\alpha_1,...,\\alpha_2)\\] where D is a vector of J\ndimensions of the form \\(D = (\\lambda_1,\n\\lambda_2,...,\\lambda_J),\\text{ and }\\alpha_i>0\\) and \\(D\\) belongs to the probability simplex\nwhere vectors are positive and the sum of their probability mass\nfunctions are always one. We use this distribution to model the allele\nfrequencies \\(p= (p_1,p_2,...,p_J)\\)\nknowing that these frequencies sum to 1.\nThe authors use a Dirichlet distribution given that it is a commonly\nused conjugate prior. As it was previously discussed having a conjugate\nprior makes Gibbs sampling a good choice to approximate the posterior\ndistribution. Conjugate priors make the process of estimating a\nposterior easier given that the posterior will be in the same\nprobability distribution family.\nNext, we will introduce some of their model notation. The authors\nassumed that each population is modeled by a characteristic set of\nallele frequencies. Now on, consider X denotes the genotypes of the\nsampled individuals, Z denotes the individual’s unknown populations of\norigin, and P denotes the unknown allele frequency in all\npopulations.\nWe adopt a Bayesian approach by specifying models priors \\(Pr(Z)\\) and \\(Pr(P)\\) for both \\(Z\\) and \\(P\\).\nHaving observed the genotypes (X), our knowledge of Z and P is given\nby the posterior distribution\n\\[Pr(Z,P\\mid X) \\propto Pr(Z)\nPr(P)Pr(X\\mid Z,P)\\]\nWhere \\(Pr(Z)\\) and \\(Pr(P)\\) are the priors and \\(Pr(X\\mid Z,P)\\) is the likelihood function\nof a genotype given a population and allele frequency.\nWe can’t compute this distribution exactly but we can obtain an\napproximate sample \\[(Z^{(1)},P^{(1)}),(Z^{(2)},P^{(2)}),\n...,(Z^{(M)},P^{(M)})\\] from \\(Pr(Z,P\\mid X)\\) using Gibbs Sampling.\nInference for Z and P may be based on summary statistics obtained from\nthis sample. We will focus on a simpler model where each person is\nassumed to have originated in a single population.\nSuppose we sample N individuals with paired chromosomes (diploid). We\nassume each individual originates in one of K populations, each with its\nown characteristic set of allele frequencies. We will use the vectors X\n(observed genotypes), Z (populations of origin of the individuals), and\nP (the unknown allele frequencies in the populations). These vectors\nconsist of the following elements.\n\\[(x_l^{(i,1)}, x_l^{(i,2)}) =\n\\text{genotype of the ith individual at the lth locus, where i=\n1,2,...,N and l= 1,2,...,L;}\\] \\[Z^{(i)}= \\text{population from which individual i\noriginated}\\] \\[p_{klj}=\n\\text{frequency of allele j at locus l in population k, where\nk=1,2,...,K and }j = 1,2,...,J_l\\]\nwhere \\(J_{l}\\) is the number of\ndistinct alleles observed at locus l, and these alleles are labeled\n1,2,…,\\(J_{l}\\).\nGiven the population of origin of each subject, the genotypes are\nassumed to be sampled by drawing alleles independently from the\nrespective population frequency distributions \\(Pr(X\\mid Z,P) = Pr(x_l^{(i,a)} = j\\mid Z,P) =\np_z(i)lj\\text{      (2)}\\) independently for each \\(x_l^{(i,a)}\\) (allele for ith individual at\nlth locus). \\(p_z(i)lj\\) is the\nfrequency of allele j at locus l in the population of origin of\nindividual i and it’s our likelihood function.\nWhen defining the prior \\(P(Z)\\) the\nauthors assumed that before observing the genotypes we have no\ninformation about the population of origin of each subject. If the\nprobability that individual i originated in population k is the same for\nall k, then\n\\[P(Z) = Pr(z^{(i)} = k) =\n\\frac{1}{K}\\text{    (4)}\\]\nindependently for all individuals.\nWhen defining the prior \\(Pr(P)\\)\nthe authors used the Dirichlet distribution to model the distribution on\nallele frequencies \\(p=\n(p_1,p_2,...,p_J)\\). These frequencies have the property that\nthey sum up to 1. This distribution specifies the probability of a\nparticular set of allele frequencies \\(p_{kl}\\) for population k at locus l.\n\\[ Pr(P) = p_{kl}\\sim D = (\\lambda_1,\n\\lambda_2,...,\\lambda_J)\\text{  (5)}\\]\nin dependently for each k,l. The expected frequency of allele j for a\npopulation k is proportional to \\(\\lambda_j\\), and the variance of this\nfrequency decreases as the sum of the Probability Mass Function (PMF) of\n\\(\\lambda_j\\) increases (as the sum of\nthe PMF is closer to 1). The authors take \\(\\lambda_1 = \\lambda_2 = ... =\n\\lambda_{Jl}=1.0\\) which gives a uniform distribution on the\nallele frequencies allowing each \\(\\lambda_j\\) to be equally likely.\nThe authors used the following conditional distributions,\n\\[Pr(Z\\mid X,P) \\propto Pr(X \\mid Z,P)\nPr(Z)\\]\nAnd, \\[Pr(P\\mid X,Z) \\propto Pr(X \\mid\nZ,P) Pr(P)\\]\nwhich are composed by a combination of the data \\(f(X \\mid Z,P)\\) and the priors \\(Pr(P)\\) \\(Pr(Z)\\) we defined previously. We can see\nthat by using these conditional distributions we can make our problem\neasier by dealing with one parameter at a time. After, defining the\nconditional distributions we can construct a Markov chain with\nstationary (target) multinomial distribution \\(Pr(Z,P\\mid X)\\) as follows:\nStarting with the initial value \\(Z^{(0)}\\) for Z (chosen randomly) we\niterate over the following steps for m=1,2,….\nStep 1. Sample \\(P^{(m)}\\) from\n\\(Pr(P \\mid X,Z^{(m-1)})\\)\nStep 2. Sample \\(Z^{(m)}\\) from\n\\(Pr(Z \\mid X,P^{(m)})\\)\nIn step 1 we are estimating allele frequencies for each population\ngiven our sampled genotype and assuming that the population of origin of\neach individual is known. In step 2 we estimate the population of origin\nof each individual, given our sampled genotype and assuming that the\npopulation’s allele frequencies are known. For sufficiently large m and\nc, \\((Z^{(m)}, P^{(m)}), (Z^{(m+c)},\nP^{(m+c)}),(Z^{(m+2c)}, P^{(m+2c)}),...\\) will be approximately\nindependent random samples from our posterior \\(Pr(Z,P \\mid X)\\).\nConclusion\nGibbs sampling is a distribution approximation method suitable for\nproblems where we are trying to estimate more than one parameter. It’s a\nvariation of the Metropolis-Hastings algorithm. We sample alternatively\nfrom conditional distributions until we approximate our posterior. Like\nother MCMC methods, it’s based on the idea of creating chains of values,\nwhere for each chain we sample from models dependent on the value of the\nprevious chain. Computationally, it’s more efficient when we work with\nconjugate priors; however, overall there has been a greater preference\nfor Hamiltonian software algorithms that are suitable for more complex\nmodels. Gibbs sampling is the best choice in specific conditions, like\nin the genetics example described earlier, it is suitable when we have a\nconjugate Dirichlet prior, and when working with one parameter at a\ntime.\nReferences\n\n\n\nAlicia A. Johnson, Mine Dogucu, Miles Q. Ott. 2022. Bayes Rules!:an\nIntroduction to Applied Bayesian Modeling. Chapman; Hall/CRC.\n\n\nBølstad, Jørgen. 2019. “How Efficient Is Stan Compared to JAGS?:\nConjugacy, Pooling, Centering, and Posterior Correlations.”\n\n\nGuo, Jonah Gabry, Jiqiang, and Sebastian Weber. 2020. “Rstan: R\nInterface to Stan.”\n\n\nJonathan K. Pritchard, Matthew Stephens, and Peter Donnelly. 2000.\n“Inference of Population Structure Using Multilocus Genotype\nData.”\n\n\nLambert, Ben. 2018. “An Introduction to Gibbs Sampling.”\n\n\nPease, Christopher. 2018. “An Overview of Monte Carlo\nMethods.”\n\n\n\n\n",
    "preview": "posts/2022-06-16-gibbssampling/MC.png",
    "last_modified": "2025-12-08T11:05:35-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-06-16-housevalues/",
    "title": "Silicon Valley Evolution and Its Impact on House Values",
    "description": "How do tech migration and other socioeconomic factors affect house values across census tracts in the Bay Area?",
    "author": [
      {
        "name": "Thu Dang, Franco Salinas",
        "url": {}
      }
    ],
    "date": "2022-06-16",
    "categories": [],
    "contents": "\n\n\n\n\n\n\n\n\n\nAn\nIntroduction to the Evolution of Silicon Valley and Its House\nValues\nSilicon Valley is the most established tech center in the world. Home\nto major technology conglomerates like Google, Apple, and Tesla, Silicon\nValley is currently worth approximately $3 trillion. The area has\nevolved in a way that gradually lays the foundation for the booming tech\nindustry. It started with various education and business opportunities\nrelated to electronics and electrical engineering. These business\nopportunities attracted companies from the semiconductor industry, which\nis essential to creating software, and the vision for the Internet. This\nmade the area suitable for the rise of startups like Apple, Oracle, and\nIntel, which have become billion-dollar-worth household names and given\nway to the rise of the tech startup culture in the area. Although the\ndot-com bubble burst in 2000 scaled back the explosive overnight growth\nof the dot-com startups, it has allowed the tech industry to grow more\nsustainably than ever. The rise of companies like Google, Meta, and\nAmazon in this century has changed the way we live and think, which has\nresulted in increasing wealth in the area, reflected in the price of\nreal estate (OnePieceWork, 2020).\nThe real estate market in the San Francisco Bay Area saw an uptick in\nprice ever since the dot-com bubble. Despite being scaled back by the\n2008 financial market crash, it has seen the greatest growth ever since\n(Carlisle, 2021). In 2019, the median housing price in the Bay Area for\na single-family home was $1.06 million (Neilson, 2021; CAR,2021), which\nis 230% more expensive than the median house sales price in the whole\nUnited States by the end of 2019 (FRED, 2022).\nResearch question\nHow do tech migration and other socioeconomic factors affect\nhouse values across census tracts in the Bay Area?\nWe believe that the wealth in tech has spilled over to the real\nestate industry in the Bay Area. As two individuals interested in\npursuing a tech career in emerging tech areas, i.e. Salt Lake City,\nUtah, or Austin, Texas, we wonder how the real estate scene there will\nevolve in the future, using the Bay Area as the point of reference.\nAn\noverall look at the house price in the Bay Area between 2009 and\n2019\nLooking at the evolution of house prices in 6 Bay Area counties\nbetween 2009 and 2019, it is clear that the house prices increase over\ntime, as indicated by the lighter color. Houses are more expensive in\nareas with a higher concentration of tech companies, such as counties in\nSan Francisco, San Mateo, and Santa Clara, with the median house prices\ngoing from around $750,000 in 2009 to over $1M\nin 2019.\n\n\n\nData Context\nIn this project, we worked with two types of data: Bay Area census\ndata and tech company data. We intentionally narrowed the data down to\nbetween 2009 and 2019 to avoid any bias from the 2008 economic downturn\nrecovery and outliers during COVID-19.\nBay Area census data\nThe data is collected on each census tract from the American\nCommunity Service (ACS) run by the U.S. Census Bureau. These yearly\nestimates are an inference from data collected over 5-year intervals\nbetween 2009 and 2019 (ACS, 2022). The data provides aggregate summaries\nof demographic information in census tracts of 6 Bay Area counties: San\nFrancisco, San Mateo, Santa Clara, Alameda, Marin, and Contra Costa.\nEach census tract contains roughly 4000 inhabitants that are intended to\nbe demographically and economically homogeneous. For the purpose of this\nanalysis we used data on median estimated income, median population,\nmedian age, median household value, median household size, the median\nnumber of houses under tenure, the median proportion of people with\ndifferent places of birth, the median proportion of different races,\nmedian proportion of different industries, median proportion of houses\nowned and rented, area, and segment. We create the segment variable to\ngroup different census tracts according to the aggregated growth of\ncompanies.\nTech company data\nWe performed our analysis on the 13 companies with the largest market\ncap in the Bay Area (Value, 2022). These companies are Apple, Google,\nMeta, Tesla, NVIDIA, Visa, Cisco, Broadcom, Adobe, Netflix, Salesforce,\nOracle, and PayPal. Although Visa started out as a financial company, we\nincluded Visa in this list because its payment services are rooted in\nsecurity technologies and the company has transformed into a financial\ntechnology (fintech) company with the evolution of technology (VISA,\n2022).\nWe chose these 13 companies instead of all of the tech companies\navailable in the Bay Area assuming the 80-20 rule applies in this case.\nWe believe that the top 20% tech companies are those that contribute to\n80% of the wealth in the Bay Area, and hence are more relevant to our\nanalysis. We collected yearly data on the Earnings Before Interest, Tax,\nDepreciation, and Amortization (EBITDA) of these companies from 2009 to\n2019, which aligns with the timeframe for the census data. We chose\nEBITDA as our growth indicator because it is a commonly used indicator\nfor a company’s financial stability. We aggregated the EBITDA data of\nall companies within a 2000 meters radius of distance from a specific\ncensus tract to create a proxy for technology-related economic activity\n(Investopedia, 2022). We obtained the tech company data from the\nMacrotrends website (Macrotrends, 2022).\nThrough longitudinal and spatial data analysis, we hoped to gain\ninsight into how the tech industry and the rise of tech hubs can affect\nhousing value.\nOur\nanalysis approach: Longitudinal and Spatial Analysis\nWe chose a longitudinal model because we wanted to observe and\nexplain the changes in house values resulting from the rise of tech\ncompanies and other socioeconomic factors in the long run. A\nlongitudinal model allows us to consider various factors that may affect\nthe outcome over time. It also provides the correlation between them,\nenabling us to determine which factors are essential in explaining house\nvalues. We believe that, while there are factors related to tech\nmigration that might lead to short-term house price increases, there is\na delay in time between the migration of tech companies and the changes\nin house value. Therefore, a longitudinal analysis allows us to\nincorporate time into our analysis. Additionally, we were able to\nobserve and compare changes across multiple observation units, which are\nthe census tracts in this case.\nIn addition to accounting for the time aspect, we also wanted to take\ninto consideration how the house prices in a census tract are impacted\nby those in the neighboring census tracts. Therefore, we used techniques\nrelated to spatial data to perform that analysis.\nLongitudinal Analysis\nIn our longitudinal analysis, we hoped to explain the changes in Bay\nArea house prices through the tech migration and other socioeconomic\nfactors over time. The delayed effect of tech companies’ growth on house\nprices can be explained by macroeconomic dynamics. In this case, we\nbelieve that there has been a boost in demand for housing due to the\nexpanding labor market caused by the growth of tech companies in the\narea. This increase in demand is expected to cause an increase in house\nvalues when the supply gradually becomes insufficient.\nWe hypothesize that there are different degrees of growth for tech\ncompanies, and the census tracts with higher growth companies will see\nmore drastic increases in house prices. In order to fulfill our research\nquestion, we set out to segment the census tracts into different groups\nand look that them in relation to different socioeconomic indicators to\nsee how they impact house prices.\nData Processing\nTo prepare the data for our analysis, we combined the two datasets,\ni.e. tech company data and the Bay Area data, into one big dataset using\ntheir longitude and latitude information. The Bay Area data already has\nspatial information, with longitudinal and latitude, while the company\ndata does not. We tackled this by using the geocode package to obtain\nthe longitudinal and latitude information of the companies based on\ntheir addresses. Specifically, geocode takes the companies’ addresses\nand adds the corresponding longitude and latitude to the existing table.\nAfter that, we transformed the coordinate system of the company data to\nmatch that of the Bay Area dataset as these two datasets have different\ntypes of coordinate systems. This way, we confirmed we could combine the\ndata. Next, we aggregate the data by matching each company with the\ncensus tracts located within its 2000 meters radius. We then computed\nthe overall EBITDA growth percentage in the census tracts that are in\nthe 2000-meter proximity to the tech companies between 2009 and\n2019.\nWe separated the census tracts into four different groups based on\ntheir yearly EBITDA growth and used different resources to determine the\ncut-off points (Geckoboard, 2022; BalboaCapital, 2022). Census tracts\nthat have yearly EBITDA growth greater than 50% are the “High Growth”\ntracts. “Medium Growth” tracts have EBITDA growth ranging from 20% to\n50%, and the figure for “Low Growth” tracts is under 20%. Census tracts\nthat are not within 2000 meters radius of tech companies belong to the\n“Control” group. For the rest of our paper, we’ll refer to these\ndifferent growth groups with the understanding that these growth groups\nare defined by the EBITDA growth of tech companies.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom this graph, we can see that the house prices across the 4 growth\nsegments follow the same trend. However, after 2014, there are some\ndrastic changes. The line representing the median house price in high\ngrowth areas is steepest, followed by medium growth and high growth.\nThis potentially reflects the fact that the high growth of tech\ncompanies had a larger effect on house values. High-growth,\nmedium-growth, and low-growth groups have higher house prices than the\ncontrol group, which is consistent with our prior beliefs that the\npresence of tech companies correlates with house values. The fact that\nareas with low tech growth present higher prices for years between 2013\nand 2017 could be caused by different factors like increases in the\nsupply of houses in medium and high growth areas. Also, during that\ntime, there were probably many emerging startups, which required more\nspace for offices within the Silicon Valley, which potentially generated\nhouse shortages and boosted the price.\nVariable consideration for\nthe model\nAfter conducting an analysis of the correlation between the different\npotential factors and Bay Area median house prices, the factors that\npresent strong correlations with the outcome are income, age, household\nsize, the proportion of people whose birthplace was in a state, the\nproportion of people in the information industry, finance, professional,\nand other industries. The proportion of white and black populations also\nhad a considerable correlation with median house value. As the analysis\nfocuses more on causal inference than prediction, we won’t omit any\nconfounding variables. It is important to mention that we are including\nindependent variables that are moderately correlated with each\nother.\nWe also include county information in the data to control for the\neffect on housing. The reason is that census tracts that are in the same\ncounty tend to have similar house prices, which makes county a\nreasonable indicator. We hypothesize that, due to the spillover effect\nfrom the thriving tech companies, counties with higher EBITDA growth\nwill have a larger house value compared to those with lower EBITDA\ngrowth. Additionally, higher incomes and population growth increase the\ndemand for housing as people can afford larger mortgages and there is a\nhigher demand for housing (Pettinger, 2021).\n\n\n\n\n\n\nGeneralized\nEstimating Equation (GEE) Model\nWe chose GEE because it allows us to work with different correlation\nstructures. In other words, we are able to test different assumptions\nregarding the correlation magnitude in our longitudinal data. Another\noption is a mixed-effects model, which we consider quite simplistic due\nto its assumption of exchangeable correlation. In other words, it is\nunrealistic to assume that longitudinal data has a constant correlation\nindependent of the difference in years.\nFurthermore, we consider that using GEE is advantageous as it has\nRobust Standard Errors (SE), which is independent of the working\ncorrelation structure and is close to the real SE. This allowed us to\ncompare the Model SE to the Robust SE when evaluating and choosing the\nbest model.\nCorrelation structure is a crucial component of the GEE model. When\nwe set out to model the median house value, we were concerned about\nfinding an appropriate correlation structure for the data we have. A\ncorrelation structure indicates our assumption for the magnitude at\nwhich the data correlate over time.\nTherefore, we created different models to test different correlation\nstructures and variables potentially impacting house values. In the\nfirst model, we used the exchangeable correlation structure, which\nassumes that the correlation between different EBITDA growth groups is\nconstant and that the correlation for each EBITDA group does not change\nover the years. We don’t think that this assumption is realistic in the\ncontext of our analysis. In fact, we believe that the correlation among\nthe observations varies across the years. Specifically, given the\neffects of the 2008 economic crisis on the housing market, we would\nexpect house values between the years 2009 and 2010 to be more similar\nthan those between 2009 and 2018.\nIn our final model, we used a correlation structure that shows an\nexponential decay to 0 with time in the correlation of observations.\nThis correlation structure is known as an Autoregression Model of order\n1 (AR1). We also chose the set of variables that would shed light on the\nchanges in house prices. We transformed some of the variables to make\nthem more concise. For our outcome variable, we decided to divide the\nmedian house values by 100,000 to decrease the magnitude of the data and\nmake computations easier. Similarly, we also reduced the magnitude of\nthe income variable by 10,000. We included the indicators for household\nsize, income, the proportion of people born in California, the\nproportion of white people, and county variables. With this set of\nvariables, we aimed to explain how tech growth and socioeconomic changes\nimpact house values.\nWe carry out a hypothesis test on the estimated coefficients in the\nGEE model to see whether there is a powerful relationship between the\nchosen factors and house values over the years. We conclude that the\nfactors contribute significantly to explaining house values, indicated\nwith a p-value lower than 0.05. Specifically, there is in fact a 0%\nchance that we would observe the data as we have if there was no\ncorrelation at all between the variables and the house value\noutcome.\n\n\n\n\n\n\n\n\n\nEvaluating\nthe GEE model through its residual plot\nA residual plot shows whether the model underpredicts or overpredicts\nhouse prices in different areas of the Bay Area. While white color shows\nthat the actual house prices are similar to the model predictions, red\nindicates areas that have higher actual house prices, and blue shows\nareas with lower actual house prices than the model predictions. Looking\nat the residual plot, we can see that the actual house values in the\ncensus tracts around the Silicon Valley are much higher than what the\nGEE model predicts. On the other hand, houses in bigger census tracts\nhave much lower values than the model estimates. From these results, we\nbelieve that there is some degree of spatial correlation that cannot be\naccounted for by a longitudinal model. In other words, the GEE model\ndoes not take into account how the median house price in a certain\ncensus tract can be influenced by the prices in the neighboring census\ntracts.\n\n\n\nIn order to verify this spatial relationship between the residuals\nafter predicting using the GEE model, we conduct the Moran’s I test. We\nfound that the residuals are spatially correlated, indicated by the\np-value smaller than 0.05 in the test (p-value = \\(2.2 {*} 10^{-16}\\)). In other words, there\nis almost a 0% chance that we would observe the actual median house\nprices like in the data if the residuals were not spatially\ncorrelated.\nTherefore, we decide to create a spatial model to account for the\nspatial correlation that is unexplained by the GEE longitudinal\nmodel.\nSpatial Analysis\nIntroduction to\nNeighborhood Structure\nCensus tracts that are closer to one another might have similar house\nprices. For example, census tracts that are in counties closer to tech\ncompanies like San Mateo and Santa Clara will be more similar compared\nto those in Marin which is much further from the tech hub. Therefore, it\nis important to statistically identify groups of census tracts that are\ncloser, or are “neighbors”, to the others. In order to identify groups\nof neighbors, we use a concept called neighborhood structure, which\nhelps us establish how close the census tracts are to one another. There\nare many techniques to define neighborhood structure, but here we used a\nmethod called K Nearest Neighbors (KNN), which calculates the distance\namong the centers of the census tracts and groups the census tracts into\ndifferent groups of k neighbors.\nWe chose the KNN neighborhood structure as we want to restrict the\nnumber of neighbors in each group. We believe that this method is\nvaluable in the sense that it creates groups of neighboring census\ntracts, each with k neighbors, and hence contributes to the\ngeneralization ability of the model. We choose 3 as the number of\nneighbors each census tract can have as it helps prevent the very\ncongested network for the counties closer to the city. In other words,\nit helps eliminate the problem of census tracts having too few or too\nmany neighbors.\nSpatial Autoregressive (SAR)\nModel\nWe use Spatial Autoregressive Models (SAR) to model the spatial\ncorrelation for the leftover residual from the linear regression model.\nThe SAR model reflects a stronger pattern of spatial covariance than CAR\n(when using the same parameter and weights), and it decays slower than\nthe CAR model. The SAR model is also better than the CAR model, as\nindicated by the BIC indices of the SAR and CAR fitted models. The BIC\nindex is the goodness-of-fit measure, and it shows how well the model\nfits the data.\nFrom the longitudinal analysis, we know that there are tech growth\nand socioeconomic factors affecting house values over time. Our goal for\nspatial analysis is to account for spatial correlation in a specific\nyear. As spatial techniques cannot be conducted on data spanning\ndifferent time periods, we choose to perform spatial analysis on 2018\ndata only.\nAfter using SAR to model the data, we ran the Moran’s I test again\nand found that the leftover errors after SAR are independent. In other\nwords, the SAR model has captured the possible spatial correlation, as\nindicated by the Moran’s I statistics, which is nearly 0, and the\np-value greater than 0.05 (p-value = 0.1489). Therefore, not only there\nis a longitudinal effect on house prices over the years in Bay Area\ncensus tracts but there is also a spatial correlation in each year, as\nwe have seen for 2018.\nEvaluating\nthe SAR model through its residual plot\nThe residual plot for the SAR model shows that there are some\ncounties whose house values are underestimated or overestimated by the\nmodel, indicated by the darker red and blue colors. Specifically, we can\nsee very large residuals in the San Mateo and Santa Clara counties as\nwell as larger counties on the leftmost and rightmost sides of the\nmap.\nWe carry out further exploration of the area using Google Maps and\ndiscover some terrain characteristics that can explain the high\nmagnitude of difference in the residuals. For example, there are two\nareas whose residuals are still high even after the GEE and SAR models\nthat are located along the inner coast of San Mateo County, as indicated\nby the deep blue color. The large residuals turn out to be reasonable as\nthese two areas are not residential areas. In other words, one of them\nis Foster City, which is an industrial area, and the other area is the\nhome of national parks. The houses in these areas might be undesirable\ndue to their proximity to industrial/manufacturing areas or pure nature\nreserves and no business or recreational endeavors. In addition, the\norange-color census tracts on the top left corner and lower right corner\nof the map show a different trend. Using Google Maps, we can see that\nthese two areas are the home of several national and local parks. As the\norange color indicates that the actual house values in these areas are\nhigher than SAR predictions, there might be luxury resorts or private\nholiday properties that might spike the house prices. For houses around\nthe Silicon Valley area that are also of an orange shade, the higher\nactual prices can be explained by higher demand given that it is close\nto big tech companies with high wealth and large labor forces. This\nmakes the area appealing to not only workers but also other big tech\ncompanies, emerging tech startups, and venture capital funds.\n\n\n\n\n\n\nResults and Discussion\nIn our analysis, we found evidence suggesting that house values grow\ndifferently for different growth segments post-2014. Especially for the\nhigh growth segment, the increase in the house prices outweighs the\nincrease in the medium and low growth groups as well as control areas.\nSuch drastic changes can be the result of the market recovery post-2018\neconomic downturn as well as the technology innovations and growth\nhappening in the area (Carlisle,2021). This is consistent with our\nhypothesis that higher company growth is associated with higher house\nvalues. Aside from the longitudinal effect of tech growth and other\nsocioeconomic factors on Bay Area house prices over the years, there is\nalso a spatial correlation in each year, meaning that house prices in\nneighboring areas of a census tract can inform the house price of that\narea.\nIn addition, income and whether the census tract has a majority white\npopulation are positively correlated with higher house values. Perhaps\nthis is because higher income allows people to afford more expensive\nhouses. Also, the higher the proportion of white residents in a census\ntract, the higher house values are likely to be. This could be due to\nthe effect of redlining, where people of color have been historically\ndiscriminated against in the housing market.\nOn the other hand, the median household size in a census tract and\nwhether there is a high proportion of people born in California\nnegatively correlate with house prices. Such a trend in household size\ncan be explained in terms of race and age. White households are usually\nsmaller in size, while people of color households tend to be bigger due\nto living with extended families or having more children. Also,\nimmigrants are more likely to live with extended families who immigrate\nwith them. Regarding the negative effects of the proportion of\nCalifornian residents, that can be explained by foreign money. If there\nis a higher proportion of people from outside of California coming to\nthe area, there will be more wealth flowing into the area, potentially\nthrough tech investments from other states or countries.\nFinally, the house prices of nearby census tracts impact one another.\nWe can see that Silicon Valley and the neighboring census tracts\nexperience relatively similar house values. Other residential census\ntracts that are further away or in a county that is not affiliated with\nbig techs like Marin or Contra Costa experience much lower house\nprices.\nMoving forward, there are many directions we can take to improve the\nanalysis. Firstly, we can consider using Uber data to determine the\ndistance between different census tracts and tech companies in terms of\nminutes. As we hypothesize that house prices are in high demand as\nhigh-income tech employees want to live closer to their companies, this\ncan allow us to identify which areas can be desirable for tech\nemployees. Secondly, natural terrain can be a factor in determining\nhouse prices. As we have mentioned, areas with national parks or right\nbeside mountains might have cheaper houses; therefore, incorporating\nnatural geographic data into the analysis can improve our models.\nThirdly, we hope to combine our longitudinal and spatial models into one\nas we potentially have to write a new library for the joined model.\nFinally, we would like to control for interest rates, considering that\nlower interest rates could lead to higher demand for housing due to\nbetter mortgage deals.\nAdditionally, as our ultimate goal is to understand the house prices\nin emerging tech hubs like Austin, Texas, and Salt Lake City, Utah, we\nwant to obtain similar data in these areas to verify the insights in\nthis analysis and potentially make a predictive model for these\nareas.\nOne limitation to our census data is that there is a ceiling, or\nmaximum, value for house prices. Before 2014, the house value is capped\nat $1M, whereas the cap is $2M after 2014.\nThere might have been some wording changes on the census survey, which\naffects the Bay Area census data.\nAcknowledgements\nWe would love to thank our amazing Prof. Brianna Heggeseth for her\nguidance and inspiration for this project. We love you, Brianna!\nReferences\nBalboa Capital. 2022. “What Is Ebitda?” https://www.balboacapital.com/blog/what-is-ebitda/.\nCalifornia Association of Realtors. 2021. “December Home Sales and\nPrice Report.” https://www.car.org/en/aboutus/mediacenter/newsreleases/2021releases/dec2020sales.\nCarlisle, Patrick. 2021. “30+ Years of Bay Area Real Estate Cycles -\nCompass.” https://www.bayareamarketreports.com/trend/3-recessions-2-bubbles-and-a-baby.\nGeckoboard. 2022. “Revenue Growth Rate.” https://www.geckoboard.com/best-practice/kpi-examples/revenue-growth-rate/.\nMacrotrends. 2022. “Macrotrends - The Premier Research Platform for\nLong Term Investors.” https://www.macrotrends.net/.\nNeilson, Susie. 2021. “Here’s What Home Prices Look Like Right Now\nfor Each Bay Area County.” https://www.sfchronicle.com/business/article/Here-s-what-home-prices-look-like-right-now-for-15906551.php#:~:text=The%20median%20price%20for%20a,2019%2C%20according%20to%20the%20report.\nOnePiece Work. 2020. “The Evolution of the Bay Area Tech Scene.” https://www.onepiecework.com/blog/the-evolution-of-the-bay-area-tech-scene.\nPettinger, Tejvan. 2021. “Factors Affecting Supply and Demand of\nHousing.” https://www.economicshelp.org/blog/15390/housing/factors-affecting-supply-and-demand-of-housing/.\nTarver, Evan. 2021. “What Does the Ebitda Margin Imply About a\nCompany’s Financial Condition?” https://www.investopedia.com/ask/answers/032715/why-ebitda-margin-considered-be-good-indicator-companys-financial-health.asp.\nUnited States Census Bureau. 2022. “American Community Survey 5-Year\nData (2009-2020).” https://www.census.gov/data/developers/data-sets/acs-5year.html.\nU.S. Census Bureau and U.S. Department of Housing and Urban\nDevelopment. 2022. “Median Sales Price of Houses Sold for the United\nStates [MSPUS].” https://fred.stlouisfed.org/series/MSPUS.\nVALUE.TODAY. 2022. “California Top Companies List by Market Cap as on\nJan 7th, 2022.” https://www.value.today/headquarters/california.\nVisa Research. 2022. “About Visa Research.” https://usa.visa.com/about-visa/visa-research.html.\n\n\n\n",
    "preview": "posts/2022-06-16-housevalues/real-estate.png",
    "last_modified": "2025-12-08T11:05:36-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-06-13-realestate/",
    "title": "Silicon Valley Evolution and Its Impact on House Values",
    "description": "How do tech migration and other socioeconomic factors affect house values across census tracts in the Bay Area?",
    "author": [
      {
        "name": "Thu Dang, Franco Salinas",
        "url": {}
      }
    ],
    "date": "2022-06-10",
    "categories": [],
    "contents": "\n\n\n\n\n\n\n\n\n\nAn\nIntroduction to the Evolution of Silicon Valley and Its House\nValues\nSilicon Valley is the most established tech center in the world. Home\nto major technology conglomerates like Google, Apple, and Tesla, Silicon\nValley is currently worth approximately $3 trillion. The area has\nevolved in a way that gradually lays the foundation for the booming tech\nindustry. It started with various education and business opportunities\nrelated to electronics and electrical engineering. These business\nopportunities attracted companies from the semiconductor industry, which\nis essential to creating software, and the vision for the Internet. This\nmade the area suitable for the rise of startups like Apple, Oracle, and\nIntel, which have become billion-dollar-worth household names and given\nway to the rise of the tech startup culture in the area. Although the\ndot-com bubble burst in 2000 scaled back the explosive overnight growth\nof the dot-com startups, it has allowed the tech industry to grow more\nsustainably than ever. The rise of companies like Google, Meta, and\nAmazon in this century has changed the way we live and think, which has\nresulted in increasing wealth in the area, reflected in the price of\nreal estate [@OnePieceWork_2020].\nThe real estate market in the San Francisco Bay Area saw an uptick in\nprice ever since the dot-com bubble. Despite being scaled back by the\n2008 financial market crash, it has seen the greatest growth ever since\n[@Carlisle_2021]. In 2019, the median\nhousing price in the Bay Area for a single-family home was $1.06 million\n[@Neilson_2021;@CAR_2021],\nwhich is 230% more expensive than the median house sales price in the\nwhole United States by the end of 2019 [@FRED_2022].\nResearch question\nHow do tech migration and other socioeconomic factors affect\nhouse values across census tracts in the Bay Area?\nWe believe that the wealth in tech has spilled over to the real\nestate industry in the Bay Area. As two individuals interested in\npursuing a tech career in emerging tech areas, i.e. Salt Lake City,\nUtah, or Austin, Texas, we wonder how the real estate scene there will\nevolve in the future, using the Bay Area as the point of reference.\nAn\noverall look at the house price in the Bay Area between 2009 and\n2019\nLooking at the evolution of house prices in 6 Bay Area counties\nbetween 2009 and 2019, it is clear that the house prices increase over\ntime, as indicated by the lighter color. Houses are more expensive in\nareas with a higher concentration of tech companies, such as counties in\nSan Francisco, San Mateo, and Santa Clara, with the median house prices\ngoing from around $750,000 in 2009 to over $1M\nin 2019.\n\n\n\nData Context\nIn this project, we worked with two types of data: Bay Area census\ndata and tech company data. We intentionally narrowed the data down to\nbetween 2009 and 2019 to avoid any bias from the 2008 economic downturn\nrecovery and outliers during COVID-19.\nBay Area census data\nThe data is collected on each census tract from the American\nCommunity Service (ACS) run by the U.S. Census Bureau. These yearly\nestimates are an inference from data collected over 5-year intervals\nbetween 2009 and 2019 [@ACS_2022]. The data provides aggregate\nsummaries of demographic information in census tracts of 6 Bay Area\ncounties: San Francisco, San Mateo, Santa Clara, Alameda, Marin, and\nContra Costa. Each census tract contains roughly 4000 inhabitants that\nare intended to be demographically and economically homogeneous. For the\npurpose of this analysis we used data on median estimated income, median\npopulation, median age, median household value, median household size,\nthe median number of houses under tenure, the median proportion of\npeople with different places of birth, the median proportion of\ndifferent races, median proportion of different industries, median\nproportion of houses owned and rented, area, and segment. We create the\nsegment variable to group different census tracts according to the\naggregated growth of companies.\nTech company data\nWe performed our analysis on the 13 companies with the largest market\ncap in the Bay Area [@Value_2022]. These companies are Apple,\nGoogle, Meta, Tesla, NVIDIA, Visa, Cisco, Broadcom, Adobe, Netflix,\nSalesforce, Oracle, and PayPal. Although Visa started out as a financial\ncompany, we included Visa in this list because its payment services are\nrooted in security technologies and the company has transformed into a\nfinancial technology (fintech) company with the evolution of technology\n[@VISA_2022].\nWe chose these 13 companies instead of all of the tech companies\navailable in the Bay Area assuming the 80-20 rule applies in this case.\nWe believe that the top 20% tech companies are those that contribute to\n80% of the wealth in the Bay Area, and hence are more relevant to our\nanalysis. We collected yearly data on the Earnings Before Interest, Tax,\nDepreciation, and Amortization (EBITDA) of these companies from 2009 to\n2019, which aligns with the timeframe for the census data. We chose\nEBITDA as our growth indicator because it is a commonly used indicator\nfor a company’s financial stability. We aggregated the EBITDA data of\nall companies within a 2000 meters radius of distance from a specific\ncensus tract to create a proxy for technology-related economic activity\n[@Investopedia_2022]. We obtained\nthe tech company data from the Macrotrends website [@Macrotrends_2022].\nThrough longitudinal and spatial data analysis, we hoped to gain\ninsight into how the tech industry and the rise of tech hubs can affect\nhousing value.\nOur\nanalysis approach: Longitudinal and Spatial Analysis\nWe chose a longitudinal model because we wanted to observe and\nexplain the changes in house values resulting from the rise of tech\ncompanies and other socioeconomic factors in the long run. A\nlongitudinal model allows us to consider various factors that may affect\nthe outcome over time. It also provides the correlation between them,\nenabling us to determine which factors are essential in explaining house\nvalues. We believe that, while there are factors related to tech\nmigration that might lead to short-term house price increases, there is\na delay in time between the migration of tech companies and the changes\nin house value. Therefore, a longitudinal analysis allows us to\nincorporate time into our analysis. Additionally, we were able to\nobserve and compare changes across multiple observation units, which are\nthe census tracts in this case.\nIn addition to accounting for the time aspect, we also wanted to take\ninto consideration how the house prices in a census tract are impacted\nby those in the neighboring census tracts. Therefore, we used techniques\nrelated to spatial data to perform that analysis.\nLongitudinal Analysis\nIn our longitudinal analysis, we hoped to explain the changes in Bay\nArea house prices through the tech migration and other socioeconomic\nfactors over time. The delayed effect of tech companies’ growth on house\nprices can be explained by macroeconomic dynamics. In this case, we\nbelieve that there has been a boost in demand for housing due to the\nexpanding labor market caused by the growth of tech companies in the\narea. This increase in demand is expected to cause an increase in house\nvalues when the supply gradually becomes insufficient.\nWe hypothesize that there are different degrees of growth for tech\ncompanies, and the census tracts with higher growth companies will see\nmore drastic increases in house prices. In order to fulfill our research\nquestion, we set out to segment the census tracts into different groups\nand look that them in relation to different socioeconomic indicators to\nsee how they impact house prices.\nData Processing\nTo prepare the data for our analysis, we combined the two datasets,\ni.e. tech company data and the Bay Area data, into one big dataset using\ntheir longitude and latitude information. The Bay Area data already has\nspatial information, with longitudinal and latitude, while the company\ndata does not. We tackled this by using the geocode package to obtain\nthe longitudinal and latitude information of the companies based on\ntheir addresses. Specifically, geocode takes the companies’ addresses\nand adds the corresponding longitude and latitude to the existing table.\nAfter that, we transformed the coordinate system of the company data to\nmatch that of the Bay Area dataset as these two datasets have different\ntypes of coordinate systems. This way, we confirmed we could combine the\ndata. Next, we aggregate the data by matching each company with the\ncensus tracts located within its 2000 meters radius. We then computed\nthe overall EBITDA growth percentage in the census tracts that are in\nthe 2000-meter proximity to the tech companies between 2009 and\n2019.\nWe separated the census tracts into four different groups based on\ntheir yearly EBITDA growth and used different resources to determine the\ncut-off points [@Geckoboard_2022;@BalboaCapital_2022].\nCensus tracts that have yearly EBITDA growth greater than 50% are the\n“High Growth” tracts. “Medium Growth” tracts have EBITDA growth ranging\nfrom 20% to 50%, and the figure for “Low Growth” tracts is under 20%.\nCensus tracts that are not within 2000 meters radius of tech companies\nbelong to the “Control” group. For the rest of our paper, we’ll refer to\nthese different growth groups with the understanding that these growth\ngroups are defined by the EBITDA growth of tech companies.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom this graph, we can see that the house prices across the 4 growth\nsegments follow the same trend. However, after 2014, there are some\ndrastic changes. The line representing the median house price in high\ngrowth areas is steepest, followed by medium growth and high growth.\nThis potentially reflects the fact that the high growth of tech\ncompanies had a larger effect on house values. High-growth,\nmedium-growth, and low-growth groups have higher house prices than the\ncontrol group, which is consistent with our prior beliefs that the\npresence of tech companies correlates with house values. The fact that\nareas with low tech growth present higher prices for years between 2013\nand 2017 could be caused by different factors like increases in the\nsupply of houses in medium and high growth areas. Also, during that\ntime, there were probably many emerging startups, which required more\nspace for offices within the Silicon Valley, which potentially generated\nhouse shortages and boosted the price.\nVariable consideration for\nthe model\nAfter conducting an analysis of the correlation between the different\npotential factors and Bay Area median house prices, the factors that\npresent strong correlations with the outcome are income, age, household\nsize, the proportion of people whose birthplace was in a state, the\nproportion of people in the information industry, finance, professional,\nand other industries. The proportion of white and black populations also\nhad a considerable correlation with median house value. As the analysis\nfocuses more on causal inference than prediction, we won’t omit any\nconfounding variables. It is important to mention that we are including\nindependent variables that are moderately correlated with each\nother.\nWe also include county information in the data to control for the\neffect on housing. The reason is that census tracts that are in the same\ncounty tend to have similar house prices, which makes county a\nreasonable indicator. We hypothesize that, due to the spillover effect\nfrom the thriving tech companies, counties with higher EBITDA growth\nwill have a larger house value compared to those with lower EBITDA\ngrowth. Additionally, higher incomes and population growth increase the\ndemand for housing as people can afford larger mortgages and there is a\nhigher demand for housing [@Pettinger_2021].\n\n\n\n\n\n\nGeneralized\nEstimating Equation (GEE) Model\nWe chose GEE because it allows us to work with different correlation\nstructures. In other words, we are able to test different assumptions\nregarding the correlation magnitude in our longitudinal data. Another\noption is a mixed-effects model, which we consider quite simplistic due\nto its assumption of exchangeable correlation. In other words, it is\nunrealistic to assume that longitudinal data has a constant correlation\nindependent of the difference in years.\nFurthermore, we consider that using GEE is advantageous as it has\nRobust Standard Errors (SE), which is independent of the working\ncorrelation structure and is close to the real SE. This allowed us to\ncompare the Model SE to the Robust SE when evaluating and choosing the\nbest model.\nCorrelation structure is a crucial component of the GEE model. When\nwe set out to model the median house value, we were concerned about\nfinding an appropriate correlation structure for the data we have. A\ncorrelation structure indicates our assumption for the magnitude at\nwhich the data correlate over time.\nTherefore, we created different models to test different correlation\nstructures and variables potentially impacting house values. In the\nfirst model, we used the exchangeable correlation structure, which\nassumes that the correlation between different EBITDA growth groups is\nconstant and that the correlation for each EBITDA group does not change\nover the years. We don’t think that this assumption is realistic in the\ncontext of our analysis. In fact, we believe that the correlation among\nthe observations varies across the years. Specifically, given the\neffects of the 2008 economic crisis on the housing market, we would\nexpect house values between the years 2009 and 2010 to be more similar\nthan those between 2009 and 2018.\nIn our final model, we used a correlation structure that shows an\nexponential decay to 0 with time in the correlation of observations.\nThis correlation structure is known as an Autoregression Model of order\n1 (AR1). We also chose the set of variables that would shed light on the\nchanges in house prices. We transformed some of the variables to make\nthem more concise. For our outcome variable, we decided to divide the\nmedian house values by 100,000 to decrease the magnitude of the data and\nmake computations easier. Similarly, we also reduced the magnitude of\nthe income variable by 10,000. We included the indicators for household\nsize, income, the proportion of people born in California, the\nproportion of white people, and county variables. With this set of\nvariables, we aimed to explain how tech growth and socioeconomic changes\nimpact house values.\nWe carry out a hypothesis test on the estimated coefficients in the\nGEE model to see whether there is a powerful relationship between the\nchosen factors and house values over the years. We conclude that the\nfactors contribute significantly to explaining house values, indicated\nwith a p-value lower than 0.05. Specifically, there is in fact a 0%\nchance that we would observe the data as we have if there was no\ncorrelation at all between the variables and the house value\noutcome.\n\n\n\n\n\n\n\n\n\nEvaluating\nthe GEE model through its residual plot\nA residual plot shows whether the model underpredicts or overpredicts\nhouse prices in different areas of the Bay Area. While white color shows\nthat the actual house prices are similar to the model predictions, red\nindicates areas that have higher actual house prices, and blue shows\nareas with lower actual house prices than the model predictions. Looking\nat the residual plot, we can see that the actual house values in the\ncensus tracts around the Silicon Valley are much higher than what the\nGEE model predicts. On the other hand, houses in bigger census tracts\nhave much lower values than the model estimates. From these results, we\nbelieve that there is some degree of spatial correlation that cannot be\naccounted for by a longitudinal model. In other words, the GEE model\ndoes not take into account how the median house price in a certain\ncensus tract can be influenced by the prices in the neighboring census\ntracts.\n\n\n\nIn order to verify this spatial relationship between the residuals\nafter predicting using the GEE model, we conduct the Moran’s I test. We\nfound that the residuals are spatially correlated, indicated by the\np-value smaller than 0.05 in the test (p-value = \\(2.2 {*} 10^{-16}\\)). In other words, there\nis almost a 0% chance that we would observe the actual median house\nprices like in the data if the residuals were not spatially\ncorrelated.\nTherefore, we decide to create a spatial model to account for the\nspatial correlation that is unexplained by the GEE longitudinal\nmodel.\nSpatial Analysis\nIntroduction to\nNeighborhood Structure\nCensus tracts that are closer to one another might have similar house\nprices. For example, census tracts that are in counties closer to tech\ncompanies like San Mateo and Santa Clara will be more similar compared\nto those in Marin which is much further from the tech hub. Therefore, it\nis important to statistically identify groups of census tracts that are\ncloser, or are “neighbors”, to the others. In order to identify groups\nof neighbors, we use a concept called neighborhood structure, which\nhelps us establish how close the census tracts are to one another. There\nare many techniques to define neighborhood structure, but here we used a\nmethod called K Nearest Neighbors (KNN), which calculates the distance\namong the centers of the census tracts and groups the census tracts into\ndifferent groups of k neighbors.\nWe chose the KNN neighborhood structure as we want to restrict the\nnumber of neighbors in each group. We believe that this method is\nvaluable in the sense that it creates groups of neighboring census\ntracts, each with k neighbors, and hence contributes to the\ngeneralization ability of the model. We choose 3 as the number of\nneighbors each census tract can have as it helps prevent the very\ncongested network for the counties closer to the city. In other words,\nit helps eliminate the problem of census tracts having too few or too\nmany neighbors.\nSpatial Autoregressive (SAR)\nModel\nWe use Spatial Autoregressive Models (SAR) to model the spatial\ncorrelation for the leftover residual from the linear regression model.\nThe SAR model reflects a stronger pattern of spatial covariance than CAR\n(when using the same parameter and weights), and it decays slower than\nthe CAR model. The SAR model is also better than the CAR model, as\nindicated by the BIC indices of the SAR and CAR fitted models. The BIC\nindex is the goodness-of-fit measure, and it shows how well the model\nfits the data.\nFrom the longitudinal analysis, we know that there are tech growth\nand socioeconomic factors affecting house values over time. Our goal for\nspatial analysis is to account for spatial correlation in a specific\nyear. As spatial techniques cannot be conducted on data spanning\ndifferent time periods, we choose to perform spatial analysis on 2018\ndata only.\nAfter using SAR to model the data, we ran the Moran’s I test again\nand found that the leftover errors after SAR are independent. In other\nwords, the SAR model has captured the possible spatial correlation, as\nindicated by the Moran’s I statistics, which is nearly 0, and the\np-value greater than 0.05 (p-value = 0.1489). Therefore, not only there\nis a longitudinal effect on house prices over the years in Bay Area\ncensus tracts but there is also a spatial correlation in each year, as\nwe have seen for 2018.\nEvaluating\nthe SAR model through its residual plot\nThe residual plot for the SAR model shows that there are some\ncounties whose house values are underestimated or overestimated by the\nmodel, indicated by the darker red and blue colors. Specifically, we can\nsee very large residuals in the San Mateo and Santa Clara counties as\nwell as larger counties on the leftmost and rightmost sides of the\nmap.\nWe carry out further exploration of the area using Google Maps and\ndiscover some terrain characteristics that can explain the high\nmagnitude of difference in the residuals. For example, there are two\nareas whose residuals are still high even after the GEE and SAR models\nthat are located along the inner coast of San Mateo County, as indicated\nby the deep blue color. The large residuals turn out to be reasonable as\nthese two areas are not residential areas. In other words, one of them\nis Foster City, which is an industrial area, and the other area is the\nhome of national parks. The houses in these areas might be undesirable\ndue to their proximity to industrial/manufacturing areas or pure nature\nreserves and no business or recreational endeavors. In addition, the\norange-color census tracts on the top left corner and lower right corner\nof the map show a different trend. Using Google Maps, we can see that\nthese two areas are the home of several national and local parks. As the\norange color indicates that the actual house values in these areas are\nhigher than SAR predictions, there might be luxury resorts or private\nholiday properties that might spike the house prices. For houses around\nthe Silicon Valley area that are also of an orange shade, the higher\nactual prices can be explained by higher demand given that it is close\nto big tech companies with high wealth and large labor forces. This\nmakes the area appealing to not only workers but also other big tech\ncompanies, emerging tech startups, and venture capital funds.\n\n\n\n\n\n\nResults and Discussion\nIn our analysis, we found evidence suggesting that house values grow\ndifferently for different growth segments post-2014. Especially for the\nhigh growth segment, the increase in the house prices outweighs the\nincrease in the medium and low growth groups as well as control areas.\nSuch drastic changes can be the result of the market recovery post-2018\neconomic downturn as well as the technology innovations and growth\nhappening in the area [@Carlisle_2021]. This is consistent\nwith our hypothesis that higher company growth is associated with higher\nhouse values. Aside from the longitudinal effect of tech growth and\nother socioeconomic factors on Bay Area house prices over the years,\nthere is also a spatial correlation in each year, meaning that house\nprices in neighboring areas of a census tract can inform the house price\nof that area.\nIn addition, income and whether the census tract has a majority white\npopulation are positively correlated with higher house values. Perhaps\nthis is because higher income allows people to afford more expensive\nhouses. Also, the higher the proportion of white residents in a census\ntract, the higher house values are likely to be. This could be due to\nthe effect of redlining, where people of color have been historically\ndiscriminated against in the housing market.\nOn the other hand, the median household size in a census tract and\nwhether there is a high proportion of people born in California\nnegatively correlate with house prices. Such a trend in household size\ncan be explained in terms of race and age. White households are usually\nsmaller in size, while people of color households tend to be bigger due\nto living with extended families or having more children. Also,\nimmigrants are more likely to live with extended families who immigrate\nwith them. Regarding the negative effects of the proportion of\nCalifornian residents, that can be explained by foreign money. If there\nis a higher proportion of people from outside of California coming to\nthe area, there will be more wealth flowing into the area, potentially\nthrough tech investments from other states or countries.\nFinally, the house prices of nearby census tracts impact one another.\nWe can see that Silicon Valley and the neighboring census tracts\nexperience relatively similar house values. Other residential census\ntracts that are further away or in a county that is not affiliated with\nbig techs like Marin or Contra Costa experience much lower house\nprices.\nMoving forward, there are many directions we can take to improve the\nanalysis. Firstly, we can consider using Uber data to determine the\ndistance between different census tracts and tech companies in terms of\nminutes. As we hypothesize that house prices are in high demand as\nhigh-income tech employees want to live closer to their companies, this\ncan allow us to identify which areas can be desirable for tech\nemployees. Secondly, natural terrain can be a factor in determining\nhouse prices. As we have mentioned, areas with national parks or right\nbeside mountains might have cheaper houses; therefore, incorporating\nnatural geographic data into the analysis can improve our models.\nThirdly, we hope to combine our longitudinal and spatial models into one\nas we potentially have to write a new library for the joined model.\nFinally, we would like to control for interest rates, considering that\nlower interest rates could lead to higher demand for housing due to\nbetter mortgage deals.\nAdditionally, as our ultimate goal is to understand the house prices\nin emerging tech hubs like Austin, Texas, and Salt Lake City, Utah, we\nwant to obtain similar data in these areas to verify the insights in\nthis analysis and potentially make a predictive model for these\nareas.\nOne limitation to our census data is that there is a ceiling, or\nmaximum, value for house prices. Before 2014, the house value is capped\nat $1M, whereas the cap is $2M after 2014.\nThere might have been some wording changes on the census survey, which\naffects the Bay Area census data.\nAcknowledgements\nWe would love to thank our amazing Prof. Brianna Heggeseth for her\nguidance and inspiration for this project. We love you, Brianna!\nReferences\n\n\n\n",
    "preview": "posts/2022-06-13-realestate/housevalue.png",
    "last_modified": "2025-12-08T11:05:35-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-05-07-accidentbehind/",
    "title": "Behind Scenes: Prediction of the severity of car accidents",
    "description": "In depth analysis of modelling decisions, coding and recommendations when approaching car accidents prediction.",
    "author": [
      {
        "name": "Juthi Dewan, Coco Li, Franco Salinas",
        "url": {}
      }
    ],
    "date": "2021-05-07",
    "categories": [],
    "contents": "\n\n\nlibrary(data.table)\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tidyverse)         # for reading in data, graphing, and cleaning\nlibrary(tidymodels)        # for modeling ... tidily\nlibrary(glmnet)            # for regularized regression, including LASSO\nlibrary(naniar)            # for examining missing values (NAs)\nlibrary(lubridate)         # for date manipulation\nlibrary(moderndive)        # for King County housing data\nlibrary(vip)               # for variable importance plots\nlibrary(rmarkdown)         # for paged tables\nlibrary(themis)            # for step functions for unbalanced data\nlibrary(stacks)            # for stacking models\nlibrary(DALEX)             # for model interpretation  \nlibrary(DALEXtra)          # for extension of DALEX\nlibrary(patchwork)         # for combining plots nicely\nlibrary(scales)\nlibrary(plotly)\nlibrary(gridExtra)\nlibrary(tidytext)\nlibrary(modelr)\nlibrary(caret)\nlibrary(ROSE)\nlibrary(glmnet)\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(randomForest)\nlibrary(shiny)\nlibrary(bslib)\noptions(warn = -1)\ntheme_set(theme_minimal()) # my favorite ggplot2 theme :)\n\n\n\n\n\ncars <- read_csv(\"small_accidents.csv\", col_types = cols(.default = col_character())) %>%\n  type_convert()\n\ncars %>%\n  group_by(City) %>%\n  summarize(Count=n()) %>%\n  arrange(desc(Count)) %>%\n  head(1000)\n\n\n# A tibble: 1,000 x 2\n   City        Count\n   <chr>       <int>\n 1 Houston      9612\n 2 Los Angeles  7771\n 3 Charlotte    7435\n 4 Dallas       6545\n 5 Austin       5832\n 6 Miami        5207\n 7 Raleigh      4420\n 8 Atlanta      3760\n 9 Orlando      3300\n10 Sacramento   3150\n# … with 990 more rows\n\nIntroduction\nThe Fatality Analysis Reporting System indicated that an estimate of 8870 people died in motor vehicle traffic crashes in the second quarter of 2020 (NHTSA, 2020). This analysis is intended to bring light to the main environmental conditions that are associated with the severity of a car accident. For the purpose of this study we defined severity as the accident’s impact on traffic.\nData\nThe data we used has 47 variables and 3 million observations for different car accidents. The data was collected from February 2016 to December 2020 for the 49 states of the US. The data base has been constructed partly by Moosavi, Sobhan, Mohammad Hossein Samavatian, Srinivasan Parthasarathy, and Rajiv Ramnath as “A Countrywide Traffic Accident Dataset” (2019). The other part of the data base was constructed by Moosavi, Sobhan, Mohammad Hossein Samavatian, Srinivasan Parthasarathy, Radu Teodorescu, and Rajiv Ramnath for their database “Accident Risk Prediction based on Heterogeneous Sparse Data: New Dataset and Insights.”\nModels\nUsing this data set, we predicted the Severity of an accident using stacked LASSO, Forest and classification three. Stacking combines predictions from many different models into a “super” predictor. In this case we would be averaging the predictions of the LASSO, Forest and classification three.\nPre-processing\n\n\ncars %>% summarise_all(~ mean(is.na(.))) %>%\n  pivot_longer(1:49, names_to = \"Variables to drop\", values_to = \"NA proportion\") %>%\n  filter(`NA proportion` >= 0.5)\n\n\n# A tibble: 3 x 2\n  `Variables to drop` `NA proportion`\n  <chr>                         <dbl>\n1 End_Lat                       0.636\n2 End_Lng                       0.636\n3 Number                        0.633\n\ndrop_na_cols <- c(\"End_Lat\", \"End_Lng\", \"Number\")\n\nnot_useful <- c(\"ID\", \"Source\", \"Timezone\", \"Airport_Code\", \"Weather_Timestamp\",\"Wind_Direction\", \"Description\", \"Bump\", \"Traffic_Calming\", \"Give_Way\", \"No_Exit\", \"Railway\", \"Roundabout\", \"Station\", \"Stop\", \"Amenity\", \"Street\", \"Zipcode\", \"Country\", \"Turning_Loop\", \"County\", \"TMC\")\n\n\ntraffic <-\n  cars %>%\n  select(-all_of(drop_na_cols), -all_of(not_useful))\n\np1 <- ggplot(cars, aes(as.factor(Station), ..prop.., group = Severity)) +\n  geom_bar(aes(fill = Severity), position = \"dodge\") +\n  scale_y_continuous(labels = percent) +\n  theme(axis.text.x = element_text(angle = 60, vjust = 0.6))\n \np2 <-  ggplot(cars, aes(Turning_Loop, ..prop.., group = Severity)) +\n  geom_bar(aes(fill = Severity), position = \"dodge\") +\n  scale_y_continuous(labels = percent) +\n  theme(axis.text.x = element_text(angle = 60, vjust = 0.6))\n\np3 <- ggplot(cars, aes(Country, ..prop.., group = Severity)) +\n  geom_bar(aes(fill = Severity), position = \"dodge\") +\n  scale_y_continuous(labels = percent) +\n  theme(axis.text.x = element_text(angle = 60, vjust = 0.6))\n\np4 <- ggplot(cars, aes(Amenity, ..prop.., group = Severity)) +\n  geom_bar(aes(fill = Severity), position = \"dodge\") +\n  scale_y_continuous(labels = percent) +\n  theme(axis.text.x = element_text(angle = 60, vjust = 0.6))\n\np5 <- ggplot(cars, aes(Stop, ..prop.., group = Severity)) +\n  geom_bar(aes(fill = Severity), position = \"dodge\") +\n  scale_y_continuous(labels = percent) +\n  theme(axis.text.x = element_text(angle = 60, vjust = 0.6))\n\np6 <- ggplot(cars, aes(Station, ..prop.., group = Severity)) +\n  geom_bar(aes(fill = Severity), position = \"dodge\") +\n  scale_y_continuous(labels = percent) +\n  theme(axis.text.x = element_text(angle = 60, vjust = 0.6))\n\np7 <- ggplot(cars, aes(Roundabout, ..prop.., group = Severity)) +\n  geom_bar(aes(fill = Severity), position = \"dodge\") +\n  scale_y_continuous(labels = percent) +\n  theme(axis.text.x = element_text(angle = 60, vjust = 0.6))\n\np8 <- ggplot(cars, aes(Railway, ..prop.., group = Severity)) +\n  geom_bar(aes(fill = Severity), position = \"dodge\") +\n  scale_y_continuous(labels = percent) +\n  theme(axis.text.x = element_text(angle = 60, vjust = 0.6))\n\np9 <- ggplot(cars, aes(No_Exit, ..prop.., group = Severity)) +\n  geom_bar(aes(fill = Severity), position = \"dodge\") +\n  scale_y_continuous(labels = percent) +\n  theme(axis.text.x = element_text(angle = 60, vjust = 0.6))\n\np10 <- ggplot(cars, aes(Give_Way, ..prop.., group = Severity)) +\n  geom_bar(aes(fill = Severity), position = \"dodge\") +\n  scale_y_continuous(labels = percent) +\n  theme(axis.text.x = element_text(angle = 60, vjust = 0.6))\n\np11 <- ggplot(cars, aes(Traffic_Calming, ..prop.., group = Severity)) +\n  geom_bar(aes(fill = Severity), position = \"dodge\") +\n  scale_y_continuous(labels = percent) +\n  theme(axis.text.x = element_text(angle = 60, vjust = 0.6))\n\n\np12 <- ggplot(cars, aes(Bump, ..prop.., group = Severity)) +\n  geom_bar(aes(fill = Severity), position = \"dodge\") +\n  scale_y_continuous(labels = percent) +\n  theme(axis.text.x = element_text(angle = 60, vjust = 0.6))\n\n\np1+ p2+ p3+ p4\n\n\n\np5+ p6+ p7+ p8\n\n\n\np9+ p10+ p11+ p12\n\n\n\n\nWe chose to not use multiple of the 47 variables that we considered weren’t relevant to the analysis we were conducting. As we can see in the table, end latitude, end longitude’s proportion of NA values are higher than 50%. Given the distribution of wind direction through the different severity levels, we decided that the variable is uninformative. The description, bump, traffic calming, give way, no exit, railway, roundabout, station, stop, amenity, street, zip code, country, turning loop, county and TMC code weren’t informative either given the distribution between the severity categories or near-zero variance.\n\n\ntraffic <-  traffic %>%\n  rename(\"Distance\" = `Distance(mi)`, \"Temperature\" = `Temperature(F)`, \"Humidity\" = `Humidity(%)`,\n         \"Pressure\" = `Pressure(in)`, \"Visibility\" = `Visibility(mi)`, \"Wind_Speed\" = `Wind_Speed(mph)`, \"Precipitation\" = `Precipitation(in)`, \"Wind_Chill\" = `Wind_Chill(F)`)\n\ntraffic$Severity <- as.character(traffic$Severity)\n\ntraffic <-\n  traffic %>%\n  add_n_miss() %>%\n  filter(n_miss_all == 0) %>%\n  select(-n_miss_all)\n\ntraffic <- traffic %>%\n  mutate(\"Status\" = factor(ifelse(Severity == \"3\" | Severity == \"4\", \"Severe\", \"Not Severe\"),\n                           levels = c(\"Not Severe\", \"Severe\")))\n\n\n\nIn this section of data cleaning and pre-processing above, we renamed some of the variables that had their units of measurement for ease of use later in our modeling and for our shinyApp. We took out entries of data that had NAs. If this was a smaller dataset, this may have negatively impacted our analysis, but we don’t believe this effected our analysis for this project because even after taking out some data, we had a lot left to work with. Another very important part of this section is that, we took the Severity variable that we are looking at and split it into two to make it into a Categorical variable called Status. Severities 1 and 2 were grouped in to be Not Severe and 3 and 4 were grouped as Severe in the Status variable.\n\n\n\ntraffic_time <- traffic %>%\n  mutate(Duration = (End_Time - Start_Time)) %>%\n  # accident duration should be positive\n  filter(!(Duration < 0)) %>%\n  separate(Start_Time, into = c(\"Date\", \"Time\"), sep = \" \") %>%\n  mutate(\"Year\" = str_sub(Date, 1, 4), \"Month\" = str_sub(Date, 6, 7), \"Day\" = str_sub(Date, 9, 10),\n         \"Wday\" = as.character(wday(Date))) %>%\n  mutate(\"Hour\" = str_sub(Time,1,2)) %>%\n  select(-c(\"Date\", \"Time\", \"End_Time\")) %>%\n  select(Severity, Year, Month, Day, Hour, Wday, Duration, everything())\n\n\n\nIn this section, we used the End_Time and Start_Time variables to come up with several other variables such as Duration, Date, Time, Year, Month, Day, Wday and Hour.\n\n\n#Drop levels that have less than 20 observations\nweather_to_drop <-\n  traffic_time %>%\n    count(Weather_Condition) %>%\n    filter(n < 20) %>%\n    select(Weather_Condition)\n\nweather_to_drop <-\n  weather_to_drop$Weather_Condition %>%\n    unlist()\n\ntraffic_weather <- traffic_time %>%\n  filter(!(Weather_Condition %in% weather_to_drop)) %>%\n  mutate(Weather_Condition = factor(Weather_Condition))\n\ntraffic2 <- traffic_weather\n\ncount_city <- traffic2 %>%\n  group_by(City) %>%\n  summarize(Count=n()) %>%\n  arrange(desc(Count)) %>%\n  head(950)  \n\ntraffic3 <-\n  traffic2 %>%\n    left_join(count_city, by=\"City\")\n\ntraffic_final <-\n  traffic3 %>%\n  add_n_miss() %>%\n  filter(n_miss_all == 0) %>%\n  select(-n_miss_all) %>%\n  select(-Count)\n\n#write.csv(traffic_final, \"traffic_final.csv\", row.names = FALSE)\n\n\n\n\n\nread_csv(\"traffic_final.csv\")\n\n\n# A tibble: 98,088 x 29\n   Severity  Year Month Day   Hour   Wday Duration Start_Lat Start_Lng\n      <dbl> <dbl> <chr> <chr> <chr> <dbl>    <dbl>     <dbl>     <dbl>\n 1        2  2016 12    07    23        4     44.4      38.6     -121.\n 2        2  2016 12    08    09        5     29.6      38.4     -123.\n 3        2  2016 12    23    09        6     29.7      38.3     -123.\n 4        2  2017 01    02    19        2     29.7      39.3     -121.\n 5        2  2017 01    22    15        1     44.6      38.1     -122.\n 6        2  2017 01    23    20        2     29.6      37.2     -122.\n 7        2  2017 01    23    22        2     29.8      37.4     -122.\n 8        2  2017 01    25    06        4     44.6      38.0     -122.\n 9        2  2016 11    30    08        4     70.0      38.7     -122.\n10        2  2016 06    21    16        3     60        34.4     -119.\n# … with 98,078 more rows, and 20 more variables: Distance <dbl>,\n#   Side <chr>, City <chr>, State <chr>, Temperature <dbl>,\n#   Wind_Chill <dbl>, Humidity <dbl>, Pressure <dbl>,\n#   Visibility <dbl>, Wind_Speed <dbl>, Precipitation <dbl>,\n#   Weather_Condition <chr>, Crossing <lgl>, Junction <lgl>,\n#   Traffic_Signal <lgl>, Sunrise_Sunset <chr>, Civil_Twilight <chr>,\n#   Nautical_Twilight <chr>, Astronomical_Twilight <chr>,\n#   Status <chr>\n\nUpon close examination of the Weather_Conditions variable, we realized that it had a lot of different observations and we wanted to narrow it down. So, we filtered and dropped levels that have less than 20 observations. We had to narrow down the City variable as well. We have over 4000 distinct cities under the top 12 states. Shiny only allows a thousand different observations and so in order for all the top cities to fit in our shiny app, we had to narrow down the list of cities to the top 950.\n\n\n#modeling pre-process for traffic_final\n\ntraffic_mod <- traffic_final %>%\n  mutate(Status = as.factor(Status)) %>%\n  mutate(across(where(is.character), as.factor)) %>%\n  select(-c(State, Severity, Year, Day)) %>%\n  # select(-arrival_date_year,\n  #        -reservation_status,\n  #        -reservation_status_date) %>%\n  add_n_miss() %>%\n  filter(n_miss_all == 0) %>%\n  select(-n_miss_all)\n\n\ntraffic_mod$Crossing <- as.factor(traffic_mod$Crossing)\ntraffic_mod$Month <- as.numeric(traffic_mod$Month)\ntraffic_mod$Wday <- as.numeric(traffic_mod$Wday)\ntraffic_mod$Hour <- as.numeric(traffic_mod$Hour)\ntraffic_mod$Duration <- as.numeric(traffic_mod$Duration)\ntraffic_mod$Junction <- as.factor(traffic_mod$Junction)\ntraffic_mod$Traffic_Signal <- as.factor(traffic_mod$Traffic_Signal)\n \n\nset.seed(494) #for reproducibility\n\n# Randomly assigns 75% of the data to training.\ntraffic_split <- initial_split(traffic_mod,\n                             prop = .50)\ntraffic_split\n\n\n<Analysis/Assess/Total>\n<49044/49044/98088>\n\ntraffic_training <- training(traffic_split)\ntraffic_testing <- testing(traffic_split)\n\n\n\nHere, we get the data ready for the modeling part by taking the non-predictive variables out of our data set, and converting the predictors’ data type into the correct type. After that, we split our data into testing and training data according to a 50 percentage split.\n\n\n#lasso\nset.seed(494)\n\nlasso_recipe <-\n  recipe(Status ~ .,\n         data = traffic_training) %>%\n  # step_mutate(County,\n  #              County = fct_lump_n(County, n = 5)) %>%\n   step_mutate(City,\n               City = fct_lump_n(City, n = 5)) %>%\n  step_normalize(all_predictors(),\n                 -all_nominal(),\n                 -all_outcomes()) %>%\n  step_dummy(all_nominal(),\n             -all_outcomes())\n\nlasso_recipe %>%\n  prep() %>%\n  juice()\n\n\n# A tibble: 49,044 x 64\n    Month    Hour    Wday Duration Start_Lat Start_Lng Distance\n    <dbl>   <dbl>   <dbl>    <dbl>     <dbl>     <dbl>    <dbl>\n 1  1.21   1.86   -0.0289  -0.0345     0.715     -1.31   -0.252\n 2  1.21  -0.527   0.536   -0.0399     0.682     -1.38   -0.252\n 3  1.21  -0.527   1.10    -0.0399     0.665     -1.38   -0.260\n 4 -1.93   0.495  -1.72    -0.0344     0.621     -1.36   -0.252\n 5 -1.93   1.69   -1.16    -0.0398     0.464     -1.34   -0.252\n 6 -0.504 -0.527  -1.16    -0.0366    -0.161     -1.15   -0.260\n 7 -0.219 -1.04    1.67    -0.0342    -0.155     -1.15   -0.260\n 8 -0.219 -1.89   -0.594   -0.0287    -0.141     -1.16   -0.260\n 9 -0.219 -0.0158 -0.0289  -0.0368    -0.178     -1.17   -0.260\n10 -0.219  1.35   -0.0289  -0.0398    -0.179     -1.16   -0.260\n# … with 49,034 more rows, and 57 more variables: Temperature <dbl>,\n#   Wind_Chill <dbl>, Humidity <dbl>, Pressure <dbl>,\n#   Visibility <dbl>, Wind_Speed <dbl>, Precipitation <dbl>,\n#   Status <fct>, Side_R <dbl>, City_Dallas <dbl>,\n#   City_Houston <dbl>, City_Los.Angeles <dbl>, City_Miami <dbl>,\n#   City_Other <dbl>, Weather_Condition_Cloudy <dbl>,\n#   Weather_Condition_Cloudy...Windy <dbl>,\n#   Weather_Condition_Drizzle <dbl>, Weather_Condition_Fair <dbl>,\n#   Weather_Condition_Fair...Windy <dbl>,\n#   Weather_Condition_Fog <dbl>, Weather_Condition_Haze <dbl>,\n#   Weather_Condition_Heavy.Rain <dbl>,\n#   Weather_Condition_Heavy.Rain...Windy <dbl>,\n#   Weather_Condition_Heavy.Snow <dbl>,\n#   Weather_Condition_Heavy.T.Storm <dbl>,\n#   Weather_Condition_Light.Drizzle <dbl>,\n#   Weather_Condition_Light.Freezing.Rain <dbl>,\n#   Weather_Condition_Light.Rain <dbl>,\n#   Weather_Condition_Light.Rain...Windy <dbl>,\n#   Weather_Condition_Light.Rain.with.Thunder <dbl>,\n#   Weather_Condition_Light.Snow <dbl>,\n#   Weather_Condition_Light.Snow...Windy <dbl>,\n#   Weather_Condition_Mist <dbl>,\n#   Weather_Condition_Mostly.Cloudy <dbl>,\n#   Weather_Condition_Mostly.Cloudy...Windy <dbl>,\n#   Weather_Condition_N.A.Precipitation <dbl>,\n#   Weather_Condition_Overcast <dbl>,\n#   Weather_Condition_Partly.Cloudy <dbl>,\n#   Weather_Condition_Partly.Cloudy...Windy <dbl>,\n#   Weather_Condition_Patches.of.Fog <dbl>,\n#   Weather_Condition_Rain <dbl>,\n#   Weather_Condition_Rain...Windy <dbl>,\n#   Weather_Condition_Scattered.Clouds <dbl>,\n#   Weather_Condition_Shallow.Fog <dbl>,\n#   Weather_Condition_Smoke <dbl>, Weather_Condition_Snow <dbl>,\n#   Weather_Condition_T.Storm <dbl>, Weather_Condition_Thunder <dbl>,\n#   Weather_Condition_Thunder.in.the.Vicinity <dbl>,\n#   Weather_Condition_Wintry.Mix <dbl>, Crossing_TRUE. <dbl>,\n#   Junction_TRUE. <dbl>, Traffic_Signal_TRUE. <dbl>,\n#   Sunrise_Sunset_Night <dbl>, Civil_Twilight_Night <dbl>,\n#   Nautical_Twilight_Night <dbl>, Astronomical_Twilight_Night <dbl>\n\nlasso_mod <-\n  logistic_reg(mixture = 1) %>%\n  set_engine(\"glmnet\") %>%\n  set_args(penalty = tune()) %>%\n  set_mode(\"classification\")\n\nlasso_wf <-\n  workflow() %>%\n  add_recipe(lasso_recipe) %>%\n  add_model(lasso_mod)\n\nset.seed(494) #for reproducible 5-fold\ntraffic_cv <- vfold_cv(traffic_training,\n                       v = 5)\n\npenalty_grid <- grid_regular(penalty(),\n                             levels = 10)\n\n# add ctrl_grid - assures predictions and workflows are saved\nctrl_grid <- control_stack_resamples()\n\nmetric <- metric_set(accuracy)\n\n# tune the model\nlasso_tune <-\n  lasso_wf %>%\n  tune_grid(\n    resamples = traffic_cv,\n    grid = penalty_grid,\n    control = ctrl_grid\n    )\n\nlasso_tune %>%\n  collect_metrics()\n\n\n# A tibble: 20 x 7\n       penalty .metric  .estimator  mean     n std_err .config        \n         <dbl> <chr>    <chr>      <dbl> <int>   <dbl> <chr>          \n 1    1.00e-10 accuracy binary     0.808     5 1.52e-3 Preprocessor1_…\n 2    1.00e-10 roc_auc  binary     0.738     5 7.72e-4 Preprocessor1_…\n 3    1.29e- 9 accuracy binary     0.808     5 1.52e-3 Preprocessor1_…\n 4    1.29e- 9 roc_auc  binary     0.738     5 7.72e-4 Preprocessor1_…\n 5    1.67e- 8 accuracy binary     0.808     5 1.52e-3 Preprocessor1_…\n 6    1.67e- 8 roc_auc  binary     0.738     5 7.72e-4 Preprocessor1_…\n 7    2.15e- 7 accuracy binary     0.808     5 1.52e-3 Preprocessor1_…\n 8    2.15e- 7 roc_auc  binary     0.738     5 7.72e-4 Preprocessor1_…\n 9    2.78e- 6 accuracy binary     0.808     5 1.52e-3 Preprocessor1_…\n10    2.78e- 6 roc_auc  binary     0.738     5 7.72e-4 Preprocessor1_…\n11    3.59e- 5 accuracy binary     0.808     5 1.53e-3 Preprocessor1_…\n12    3.59e- 5 roc_auc  binary     0.738     5 7.75e-4 Preprocessor1_…\n13    4.64e- 4 accuracy binary     0.808     5 1.69e-3 Preprocessor1_…\n14    4.64e- 4 roc_auc  binary     0.737     5 9.58e-4 Preprocessor1_…\n15    5.99e- 3 accuracy binary     0.807     5 2.46e-3 Preprocessor1_…\n16    5.99e- 3 roc_auc  binary     0.730     5 2.33e-3 Preprocessor1_…\n17    7.74e- 2 accuracy binary     0.806     5 2.34e-3 Preprocessor1_…\n18    7.74e- 2 roc_auc  binary     0.5       5 0.      Preprocessor1_…\n19    1.00e+ 0 accuracy binary     0.806     5 2.34e-3 Preprocessor1_…\n20    1.00e+ 0 roc_auc  binary     0.5       5 0.      Preprocessor1_…\n\nbest_param <- lasso_tune %>%\n  select_best(metric = \"accuracy\")\nbest_param\n\n\n# A tibble: 1 x 2\n   penalty .config              \n     <dbl> <chr>                \n1 0.000464 Preprocessor1_Model07\n\nfinal_lasso <- lasso_wf %>%\n  finalize_workflow(best_param) %>%\n  fit(data = traffic_training)\n\nfinal_lasso %>%\n  pull_workflow_fit() %>%\n  tidy()\n\n\n# A tibble: 64 x 3\n   term         estimate  penalty\n   <chr>           <dbl>    <dbl>\n 1 (Intercept) -3.56     0.000464\n 2 Month       -0.353    0.000464\n 3 Hour         0.0587   0.000464\n 4 Wday        -0.000337 0.000464\n 5 Duration     0        0.000464\n 6 Start_Lat    0.135    0.000464\n 7 Start_Lng    0.462    0.000464\n 8 Distance     0.0946   0.000464\n 9 Temperature  0.224    0.000464\n10 Wind_Chill  -0.0209   0.000464\n# … with 54 more rows\n\nThe first model we build is a classification LASSO model, which selects the variables based on the magnitude of their coefficients. We tuned the LASSO model using a level 10 panelty grid, and selected the tuning parameter with the best prediction accuracy as the parameter for the final model. The accuracy for the best LASSO model is 80.956%, which means that the LASSO model predicts the right severity level 80.956% of the times.\n\n\n#classification rf\nset.seed(494)\n\nrf_recipe <-\n  recipe(Status ~ .,\n         data = traffic_training) %>%\n  step_mutate_at(all_numeric(),\n                 fn = ~as.numeric(.))\n\n\nrf_recipe %>%\n  prep() %>%\n  juice()\n\n\n# A tibble: 49,044 x 25\n   Month  Hour  Wday Duration Start_Lat Start_Lng Distance Side  City \n   <dbl> <dbl> <dbl>    <dbl>     <dbl>     <dbl>    <dbl> <fct> <fct>\n 1    12    24     4     44.4      38.6     -121.     0.01 R     Sacr…\n 2    12    10     5     29.6      38.4     -123.     0.01 R     Sant…\n 3    12    10     6     29.7      38.3     -123.     0    L     Seba…\n 4     1    16     1     44.6      38.1     -122.     0.01 R     Peta…\n 5     1    23     2     29.8      37.4     -122.     0.01 L     Sant…\n 6     6    10     2     38.6      34.4     -119.     0    R     Newh…\n 7     7     7     7     45        34.4     -119.     0    R     Vale…\n 8     7     2     3     60        34.5     -119.     0    L     Cast…\n 9     7    13     4     38.1      34.3     -119.     0    R     Simi…\n10     7    21     4     30        34.3     -119.     0    R     Simi…\n# … with 49,034 more rows, and 16 more variables: Temperature <dbl>,\n#   Wind_Chill <dbl>, Humidity <dbl>, Pressure <dbl>,\n#   Visibility <dbl>, Wind_Speed <dbl>, Precipitation <dbl>,\n#   Weather_Condition <fct>, Crossing <fct>, Junction <fct>,\n#   Traffic_Signal <fct>, Sunrise_Sunset <fct>, Civil_Twilight <fct>,\n#   Nautical_Twilight <fct>, Astronomical_Twilight <fct>,\n#   Status <fct>\n\nrf_model <-\n  rand_forest(mtry = tune(),\n              min_n = tune(),\n              trees = 10) %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"ranger\")\n\n\nrf_workflow <-\n  workflow() %>%\n  add_recipe(rf_recipe) %>%\n  add_model(rf_model)\n\n\nrf_penalty_grid <-\n  grid_regular(finalize(mtry(),\n                        traffic_training %>%\n                          select(-Status)),\n               min_n(),\n               levels = 3)\n\n\n# traffic_cv <- vfold_cv(traffic_training,\n#                        v = 5)\n\nrf_tune <-\n  rf_workflow %>%\n  tune_grid(\n    resamples = traffic_cv,\n    grid = rf_penalty_grid,\n    control = control_stack_grid()\n  )\n\nrf_tune %>%\n  collect_metrics()\n\n\n# A tibble: 18 x 8\n    mtry min_n .metric  .estimator  mean     n std_err .config        \n   <int> <int> <chr>    <chr>      <dbl> <int>   <dbl> <chr>          \n 1     1     2 accuracy binary     0.806     5 0.00225 Preprocessor1_…\n 2     1     2 roc_auc  binary     0.786     5 0.00307 Preprocessor1_…\n 3    12     2 accuracy binary     0.842     5 0.00184 Preprocessor1_…\n 4    12     2 roc_auc  binary     0.851     5 0.00291 Preprocessor1_…\n 5    24     2 accuracy binary     0.841     5 0.00267 Preprocessor1_…\n 6    24     2 roc_auc  binary     0.848     5 0.00259 Preprocessor1_…\n 7     1    21 accuracy binary     0.807     5 0.00210 Preprocessor1_…\n 8     1    21 roc_auc  binary     0.796     5 0.00191 Preprocessor1_…\n 9    12    21 accuracy binary     0.846     5 0.00172 Preprocessor1_…\n10    12    21 roc_auc  binary     0.868     5 0.00193 Preprocessor1_…\n11    24    21 accuracy binary     0.844     5 0.00125 Preprocessor1_…\n12    24    21 roc_auc  binary     0.862     5 0.00229 Preprocessor1_…\n13     1    40 accuracy binary     0.807     5 0.00210 Preprocessor1_…\n14     1    40 roc_auc  binary     0.788     5 0.00353 Preprocessor1_…\n15    12    40 accuracy binary     0.848     5 0.00105 Preprocessor1_…\n16    12    40 roc_auc  binary     0.872     5 0.00170 Preprocessor1_…\n17    24    40 accuracy binary     0.844     5 0.00189 Preprocessor1_…\n18    24    40 roc_auc  binary     0.866     5 0.00212 Preprocessor1_…\n\nAfter conducting the LASSO model, we also build the random forest model, which builds 10 trees and gives out the mode of the predictions of these 10 trees. We thought that this model might be more accurate than the LASSO model although it is also more computationally inefficient. We used a panelty grid of level 3 to tune our random forest model, and the tuning parameter with the largest accuracy is with mtry = 12 and min_n = 40. The largest accuracy is 84.693%, which is higher than the LASSO model.\n\n\n#decision trees\nset.seed(494)\n\ntree_model <-\n  decision_tree() %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"rpart\")\n\ntree_workflow <-\n  workflow() %>%\n  add_recipe(rf_recipe) %>%  \n  add_model(tree_model)\n\ntree_fit <-\n  tree_workflow %>%\n  fit_resamples(traffic_cv,\n                # metrics = metric,\n                control = control_stack_resamples()\n  )\n\ncollect_metrics(tree_fit)\n\n\n# A tibble: 2 x 6\n  .metric  .estimator  mean     n std_err .config             \n  <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n1 accuracy binary     0.824     5 0.00143 Preprocessor1_Model1\n2 roc_auc  binary     0.686     5 0.00309 Preprocessor1_Model1\n\nFinally, in order to create a stacked model, we build a third model which is just a simple classification decision tree. The accuracy for the decision tree model is 82.559%, which is also higher than the LASSO.\n\n\n# model stacking\nlasso_tune %>%\n  collect_metrics()\n\n\n# A tibble: 20 x 7\n       penalty .metric  .estimator  mean     n std_err .config        \n         <dbl> <chr>    <chr>      <dbl> <int>   <dbl> <chr>          \n 1    1.00e-10 accuracy binary     0.808     5 1.52e-3 Preprocessor1_…\n 2    1.00e-10 roc_auc  binary     0.738     5 7.72e-4 Preprocessor1_…\n 3    1.29e- 9 accuracy binary     0.808     5 1.52e-3 Preprocessor1_…\n 4    1.29e- 9 roc_auc  binary     0.738     5 7.72e-4 Preprocessor1_…\n 5    1.67e- 8 accuracy binary     0.808     5 1.52e-3 Preprocessor1_…\n 6    1.67e- 8 roc_auc  binary     0.738     5 7.72e-4 Preprocessor1_…\n 7    2.15e- 7 accuracy binary     0.808     5 1.52e-3 Preprocessor1_…\n 8    2.15e- 7 roc_auc  binary     0.738     5 7.72e-4 Preprocessor1_…\n 9    2.78e- 6 accuracy binary     0.808     5 1.52e-3 Preprocessor1_…\n10    2.78e- 6 roc_auc  binary     0.738     5 7.72e-4 Preprocessor1_…\n11    3.59e- 5 accuracy binary     0.808     5 1.53e-3 Preprocessor1_…\n12    3.59e- 5 roc_auc  binary     0.738     5 7.75e-4 Preprocessor1_…\n13    4.64e- 4 accuracy binary     0.808     5 1.69e-3 Preprocessor1_…\n14    4.64e- 4 roc_auc  binary     0.737     5 9.58e-4 Preprocessor1_…\n15    5.99e- 3 accuracy binary     0.807     5 2.46e-3 Preprocessor1_…\n16    5.99e- 3 roc_auc  binary     0.730     5 2.33e-3 Preprocessor1_…\n17    7.74e- 2 accuracy binary     0.806     5 2.34e-3 Preprocessor1_…\n18    7.74e- 2 roc_auc  binary     0.5       5 0.      Preprocessor1_…\n19    1.00e+ 0 accuracy binary     0.806     5 2.34e-3 Preprocessor1_…\n20    1.00e+ 0 roc_auc  binary     0.5       5 0.      Preprocessor1_…\n\nrf_tune %>%\n  collect_metrics()\n\n\n# A tibble: 18 x 8\n    mtry min_n .metric  .estimator  mean     n std_err .config        \n   <int> <int> <chr>    <chr>      <dbl> <int>   <dbl> <chr>          \n 1     1     2 accuracy binary     0.806     5 0.00225 Preprocessor1_…\n 2     1     2 roc_auc  binary     0.786     5 0.00307 Preprocessor1_…\n 3    12     2 accuracy binary     0.842     5 0.00184 Preprocessor1_…\n 4    12     2 roc_auc  binary     0.851     5 0.00291 Preprocessor1_…\n 5    24     2 accuracy binary     0.841     5 0.00267 Preprocessor1_…\n 6    24     2 roc_auc  binary     0.848     5 0.00259 Preprocessor1_…\n 7     1    21 accuracy binary     0.807     5 0.00210 Preprocessor1_…\n 8     1    21 roc_auc  binary     0.796     5 0.00191 Preprocessor1_…\n 9    12    21 accuracy binary     0.846     5 0.00172 Preprocessor1_…\n10    12    21 roc_auc  binary     0.868     5 0.00193 Preprocessor1_…\n11    24    21 accuracy binary     0.844     5 0.00125 Preprocessor1_…\n12    24    21 roc_auc  binary     0.862     5 0.00229 Preprocessor1_…\n13     1    40 accuracy binary     0.807     5 0.00210 Preprocessor1_…\n14     1    40 roc_auc  binary     0.788     5 0.00353 Preprocessor1_…\n15    12    40 accuracy binary     0.848     5 0.00105 Preprocessor1_…\n16    12    40 roc_auc  binary     0.872     5 0.00170 Preprocessor1_…\n17    24    40 accuracy binary     0.844     5 0.00189 Preprocessor1_…\n18    24    40 roc_auc  binary     0.866     5 0.00212 Preprocessor1_…\n\ntree_fit %>%\n  collect_metrics()\n\n\n# A tibble: 2 x 6\n  .metric  .estimator  mean     n std_err .config             \n  <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n1 accuracy binary     0.824     5 0.00143 Preprocessor1_Model1\n2 roc_auc  binary     0.686     5 0.00309 Preprocessor1_Model1\n\n\n\ntraffic_stack <-\n  stacks() %>%\n  add_candidates(lasso_tune) %>%\n  add_candidates(rf_tune) %>%\n  add_candidates(tree_fit)\n\n\n\n\n\ntraffic_blend <-\n  traffic_stack %>%\n  blend_predictions()\ntraffic_blend\n\n\n# A tibble: 9 x 3\n  member                    type          weight\n  <chr>                     <chr>          <dbl>\n1 .pred_Severe_rf_tune_1_8  rand_forest    1.80 \n2 .pred_Severe_rf_tune_1_5  rand_forest    1.47 \n3 .pred_Severe_rf_tune_1_2  rand_forest    0.871\n4 .pred_Severe_tree_fit_1_1 decision_tree  0.783\n5 .pred_Severe_rf_tune_1_9  rand_forest    0.642\n6 .pred_Severe_rf_tune_1_4  rand_forest    0.639\n7 .pred_Severe_rf_tune_1_3  rand_forest    0.604\n8 .pred_Severe_rf_tune_1_6  rand_forest    0.578\n9 .pred_Severe_rf_tune_1_7  rand_forest    0.225\n\n\n\ntraffic_final_stack <- traffic_blend %>%\n  fit_members()\n\n#saveRDS(traffic_final_stack, \"traffic_final_stacked.rds\")\n\n\n\nShiny App\nUser Interface (UI)\nWe also developed a shiny app that included all of the relevant variables of our analysis. The purpose of the app was to allow the user to plug different values for environmental conditions or locations to see how the severity predicted changes. When creating the UI, we decided to use sliders for each of the numerical variables. The sliders referenced minimum and maximum variables that I had defined previously in line 553. For variables with multiple levels like City and Weather condition, we decided to create a list of levels that we referenced later in the selectInput function. This saved us the time of having to type the name of each of the variables’ levels. We also formatted the app using the bslib package, we included this in the theme argument in line 566. We used the package to define a font, the primary, secondary and bootswatch colors. Given the large number of variables included, we also added a scrollable side panel in lineS 577 to 580.\nServer\nWe then defined the input variables for the server. We run into errors regarding incoherence between the variables used in the UI and the server. We decided to assign arbitrary values to the input variables and one by one we tested what variables were not being recognized. This also allowed us to detect certain variables that we no longer considered relevant, like county. We also had to pre-processing to change the names of variables like “wind_chill(F)” which were for some reason not recognized by the tibble function. Then, in lines 836 - 840 we defined the output by using the stacked model in our data and asked the app to show our prediction.\nEmbedded app\nWe decided to use an embedded app in a different r-markdown file considering that the size of our rsd file (our model) was too big to be deployed. We added a “runtime: shiny” argument in the YAML. Within an r code chunk, we still had to call the lists of variable levels and the minimum and maximum variable values before copying the code for our app. We used the shinyApp function in line 564 before plugging the remainder of our code. The app works but it could only be seen by people that have R installed in the computers and that have the project file. This is an issue that we further need to work on. We believe there are ways to reduce the size of our rsd file.\nShiny App Code\n\n\n# traffic_mod <- readRDS(\"traffic_final_stacked.rds\")\n# traffic_mod <- readRDS(\"traffic_final_stacked.rds\")\n# \n# Cities <-\n#   traffic_mod$train  %>%\n#   select(City) %>%\n#   distinct(City) %>%\n#   arrange(City) %>%\n#   pull(City)\n# \n# Weather <-\n#   traffic_mod$train  %>%\n#   select(Weather_Condition) %>%\n#   distinct(Weather_Condition) %>%\n#   arrange(Weather_Condition) %>%\n#   pull(Weather_Condition)\n# \n# \n# # Find min's, max's, and median's for quantitative vars:\n# \n# stats_num <-\n#   traffic_mod$train  %>%\n#   select(where(is.numeric)) %>%\n#   pivot_longer(cols = everything(),\n#                names_to = \"variable\",\n#                values_to = \"value\") %>%\n#   group_by(variable) %>%\n#   summarize(min_val = min(value),\n#             max_val = max(value),\n#             med_val = median(value))\n# \n# shinyApp(\n#   ui <- fluidPage(\n#   theme = bs_theme(primary = \"#123B60\",\n#                    secondary = \"#D44420\",\n#                    base_font = list(font_google(\"Raleway\"), \"-apple-system\",\n#                                     \"BlinkMacSystemFont\", \"Segoe UI\", \"Helvetica Neue\", \"Arial\",\n#                                     \"sans-serif\", \"Apple Color Emoji\", \"Segoe UI Emoji\",\n#                                     \"Segoe UI Symbol\"),\n#                    bootswatch = \"sandstone\"),\n#   # Application title\n#   sidebarLayout(\n#     sidebarPanel(\n#       # added this for scrollable side panel:\n#       tags$head(tags$style(\n#         type = 'text/css',\n#         'form.well { max-height: 600px; overflow-y: auto; }'\n#       )),\n#       sliderInput(inputId = \"Hour\",\n#               label = \"Hour of Accident\",\n#               min = stats_num %>%\n#                 filter(variable ==\"Hour\") %>%\n#                 pull(min_val),\n#               max = stats_num %>%\n#                 filter(variable ==\"Hour\") %>%\n#                 pull(max_val),\n#               value = stats_num %>%\n#                 filter(variable ==\"Hour\") %>%\n#                 pull(med_val),\n#               step = 1,\n#               round = TRUE),\n#       sliderInput(inputId = \"Month\",\n#               label = \"Month of Accident\",\n#               min = stats_num %>%\n#                 filter(variable ==\"Month\") %>%\n#                 pull(min_val),\n#               max = stats_num %>%\n#                 filter(variable ==\"Month\") %>%\n#                 pull(max_val),\n#               value = stats_num %>%\n#                 filter(variable ==\"Month\") %>%\n#                 pull(med_val),\n#               step = 1,\n#               round = TRUE),\n#       sliderInput(inputId = \"Wday\",\n#               label = \"Week day of Accident\",\n#               min = stats_num %>%\n#                 filter(variable ==\"Wday\") %>%\n#                 pull(min_val),\n#               max = stats_num %>%\n#                 filter(variable ==\"Wday\") %>%\n#                 pull(max_val),\n#               value = stats_num %>%\n#                 filter(variable ==\"Wday\") %>%\n#                 pull(med_val),\n#               step = 1,\n#               round = TRUE),\n#       sliderInput(inputId = \"Duration\",\n#               label = \"Duration of Accident in seconds\",\n#               min = stats_num %>%\n#                 filter(variable ==\"Duration\") %>%\n#                 pull(min_val),\n#               max = stats_num %>%\n#                 filter(variable ==\"Duration\") %>%\n#                 pull(max_val),\n#               value = stats_num %>%\n#                 filter(variable ==\"Duration\") %>%\n#                 pull(med_val),\n#               step = 1,\n#               round = TRUE),\n#       sliderInput(inputId = \"Start_Lat\",\n#               label = \"Starting latitude of the Accident\",\n#               min = stats_num %>%\n#                 filter(variable ==\"Start_Lat\") %>%\n#                 pull(min_val),\n#               max = stats_num %>%\n#                 filter(variable ==\"Start_Lat\") %>%\n#                 pull(max_val),\n#               value = stats_num %>%\n#                 filter(variable ==\"Start_Lat\") %>%\n#                 pull(med_val),\n#               step = 1,\n#               round = TRUE),\n#       sliderInput(inputId = \"Start_Lng\",\n#               label = \"Starting longitude of the Accident\",\n#               min = stats_num %>%\n#                 filter(variable ==\"Start_Lng\") %>%\n#                 pull(min_val),\n#               max = stats_num %>%\n#                 filter(variable ==\"Start_Lng\") %>%\n#                 pull(max_val),\n#               value = stats_num %>%\n#                 filter(variable ==\"Start_Lng\") %>%\n#                 pull(med_val),\n#               step = 1,\n#               round = TRUE),\n#       sliderInput(inputId = \"Distance\",\n#               label = \"Distance of the Accident\",\n#               min = stats_num %>%\n#                 filter(variable ==\"Distance\") %>%\n#                 pull(min_val),\n#               max = stats_num %>%\n#                 filter(variable ==\"Distance\") %>%\n#                 pull(max_val),\n#               value = stats_num %>%\n#                 filter(variable ==\"Distance\") %>%\n#                 pull(med_val),\n#               step = 1,\n#               round = TRUE),\n#       selectInput(inputId = \"Side\",\n#               label = \"Side of the street where the accident happened\",\n#               choices = list(Right = \"R\",\n#                              Left = \"L\")),\n#       selectInput(inputId = \"City\",\n#                   label = \"City where the accident happened\",\n#                   choices = Cities),\n#       sliderInput(inputId = \"Temperature\",\n#               label = \"Temperature when accident happened\",\n#               min = stats_num %>%\n#                 filter(variable ==\"Temperature\") %>%\n#                 pull(min_val),\n#               max = stats_num %>%\n#                 filter(variable ==\"Temperature\") %>%\n#                 pull(max_val),\n#               value = stats_num %>%\n#                 filter(variable ==\"Temperature\") %>%\n#                 pull(med_val),\n#               step = 1,\n#               round = TRUE),\n#       sliderInput(inputId = \"Wind_Chill\",\n#               label = \"Wind chill in degrees Farenheit when accident happened\",\n#               min = stats_num %>%\n#                 filter(variable ==\"Wind_Chill\") %>%\n#                 pull(min_val),\n#               max = stats_num %>%\n#                 filter(variable ==\"Wind_Chill\") %>%\n#                 pull(max_val),\n#               value = stats_num %>%\n#                 filter(variable ==\"Wind_Chill\") %>%\n#                 pull(med_val),\n#               step = 1,\n#               round = TRUE),\n#       sliderInput(inputId = \"Humidity\",\n#               label = \"Humidity when accident happened\",\n#               min = stats_num %>%\n#                 filter(variable ==\"Humidity\") %>%\n#                 pull(min_val),\n#               max = stats_num %>%\n#                 filter(variable ==\"Humidity\") %>%\n#                 pull(max_val),\n#               value = stats_num %>%\n#                 filter(variable ==\"Humidity\") %>%\n#                 pull(med_val),\n#               step = 1,\n#               round = TRUE),\n#       sliderInput(inputId = \"Pressure\",\n#               label = \"Pressure when accident happened\",\n#               min = stats_num %>%\n#                 filter(variable ==\"Pressure\") %>%\n#                 pull(min_val),\n#               max = stats_num %>%\n#                 filter(variable ==\"Pressure\") %>%\n#                 pull(max_val),\n#               value = stats_num %>%\n#                 filter(variable ==\"Pressure\") %>%\n#                 pull(med_val),\n#               step = 1,\n#               round = TRUE),\n#       sliderInput(inputId = \"Visibility\",\n#               label = \"Visibility when accident happened\",\n#               min = stats_num %>%\n#                 filter(variable ==\"Visibility\") %>%\n#                 pull(min_val),\n#               max = stats_num %>%\n#                 filter(variable ==\"Visibility\") %>%\n#                 pull(max_val),\n#               value = stats_num %>%\n#                 filter(variable ==\"Visibility\") %>%\n#                 pull(med_val),\n#               step = 1,\n#               round = TRUE),\n#       sliderInput(inputId = \"Wind_Speed\",\n#               label = \"Wind speed when accident happened\",\n#               min = stats_num %>%\n#                 filter(variable ==\"Wind_Speed\") %>%\n#                 pull(min_val),\n#               max = stats_num %>%\n#                 filter(variable ==\"Wind_Speed\") %>%\n#                 pull(max_val),\n#               value = stats_num %>%\n#                 filter(variable ==\"Wind_Speed\") %>%\n#                 pull(med_val),\n#               step = 1,\n#               round = TRUE),\n#       sliderInput(inputId = \"Precipitation\",\n#               label = \"Precipitation when accident happened in inches\",\n#               min = stats_num %>%\n#                 filter(variable ==\"Precipitation\") %>%\n#                 pull(min_val),\n#               max = stats_num %>%\n#                 filter(variable ==\"Precipitation\") %>%\n#                 pull(max_val),\n#               value = stats_num %>%\n#                 filter(variable ==\"Precipitation\") %>%\n#                 pull(med_val),\n#               step = 1,\n#               round = TRUE),\n#       selectInput(inputId = \"Crossing\",\n#               label = \"Is there a crossing where the accident happened?\",\n#               choices = list(Yes = \"TRUE\",\n#                              No = \"FALSE\")),\n#       selectInput(inputId = \"Junction\",\n#               label = \"Is there a junction where the accident happened?\",\n#               choices = list(Yes = \"TRUE\",\n#                              No = \"FALSE\")),\n#       selectInput(inputId = \"Traffic_Signal\",\n#               label = \"Is there a traffic signal where the accident happened?\",\n#               choices = list(Yes = \"TRUE\",\n#                              No = \"FALSE\")),\n#       selectInput(inputId = \"Sunrise_Sunset\",\n#               label = \"Is it night or day?\",\n#               choices = list(Night = \"Night\",\n#                              Day = \"Day\")),\n#       selectInput(inputId = \"Civil_Twilight\",\n#               label = \"Is there enough natural light to be day?\",\n#               choices = list(Yes = \"Day\",\n#                              No = \"Night\")),\n#       selectInput(inputId = \"Nautical_Twilight\",\n#               label = \"Is it nautical day or night?\",\n#               choices = list(\"Day\",\"Night\")),\n#       selectInput(inputId = \"Astronomical_Twilight\",\n#               label = \"Was the sky illuminated by the sun?\",\n#               choices = list(Yes = \"Day\",\n#                              No = \"Night\")),\n#       selectInput(inputId = \"Weather_Condition\",\n#               label = \"Weather condition when accident happened\",\n#               choices = Weather),\n#       submitButton(text = \"Get the Prediction\"),\n#     ),\n#       mainPanel(\n#         verbatimTextOutput(\"Pred\")\n#       )\n#    )\n# ),\n# server = function (input,output) {\n#   output$Pred <- renderPrint({\n#     data <- tibble(\n#       # TMC=input$TMC,\n#       Month=input$Month,\n#       Hour=input$Hour,\n#       Wday=input$Wday,\n#       Duration=input$Duration,\n#       Start_Lat=input$Start_Lat,\n#       Start_Lng=input$Start_Lng,\n#       Distance=input$Distance,\n#       Side=input$Side,\n#       City=input$City,\n#       Temperature=input$Temperature,\n#       Wind_Chill=input$Wind_Chill,\n#       Humidity=input$Humidity,\n#       Pressure=input$Pressure,\n#       Visibility=input$Visibility,\n#       Wind_Speed=input$Wind_Speed,\n#       Precipitation=input$Precipitation,\n#       Crossing=input$Crossing,\n#       Junction=input$Junction,\n#       Traffic_Signal=input$Traffic_Signal,\n#       Sunrise_Sunset=input$Sunrise_Sunset,\n#       Civil_Twilight=input$Civil_Twilight,\n#       Nautical_Twilight=input$Nautical_Twilight,\n#       Astronomical_Twilight=input$Astronomical_Twilight,\n#       Weather_Condition=input$Weather_Condition\n#     )\n#     pred <-\n#       predict(traffic_mod,data) %>%\n#       pull(.pred_class)\n# \n#     pred}\n#   )\n# },\n# \n#   options = list(height = 500)\n# )\n\n\n\n\n\n\n",
    "preview": "posts/2021-05-07-accidentbehind/behind.jpg",
    "last_modified": "2025-12-08T11:05:27-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-05-07-accidentprediction/",
    "title": "Prediction of the severity of car accidents",
    "description": "Analyze the environmental factors that are more strongly associated with car accident in the United States to create a prediction model.",
    "author": [
      {
        "name": "Juthi Dewan, Coco Li, Franco Salinas",
        "url": {}
      }
    ],
    "date": "2021-05-07",
    "categories": [],
    "contents": "\n\n\n\n\n\n\nIntroduction\nThe Fatality Analysis Reporting System indicated that an estimate of 8870 people died in motor vehicle traffic crashes in the second quarter of 2020 (NHTSA, 2020). This analysis is intended to bring light to the main environmental conditions that are associated with the severity of a car accident. For the purpose of this study we defined severity as the accident’s impact on traffic.\nData\nThe data we used has 47 variables and 3 million observations for different car accidents. The data was collected from February 2016 to December 2020 for the 49 states of the US. The data base has been constructed partly by Moosavi, Sobhan, Mohammad Hossein Samavatian, Srinivasan Parthasarathy, and Rajiv Ramnath as “A Countrywide Traffic Accident Dataset” (2019). The other part of the data base was constructed by Moosavi, Sobhan, Mohammad Hossein Samavatian, Srinivasan Parthasarathy, Radu Teodorescu, and Rajiv Ramnath for their database “Accident Risk Prediction based on Heterogeneous Sparse Data: New Dataset and Insights.”\nModels\nUsing this data set, we predicted the Severity of an accident using stacked LASSO, Forest and classification three. Stacking combines predictions from many different models into a “super” predictor. In this case we would be averaging the predictions of the LASSO, Forest and classification three.\nPre-processing\n\n# A tibble: 3 x 2\n  `Variables to drop` `NA proportion`\n  <chr>                         <dbl>\n1 End_Lat                       0.636\n2 End_Lng                       0.636\n3 Number                        0.633\n\n\n\n\nWe chose not to use multiple of the 47 variables that we considered weren’t relevant to the analysis we were conducting. As we can see in the table, end latitude, end longitude’s proportion of NA values are higher than 50%. Given the distribution of wind direction through the different severity levels, we decided that the variable is uninformative. The description, bump, traffic calming, give way, no exit, railway, roundabout, station, stop, amenity, street, zip code, country, turning loop, county and TMC code weren’t informative either given the distribution between the severity categories or near-zero variance. We also decided to group the four levels of severity status into two categories. Given the large amount of levels for the weather condition variable I decided to use only the levels that had a count higher than 20.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlobal interpretation\nGlobal model interpretations explain the overall relationships between the predictor variables and the response.\nModel performance\n\n\n\n\nPreparation of a new explainer is initiated\n  -> model label       :  rf \n  -> data              :  49044  rows  24  cols \n  -> data              :  tibble converted into a data.frame \n  -> target variable   :  49044  values \n  -> predict function  :  yhat.workflow  will be used ( [33m default [39m )\n  -> predicted values  :  No value for predict function target column. ( [33m default [39m )\n  -> model_info        :  package tidymodels , ver. 0.1.2 , task classification ( [33m default [39m ) \n  -> predicted values  :  numerical, min =  0 , mean =  0.1959684 , max =  1  \n  -> residual function :  difference between y and yhat ( [33m default [39m )\n  -> residuals         :  numerical, min =  -0.8282602 , mean =  -0.001734642 , max =  0.9717742  \n [32m A new explainer has been created! [39m \nPreparation of a new explainer is initiated\n  -> model label       :  lasso \n  -> data              :  49044  rows  24  cols \n  -> data              :  tibble converted into a data.frame \n  -> target variable   :  49044  values \n  -> predict function  :  yhat.workflow  will be used ( [33m default [39m )\n  -> predicted values  :  No value for predict function target column. ( [33m default [39m )\n  -> model_info        :  package tidymodels , ver. 0.1.2 , task classification ( [33m default [39m ) \n  -> predicted values  :  numerical, min =  0.00227835 , mean =  0.1942335 , max =  0.9658464  \n  -> residual function :  difference between y and yhat ( [33m default [39m )\n  -> residuals         :  numerical, min =  -0.8613942 , mean =  2.375737e-07 , max =  0.9934433  \n [32m A new explainer has been created! [39m \nPreparation of a new explainer is initiated\n  -> model label       :  tree \n  -> data              :  49044  rows  24  cols \n  -> data              :  tibble converted into a data.frame \n  -> target variable   :  49044  values \n  -> predict function  :  yhat.workflow  will be used ( [33m default [39m )\n  -> predicted values  :  No value for predict function target column. ( [33m default [39m )\n  -> model_info        :  package tidymodels , ver. 0.1.2 , task classification ( [33m default [39m ) \n  -> predicted values  :  numerical, min =  0.1165841 , mean =  0.1942337 , max =  0.7279736  \n  -> residual function :  difference between y and yhat ( [33m default [39m )\n  -> residuals         :  numerical, min =  -0.7279736 , mean =  3.562719e-18 , max =  0.8834159  \n [32m A new explainer has been created! [39m \n\n\n\n\n\n\n\nAs we can see from the histograms, the majority of the residual values are clustered in values close to 0. The model with the lowest average residual value is lasso, followed by the tree model and the forest model. While the lasso and the forest models are left skewed this is not the case for the classification tree. The residuals for the classification tree are more spread towards the positive and the negative values, showing that this model might be less precise than the other two.\nVariable of importance\n\n\n\n\n\n\n\n\n\nWe can see that there are more variables with greater importance in the lasso and the forest models. This means that, the values that are at the top generate great increases in the performance of the model when permuted relative to the other variables. The length of the bars indicate how much the performance increases when that variable is permuted. Permuting is the process of exchanging the values of a variable between observations. We can see that the lasso and forest model’s performance increase when start longitude is permuted. This is the case for city in the classification tree.\nCeteris-Paribus Profile\nThis profiles show how one variable affects the outcome holding all other variables fixed for one observation.\n\n\n\nIn this graph we can see how changes to the values of the starting longitude affect the probability that the accident is Severe, holding all the other variables constant. As we can see, the probability that an accident is severe changes drastically whit different start longitude values, reflecting the importance of the variable for the forest model.\nPartial Dependence Plots\nRemember the CP profile used only one observation? A partial dependence plot is created by averaging the CP profiles for a sample of observations. The partial dependence profile is the blue line. We can see that overall, changes to the value of the longitude affect the probability that an accident is sever significantly. If we were to analyze the dependence plot for lasso the lines we would see inclined parallel lines given that lasso is additive. This is not very informative, that’s why we decided to focus on the forest model.\n\n\n\nLocal Model Interpretation\nLocal model interpretation helps us understand the impact of variables on individual observations. We will focus on the random forest model given that it is the model with the higher accuracy. We would like to do the interpretation for the stacked model, but this isn’t possible using DALEX and DALEXtra. Considering that out stacked model has few models stacked, we think that doing the analysis of the random forest model should give us a fair idea of what is the local importance of our variables.\nShapley Additive Explanations (SHAP)\nFor Break Down profiles, the contributions of variables would change with the order in which the variables are considered in the random forest model. Therefore we decided to use the SHAP.\n\n\n\nEach bar shows the average contribution of each variable’s value to the predicted severity for this observed accident. We can see that a duration of 44.62 contributes almost an additional 0.15 to the predicted probability of a severe accident, on average. The boxplot shows the variation across permutations of the order of the variable. A large variation would mean that we should be less confident in its exact effect. For example we see that Nautical twilight has a large variation; therefore, we aren’t confident about it’s contribution to the prediction.\nLocal Interpretable Model-agnostic Explanations (LIME)\n\n# A tibble: 1 x 3\n  model_r2 model_prediction prediction\n     <dbl>            <dbl>      <dbl>\n1    0.217            0.155      0.178\n\n\nThe table shows the predicted value from the local model as 0.2624304. and the prediction from the original forest model of 6.17 This graph shows us the predicted value from the original random forest model as “Prediction”. The r-squared value of the model is shown as the “Explanation fit” showing that the model explains 25% of the variance of our sample. The bars show that distance is the most important variable in the local model.\nConclusion:\nWe have explored how changes in the values of variables for an observation can affect the predicted outcome using two methods. Both methods show similar results; however, it’s important to note that while the SHAP plot includes more variables, it is less reliable. We have seen also that starting longitude is the most important variable according to two of our models, and that changing its value for an observation holding the other variables constant changes the predicted value significantly. Even with slight differences all of the models showed similar results for the variables of interest, showing that the results are coherent and reasonable. We can conclude that overall, distance, starting longitude, duration and month are the most important variables for the prediction of the severity of an accident. This means that depending on the month, the weather or the amount of traffic could be associated with the severity of an accident. The length of the road extent affected by the accident also is a good predictor for severity. The time between the start of the accident and the end of the impact on traffic flow also are associated with the severity. Lastly, there is a strong association between starting longitude of the accident and the severity which could be driven by demographic or other non-controlled characteristics that differentiate western to eastern states.\nRepercussions:\nOur analysis only focused on environmental factors that are associated with the severity of an accident. There are multiple other factors that we didn’t include like speed or whether the driver was intoxicated. Therefore, the model and the analysis is not complete and should not be regarded as such. On the other hand, this analysis didn’t account for demographics; therefore, the differences in locations do not respond to anyone’s ethnicity, education, sex or any other identity. This model is meant to further analyze what external factors contribute the most to the severity of an accident.\n\n\n\n",
    "preview": "posts/2021-05-07-accidentprediction/accident.png",
    "last_modified": "2025-12-08T11:05:31-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-19-buildingdistill/",
    "title": "Changes on the stock market during the COVID-19 pandemic",
    "description": "Analysis of the change in stock market prices, cumulative returns, and trading volumes during the COVID-19 pandemic for the biggest companies in each sector.",
    "author": [
      {
        "name": "Franco Salinas, Duc Ngo, Vichearith Meas, and Max Wang",
        "url": {}
      }
    ],
    "date": "2021-03-19",
    "categories": [],
    "contents": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction:\nWe have seen multiple changes around the world after one year. The main reason for that is because of the coronavirus pandemic. Firstly thought of as a common flu in Wuhan, China, the pandemic has spread around the world, with more than 100 million cases and more than 2.5 million deaths. Especially in the US, we have seen around 30 million cases with more than 500,000 deaths resulting from the pandemic (according to Worldometer) . Coronavirus has changed our perspective in multiple areas, however, in this blog post, we focus specifically on its impact on companies and sectors in the US stock market. We will see what are the changes in stock price, in market capitalization as well as how each sector has changed after the pandemic.\nFirst, as we have more than 2,800 companies trading in the New York Stock Exchange (NYSE), it is really difficult to do it in a short period of time. Instead, we used the website Yahoo Finance to find the data for the 3 companies with highest market cap in different 11 sectors: Communication Discretionary, Communication Services, Consumer Staples, Energy, Financials, Health Care, Industrials, Material, Real Estate, Technology, Utilities. Then, to compare the results from different sectors to the wider market, we used the SPY, which is an ETF for the S&P 500 that tracks the changes in the value of the biggest 500 companies in the market. We classified the SPY data as “All fields”. We tracked the data from 2020-01-02 to 2021-03-04, using the volume and the closing price. We assembled the data in a time series format creating dummy variables for each sector, then we chose to use the closing price given that it shows the final value that market participants attribute to a share after trading, providing a more realistic depiction of its value. Then, we calculated the Cumulative Rate of Return everyday by using the following formula:\n\\[\nreturn = \\frac{Price_i - Price_1}{Price_1}* 100\n\\]\nWe also obtained the market cap by multiplying the number of market shares of each company by the Closing price each day. We retrieved the number of shares from the SEC filing website.\nAfter we have found the way to conduct research with our data, we have created three main questions:\na. Which companies and which sectors have performed the best during the period? What is the proportion of negative and positive daily average return for each sector?\nb. What are the changes in market capitalization?\nc. Which companies have the highest trading volume? Which sectors have the highest number of trading?\nThe performance of companies and the sectors:\nReturn by sectors:\n\n\n\n\n\n\n\n\n\n\n\n\nOverall, it seems that the return for the Communication Discretionary is consistently growing, almost in a positively linear way. The lowest return is still not below 0. Changes in return of Communication Services and Technology follow a similar trend to that of Communication Discretionary in shape, but have a smaller magnitude (-20~ 60 for CS, -25~100 for T, compared to 0~300 for CD). Consumer Staples fluctuated mildly, falling sharply first to -10 and rebounding quickly, then growing steadily to finally decrease back to 0 return. The situation for the Energy and Financials sectors is the worst. Since the first day, Energy’s cumulative return is negative. Only the last month is positive for the Financials sector. Health Care, Industrials, Material, Real Estate, and Utilities followed a similar trend, but these sectors’ conditions are better than that of Energy and Financials. Health Care, Industrials, Material, Real Estate, and Utilities have similar trends compared to the overall market. In general, the return sharply dropped from 2020-1-1 to April 2020 and then rebounded at a lower rate, back to 0 in August 2020. After, it oscillated up and down until 2020 November and then continuously increased slowly. So far, return as a whole is 20%.\n\n\n\nAdditionally, we try to see the proportion of time in which the average of the cumulative return of the best stocks per sector have been over or under the base price. In here, it comes as no surprise as Communication Discretionary was the sector with the highest proportion of positive returns relative to the negative returns. This meaning, that 97% of the time the Communication Discretionary sector had positive returns relative to the initial price. Had a person bought shares of the companies in this sector on January 2nd, they would have slept peacefully 97% of the time. On the other hand, the Energy sector was the sector with the highest proportion of negative returns relative to positive returns. As we can see, the pandemic affected different sectors in a different way. The least affected sectors are Communication Discretionary and Technology, one of the reasons being that their consumption doesn’t require in person presence. In the case of energy, a drop in the demand of petroleum as a consequence of the lockdown in different countries decreased their prices. Given that oil companies’ revenue depends on price, the investors sold their shares expecting losses on the oil industry. The Financials sector was also drastically affected, as the FED lowered the interest rates and there were negative interest rates in Europe, understanding that banks are the suppliers of capital, lower (or negative) interest rates affect banks’ revenue.\nReturn by companies:\n\n\n\n\n\n\nWhen we go deeper into each company in the field, it will be Tesla that grows the most during the period. Tesla’s stock price has increased more than 9 times for the period, followed by Nvidia as the company tripled in value. After that, we can see Apple, Amazon, Microsoft or Facebook increased quite heavily during the period, all increasing by 100-150%. Given that the Energy and Financial sectors did not see an increase in the stock price, it comes as no surprise that we can’t see any companies in those sectors in the top ten list.\nWhen we look closer at the shorter period, in March or April, we can see Walmart (one of the companies in the Consumer Staples section) that was in top 10 companies with the greatest return. However, after this period, it was still companies in Communication Discretionary and Technology that have the highest return such as Tesla, Nvidia, Microsoft, Amazon or Apple.\nThe change in market capitalization for each sector and for the company:\n\n\n\n\n\n\nLooking at the distribution of market cap by sector during the Covid-19 period, it is seen clearly that Communication Discretionary and Technology have consistently been the top first and second largest sectors respectively with combined shares at more than 60%. The two sectors remain the leading sectors throughout the period from January 2020 to March 2021.\nConsumer Discretionary is the biggest sector which accounts for around 46% of the entire market cap. The highest percentage of this sector was 49.7888% in late October 2020. This increasing trend started in January and hit the peak in October 2020 before having a decreasing trend from October to March the next year. The next sector, Technology, also had an increasing percentage over time as the area in the graph increased. This percentage of market cap for this sector started from 19.85% in January 2020, at 21.48% in the 1st Quarter in April 2020 and at 24.17% in November 2020. The max for this sector was 26.84%.\n\n\n\n\n\n\n\n\n\nThe distribution of market cap by company graph shows the top 20 companies that have the greatest amount of market cap proportion. Apple (AAPL), Amazon (AMZN), and Microsoft (MSFT) are the top three companies that are from the top two sectors: Communication Discretionary and Technology. From this we can see how the two sectors are the dominant by looking which companies have the greatest market cap.\nThe change in trading volume for each sector.\n\n\n\nThe bar graph shows the sum of the monthly transactions for the three biggest companies in each sector. We can see that the sector with the highest volume is Technology, and the sector with the lowest volume is the Real Estate sector. The high volumes in the Technology sector potentially reflect that investors preferred large technology companies over the pandemic.\n\n\n\n\n\n\nThese companies have large market caps, and given that the 5 largest companies in the Technology sector account for a large fraction of the S&P 500, these “blue chip” companies are expected to have larger volumes. One of the reasons is their liquidity. They are very liquid because many people are trading them; therefore, you can find buyers and sellers quickly. As a result there are no price distortions as a consequence of delays in transactions. On the other hand, this could be just a continuation of past trends, given that the technology sector has been the most popular in the last years.\nAnother potential explanation is that Technology stocks are always more risky given the larger returns; therefore, there are major price swings that cause people to increase their transactions. Also, larger volumes reflect the strength of price changes during the pandemic. In the case of Real Estate, low volumes reflect the lack of strength in price changes, reflected on subtle changes in the cumulative returns graphed. As we can see in the graph for Real Estate companies, the trend is flatter and the changes are of a smaller magnitude, proving that there weren’t significant changes in return. Therefore,there weren’t many changes in the demand or supply of Real Estate stock.\nWe can also observe that in periods with low prices, the volume tends to increase. In this case, it was in March given the declaration of sanitary emergency and the first COVID-19 cases in the country. This is because many people attempt to sell their shares quickly while other investors seek to buy shares at lower prices.\nConclusion:\nThe following post has shown some of the key changes within the period. It has shown how each sector has changed and how each company reacts to the pandemic. Some companies thrived, like Tesla, however, some took lots of time to recover, especially in the Energy and Finance sector. For the next steps, we could try and improve our information by scraping and getting all of the data in every company to illustrate and show the best picture for these sectors. This will be the improvement that we might need to do to see the wider picture of the market.\nOverall, our analysis has shown the changes in stock prices, volume, market capitalization before and after the pandemic. The pandemic and its consequences on the economy make it difficult to predict which companies will thrive and succeed in the future. However, we hope that some of our findings will shed light on some information about the general market trends observed when dealing with a global pandemic.\n\n\n\n",
    "preview": "posts/2021-03-19-buildingdistill/final_distill_foto.png",
    "last_modified": "2025-12-08T11:05:26-06:00",
    "input_file": {}
  }
]
