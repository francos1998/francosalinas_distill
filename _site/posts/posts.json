[
  {
    "path": "posts/2022-06-13-realestate/",
    "title": "Silicon Valley Evolution and Its Impact on House Values",
    "description": "How do tech migration and other socioeconomic factors affect house values across census tracts in the Bay Area?",
    "author": [
      {
        "name": "Thu Dang, Franco Salinas",
        "url": {}
      }
    ],
    "date": "2022-06-10",
    "categories": [],
    "contents": "\n\n\n\n\n\n\n\n\n\nAn\nIntroduction to the Evolution of Silicon Valley and Its House\nValues\nSilicon Valley is the most established tech center in the world. Home\nto major technology conglomerates like Google, Apple, and Tesla, Silicon\nValley is currently worth approximately $3 trillion. The area has\nevolved in a way that gradually lays the foundation for the booming tech\nindustry. It started with various education and business opportunities\nrelated to electronics and electrical engineering. These business\nopportunities attracted companies from the semiconductor industry, which\nis essential to creating software, and the vision for the Internet. This\nmade the area suitable for the rise of startups like Apple, Oracle, and\nIntel, which have become billion-dollar-worth household names and given\nway to the rise of the tech startup culture in the area. Although the\ndot-com bubble burst in 2000 scaled back the explosive overnight growth\nof the dot-com startups, it has allowed the tech industry to grow more\nsustainably than ever. The rise of companies like Google, Meta, and\nAmazon in this century has changed the way we live and think, which has\nresulted in increasing wealth in the area, reflected in the price of\nreal estate [@OnePieceWork_2020].\nThe real estate market in the San Francisco Bay Area saw an uptick in\nprice ever since the dot-com bubble. Despite being scaled back by the\n2008 financial market crash, it has seen the greatest growth ever since\n[@Carlisle_2021]. In 2019, the median\nhousing price in the Bay Area for a single-family home was $1.06 million\n[@Neilson_2021;@CAR_2021],\nwhich is 230% more expensive than the median house sales price in the\nwhole United States by the end of 2019 [@FRED_2022].\nResearch question\nHow do tech migration and other socioeconomic factors affect\nhouse values across census tracts in the Bay Area?\nWe believe that the wealth in tech has spilled over to the real\nestate industry in the Bay Area. As two individuals interested in\npursuing a tech career in emerging tech areas, i.e. Salt Lake City,\nUtah, or Austin, Texas, we wonder how the real estate scene there will\nevolve in the future, using the Bay Area as the point of reference.\nAn\noverall look at the house price in the Bay Area between 2009 and\n2019\nLooking at the evolution of house prices in 6 Bay Area counties\nbetween 2009 and 2019, it is clear that the house prices increase over\ntime, as indicated by the lighter color. Houses are more expensive in\nareas with a higher concentration of tech companies, such as counties in\nSan Francisco, San Mateo, and Santa Clara, with the median house prices\ngoing from around $750,000 in 2009 to over $1M\nin 2019.\n\n\n\nData Context\nIn this project, we worked with two types of data: Bay Area census\ndata and tech company data. We intentionally narrowed the data down to\nbetween 2009 and 2019 to avoid any bias from the 2008 economic downturn\nrecovery and outliers during COVID-19.\nBay Area census data\nThe data is collected on each census tract from the American\nCommunity Service (ACS) run by the U.S. Census Bureau. These yearly\nestimates are an inference from data collected over 5-year intervals\nbetween 2009 and 2019 [@ACS_2022]. The data provides aggregate\nsummaries of demographic information in census tracts of 6 Bay Area\ncounties: San Francisco, San Mateo, Santa Clara, Alameda, Marin, and\nContra Costa. Each census tract contains roughly 4000 inhabitants that\nare intended to be demographically and economically homogeneous. For the\npurpose of this analysis we used data on median estimated income, median\npopulation, median age, median household value, median household size,\nthe median number of houses under tenure, the median proportion of\npeople with different places of birth, the median proportion of\ndifferent races, median proportion of different industries, median\nproportion of houses owned and rented, area, and segment. We create the\nsegment variable to group different census tracts according to the\naggregated growth of companies.\nTech company data\nWe performed our analysis on the 13 companies with the largest market\ncap in the Bay Area [@Value_2022]. These companies are Apple,\nGoogle, Meta, Tesla, NVIDIA, Visa, Cisco, Broadcom, Adobe, Netflix,\nSalesforce, Oracle, and PayPal. Although Visa started out as a financial\ncompany, we included Visa in this list because its payment services are\nrooted in security technologies and the company has transformed into a\nfinancial technology (fintech) company with the evolution of technology\n[@VISA_2022].\nWe chose these 13 companies instead of all of the tech companies\navailable in the Bay Area assuming the 80-20 rule applies in this case.\nWe believe that the top 20% tech companies are those that contribute to\n80% of the wealth in the Bay Area, and hence are more relevant to our\nanalysis. We collected yearly data on the Earnings Before Interest, Tax,\nDepreciation, and Amortization (EBITDA) of these companies from 2009 to\n2019, which aligns with the timeframe for the census data. We chose\nEBITDA as our growth indicator because it is a commonly used indicator\nfor a company’s financial stability. We aggregated the EBITDA data of\nall companies within a 2000 meters radius of distance from a specific\ncensus tract to create a proxy for technology-related economic activity\n[@Investopedia_2022]. We obtained\nthe tech company data from the Macrotrends website [@Macrotrends_2022].\nThrough longitudinal and spatial data analysis, we hoped to gain\ninsight into how the tech industry and the rise of tech hubs can affect\nhousing value.\nOur\nanalysis approach: Longitudinal and Spatial Analysis\nWe chose a longitudinal model because we wanted to observe and\nexplain the changes in house values resulting from the rise of tech\ncompanies and other socioeconomic factors in the long run. A\nlongitudinal model allows us to consider various factors that may affect\nthe outcome over time. It also provides the correlation between them,\nenabling us to determine which factors are essential in explaining house\nvalues. We believe that, while there are factors related to tech\nmigration that might lead to short-term house price increases, there is\na delay in time between the migration of tech companies and the changes\nin house value. Therefore, a longitudinal analysis allows us to\nincorporate time into our analysis. Additionally, we were able to\nobserve and compare changes across multiple observation units, which are\nthe census tracts in this case.\nIn addition to accounting for the time aspect, we also wanted to take\ninto consideration how the house prices in a census tract are impacted\nby those in the neighboring census tracts. Therefore, we used techniques\nrelated to spatial data to perform that analysis.\nLongitudinal Analysis\nIn our longitudinal analysis, we hoped to explain the changes in Bay\nArea house prices through the tech migration and other socioeconomic\nfactors over time. The delayed effect of tech companies’ growth on house\nprices can be explained by macroeconomic dynamics. In this case, we\nbelieve that there has been a boost in demand for housing due to the\nexpanding labor market caused by the growth of tech companies in the\narea. This increase in demand is expected to cause an increase in house\nvalues when the supply gradually becomes insufficient.\nWe hypothesize that there are different degrees of growth for tech\ncompanies, and the census tracts with higher growth companies will see\nmore drastic increases in house prices. In order to fulfill our research\nquestion, we set out to segment the census tracts into different groups\nand look that them in relation to different socioeconomic indicators to\nsee how they impact house prices.\nData Processing\nTo prepare the data for our analysis, we combined the two datasets,\ni.e. tech company data and the Bay Area data, into one big dataset using\ntheir longitude and latitude information. The Bay Area data already has\nspatial information, with longitudinal and latitude, while the company\ndata does not. We tackled this by using the geocode package to obtain\nthe longitudinal and latitude information of the companies based on\ntheir addresses. Specifically, geocode takes the companies’ addresses\nand adds the corresponding longitude and latitude to the existing table.\nAfter that, we transformed the coordinate system of the company data to\nmatch that of the Bay Area dataset as these two datasets have different\ntypes of coordinate systems. This way, we confirmed we could combine the\ndata. Next, we aggregate the data by matching each company with the\ncensus tracts located within its 2000 meters radius. We then computed\nthe overall EBITDA growth percentage in the census tracts that are in\nthe 2000-meter proximity to the tech companies between 2009 and\n2019.\nWe separated the census tracts into four different groups based on\ntheir yearly EBITDA growth and used different resources to determine the\ncut-off points [@Geckoboard_2022;@BalboaCapital_2022].\nCensus tracts that have yearly EBITDA growth greater than 50% are the\n“High Growth” tracts. “Medium Growth” tracts have EBITDA growth ranging\nfrom 20% to 50%, and the figure for “Low Growth” tracts is under 20%.\nCensus tracts that are not within 2000 meters radius of tech companies\nbelong to the “Control” group. For the rest of our paper, we’ll refer to\nthese different growth groups with the understanding that these growth\ngroups are defined by the EBITDA growth of tech companies.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom this graph, we can see that the house prices across the 4 growth\nsegments follow the same trend. However, after 2014, there are some\ndrastic changes. The line representing the median house price in high\ngrowth areas is steepest, followed by medium growth and high growth.\nThis potentially reflects the fact that the high growth of tech\ncompanies had a larger effect on house values. High-growth,\nmedium-growth, and low-growth groups have higher house prices than the\ncontrol group, which is consistent with our prior beliefs that the\npresence of tech companies correlates with house values. The fact that\nareas with low tech growth present higher prices for years between 2013\nand 2017 could be caused by different factors like increases in the\nsupply of houses in medium and high growth areas. Also, during that\ntime, there were probably many emerging startups, which required more\nspace for offices within the Silicon Valley, which potentially generated\nhouse shortages and boosted the price.\nVariable consideration for\nthe model\nAfter conducting an analysis of the correlation between the different\npotential factors and Bay Area median house prices, the factors that\npresent strong correlations with the outcome are income, age, household\nsize, the proportion of people whose birthplace was in a state, the\nproportion of people in the information industry, finance, professional,\nand other industries. The proportion of white and black populations also\nhad a considerable correlation with median house value. As the analysis\nfocuses more on causal inference than prediction, we won’t omit any\nconfounding variables. It is important to mention that we are including\nindependent variables that are moderately correlated with each\nother.\nWe also include county information in the data to control for the\neffect on housing. The reason is that census tracts that are in the same\ncounty tend to have similar house prices, which makes county a\nreasonable indicator. We hypothesize that, due to the spillover effect\nfrom the thriving tech companies, counties with higher EBITDA growth\nwill have a larger house value compared to those with lower EBITDA\ngrowth. Additionally, higher incomes and population growth increase the\ndemand for housing as people can afford larger mortgages and there is a\nhigher demand for housing [@Pettinger_2021].\n\n\n\n\n\n\nGeneralized\nEstimating Equation (GEE) Model\nWe chose GEE because it allows us to work with different correlation\nstructures. In other words, we are able to test different assumptions\nregarding the correlation magnitude in our longitudinal data. Another\noption is a mixed-effects model, which we consider quite simplistic due\nto its assumption of exchangeable correlation. In other words, it is\nunrealistic to assume that longitudinal data has a constant correlation\nindependent of the difference in years.\nFurthermore, we consider that using GEE is advantageous as it has\nRobust Standard Errors (SE), which is independent of the working\ncorrelation structure and is close to the real SE. This allowed us to\ncompare the Model SE to the Robust SE when evaluating and choosing the\nbest model.\nCorrelation structure is a crucial component of the GEE model. When\nwe set out to model the median house value, we were concerned about\nfinding an appropriate correlation structure for the data we have. A\ncorrelation structure indicates our assumption for the magnitude at\nwhich the data correlate over time.\nTherefore, we created different models to test different correlation\nstructures and variables potentially impacting house values. In the\nfirst model, we used the exchangeable correlation structure, which\nassumes that the correlation between different EBITDA growth groups is\nconstant and that the correlation for each EBITDA group does not change\nover the years. We don’t think that this assumption is realistic in the\ncontext of our analysis. In fact, we believe that the correlation among\nthe observations varies across the years. Specifically, given the\neffects of the 2008 economic crisis on the housing market, we would\nexpect house values between the years 2009 and 2010 to be more similar\nthan those between 2009 and 2018.\nIn our final model, we used a correlation structure that shows an\nexponential decay to 0 with time in the correlation of observations.\nThis correlation structure is known as an Autoregression Model of order\n1 (AR1). We also chose the set of variables that would shed light on the\nchanges in house prices. We transformed some of the variables to make\nthem more concise. For our outcome variable, we decided to divide the\nmedian house values by 100,000 to decrease the magnitude of the data and\nmake computations easier. Similarly, we also reduced the magnitude of\nthe income variable by 10,000. We included the indicators for household\nsize, income, the proportion of people born in California, the\nproportion of white people, and county variables. With this set of\nvariables, we aimed to explain how tech growth and socioeconomic changes\nimpact house values.\nWe carry out a hypothesis test on the estimated coefficients in the\nGEE model to see whether there is a powerful relationship between the\nchosen factors and house values over the years. We conclude that the\nfactors contribute significantly to explaining house values, indicated\nwith a p-value lower than 0.05. Specifically, there is in fact a 0%\nchance that we would observe the data as we have if there was no\ncorrelation at all between the variables and the house value\noutcome.\n\n\n\n\n\n\n\n\n\nEvaluating\nthe GEE model through its residual plot\nA residual plot shows whether the model underpredicts or overpredicts\nhouse prices in different areas of the Bay Area. While white color shows\nthat the actual house prices are similar to the model predictions, red\nindicates areas that have higher actual house prices, and blue shows\nareas with lower actual house prices than the model predictions. Looking\nat the residual plot, we can see that the actual house values in the\ncensus tracts around the Silicon Valley are much higher than what the\nGEE model predicts. On the other hand, houses in bigger census tracts\nhave much lower values than the model estimates. From these results, we\nbelieve that there is some degree of spatial correlation that cannot be\naccounted for by a longitudinal model. In other words, the GEE model\ndoes not take into account how the median house price in a certain\ncensus tract can be influenced by the prices in the neighboring census\ntracts.\n\n\n\nIn order to verify this spatial relationship between the residuals\nafter predicting using the GEE model, we conduct the Moran’s I test. We\nfound that the residuals are spatially correlated, indicated by the\np-value smaller than 0.05 in the test (p-value = \\(2.2 {*} 10^{-16}\\)). In other words, there\nis almost a 0% chance that we would observe the actual median house\nprices like in the data if the residuals were not spatially\ncorrelated.\nTherefore, we decide to create a spatial model to account for the\nspatial correlation that is unexplained by the GEE longitudinal\nmodel.\nSpatial Analysis\nIntroduction to\nNeighborhood Structure\nCensus tracts that are closer to one another might have similar house\nprices. For example, census tracts that are in counties closer to tech\ncompanies like San Mateo and Santa Clara will be more similar compared\nto those in Marin which is much further from the tech hub. Therefore, it\nis important to statistically identify groups of census tracts that are\ncloser, or are “neighbors”, to the others. In order to identify groups\nof neighbors, we use a concept called neighborhood structure, which\nhelps us establish how close the census tracts are to one another. There\nare many techniques to define neighborhood structure, but here we used a\nmethod called K Nearest Neighbors (KNN), which calculates the distance\namong the centers of the census tracts and groups the census tracts into\ndifferent groups of k neighbors.\nWe chose the KNN neighborhood structure as we want to restrict the\nnumber of neighbors in each group. We believe that this method is\nvaluable in the sense that it creates groups of neighboring census\ntracts, each with k neighbors, and hence contributes to the\ngeneralization ability of the model. We choose 3 as the number of\nneighbors each census tract can have as it helps prevent the very\ncongested network for the counties closer to the city. In other words,\nit helps eliminate the problem of census tracts having too few or too\nmany neighbors.\nSpatial Autoregressive (SAR)\nModel\nWe use Spatial Autoregressive Models (SAR) to model the spatial\ncorrelation for the leftover residual from the linear regression model.\nThe SAR model reflects a stronger pattern of spatial covariance than CAR\n(when using the same parameter and weights), and it decays slower than\nthe CAR model. The SAR model is also better than the CAR model, as\nindicated by the BIC indices of the SAR and CAR fitted models. The BIC\nindex is the goodness-of-fit measure, and it shows how well the model\nfits the data.\nFrom the longitudinal analysis, we know that there are tech growth\nand socioeconomic factors affecting house values over time. Our goal for\nspatial analysis is to account for spatial correlation in a specific\nyear. As spatial techniques cannot be conducted on data spanning\ndifferent time periods, we choose to perform spatial analysis on 2018\ndata only.\nAfter using SAR to model the data, we ran the Moran’s I test again\nand found that the leftover errors after SAR are independent. In other\nwords, the SAR model has captured the possible spatial correlation, as\nindicated by the Moran’s I statistics, which is nearly 0, and the\np-value greater than 0.05 (p-value = 0.1489). Therefore, not only there\nis a longitudinal effect on house prices over the years in Bay Area\ncensus tracts but there is also a spatial correlation in each year, as\nwe have seen for 2018.\nEvaluating\nthe SAR model through its residual plot\nThe residual plot for the SAR model shows that there are some\ncounties whose house values are underestimated or overestimated by the\nmodel, indicated by the darker red and blue colors. Specifically, we can\nsee very large residuals in the San Mateo and Santa Clara counties as\nwell as larger counties on the leftmost and rightmost sides of the\nmap.\nWe carry out further exploration of the area using Google Maps and\ndiscover some terrain characteristics that can explain the high\nmagnitude of difference in the residuals. For example, there are two\nareas whose residuals are still high even after the GEE and SAR models\nthat are located along the inner coast of San Mateo County, as indicated\nby the deep blue color. The large residuals turn out to be reasonable as\nthese two areas are not residential areas. In other words, one of them\nis Foster City, which is an industrial area, and the other area is the\nhome of national parks. The houses in these areas might be undesirable\ndue to their proximity to industrial/manufacturing areas or pure nature\nreserves and no business or recreational endeavors. In addition, the\norange-color census tracts on the top left corner and lower right corner\nof the map show a different trend. Using Google Maps, we can see that\nthese two areas are the home of several national and local parks. As the\norange color indicates that the actual house values in these areas are\nhigher than SAR predictions, there might be luxury resorts or private\nholiday properties that might spike the house prices. For houses around\nthe Silicon Valley area that are also of an orange shade, the higher\nactual prices can be explained by higher demand given that it is close\nto big tech companies with high wealth and large labor forces. This\nmakes the area appealing to not only workers but also other big tech\ncompanies, emerging tech startups, and venture capital funds.\n\n\n\n\n\n\nResults and Discussion\nIn our analysis, we found evidence suggesting that house values grow\ndifferently for different growth segments post-2014. Especially for the\nhigh growth segment, the increase in the house prices outweighs the\nincrease in the medium and low growth groups as well as control areas.\nSuch drastic changes can be the result of the market recovery post-2018\neconomic downturn as well as the technology innovations and growth\nhappening in the area [@Carlisle_2021]. This is consistent\nwith our hypothesis that higher company growth is associated with higher\nhouse values. Aside from the longitudinal effect of tech growth and\nother socioeconomic factors on Bay Area house prices over the years,\nthere is also a spatial correlation in each year, meaning that house\nprices in neighboring areas of a census tract can inform the house price\nof that area.\nIn addition, income and whether the census tract has a majority white\npopulation are positively correlated with higher house values. Perhaps\nthis is because higher income allows people to afford more expensive\nhouses. Also, the higher the proportion of white residents in a census\ntract, the higher house values are likely to be. This could be due to\nthe effect of redlining, where people of color have been historically\ndiscriminated against in the housing market.\nOn the other hand, the median household size in a census tract and\nwhether there is a high proportion of people born in California\nnegatively correlate with house prices. Such a trend in household size\ncan be explained in terms of race and age. White households are usually\nsmaller in size, while people of color households tend to be bigger due\nto living with extended families or having more children. Also,\nimmigrants are more likely to live with extended families who immigrate\nwith them. Regarding the negative effects of the proportion of\nCalifornian residents, that can be explained by foreign money. If there\nis a higher proportion of people from outside of California coming to\nthe area, there will be more wealth flowing into the area, potentially\nthrough tech investments from other states or countries.\nFinally, the house prices of nearby census tracts impact one another.\nWe can see that Silicon Valley and the neighboring census tracts\nexperience relatively similar house values. Other residential census\ntracts that are further away or in a county that is not affiliated with\nbig techs like Marin or Contra Costa experience much lower house\nprices.\nMoving forward, there are many directions we can take to improve the\nanalysis. Firstly, we can consider using Uber data to determine the\ndistance between different census tracts and tech companies in terms of\nminutes. As we hypothesize that house prices are in high demand as\nhigh-income tech employees want to live closer to their companies, this\ncan allow us to identify which areas can be desirable for tech\nemployees. Secondly, natural terrain can be a factor in determining\nhouse prices. As we have mentioned, areas with national parks or right\nbeside mountains might have cheaper houses; therefore, incorporating\nnatural geographic data into the analysis can improve our models.\nThirdly, we hope to combine our longitudinal and spatial models into one\nas we potentially have to write a new library for the joined model.\nFinally, we would like to control for interest rates, considering that\nlower interest rates could lead to higher demand for housing due to\nbetter mortgage deals.\nAdditionally, as our ultimate goal is to understand the house prices\nin emerging tech hubs like Austin, Texas, and Salt Lake City, Utah, we\nwant to obtain similar data in these areas to verify the insights in\nthis analysis and potentially make a predictive model for these\nareas.\nOne limitation to our census data is that there is a ceiling, or\nmaximum, value for house prices. Before 2014, the house value is capped\nat $1M, whereas the cap is $2M after 2014.\nThere might have been some wording changes on the census survey, which\naffects the Bay Area census data.\nAcknowledgements\nWe would love to thank our amazing Prof. Brianna Heggeseth for her\nguidance and inspiration for this project. We love you, Brianna!\nReferences\n\n\n\n",
    "preview": "posts/2022-06-13-realestate/housevalue.png",
    "last_modified": "2022-06-13T09:51:22-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-05-07-accidentbehind/",
    "title": "Behind Scenes: Prediction of the severity of car accidents",
    "description": "In depth analysis of modelling decisions, coding and recommendations when approaching car accidents prediction.",
    "author": [
      {
        "name": "Juthi Dewan, Coco Li, Franco Salinas",
        "url": {}
      }
    ],
    "date": "2021-05-07",
    "categories": [],
    "contents": "\n\n\nlibrary(data.table)\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tidyverse)         # for reading in data, graphing, and cleaning\nlibrary(tidymodels)        # for modeling ... tidily\nlibrary(glmnet)            # for regularized regression, including LASSO\nlibrary(naniar)            # for examining missing values (NAs)\nlibrary(lubridate)         # for date manipulation\nlibrary(moderndive)        # for King County housing data\nlibrary(vip)               # for variable importance plots\nlibrary(rmarkdown)         # for paged tables\nlibrary(themis)            # for step functions for unbalanced data\nlibrary(stacks)            # for stacking models\nlibrary(DALEX)             # for model interpretation  \nlibrary(DALEXtra)          # for extension of DALEX\nlibrary(patchwork)         # for combining plots nicely\nlibrary(scales)\nlibrary(plotly)\nlibrary(gridExtra)\nlibrary(tidytext)\nlibrary(modelr)\nlibrary(caret)\nlibrary(ROSE)\nlibrary(glmnet)\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(randomForest)\nlibrary(shiny)\nlibrary(bslib)\noptions(warn = -1)\ntheme_set(theme_minimal()) # my favorite ggplot2 theme :)\n\n\n\n\n\ncars <- read_csv(\"small_accidents.csv\", col_types = cols(.default = col_character())) %>%\n  type_convert()\n\ncars %>%\n  group_by(City) %>%\n  summarize(Count=n()) %>%\n  arrange(desc(Count)) %>%\n  head(1000)\n\n\n# A tibble: 1,000 x 2\n   City        Count\n   <chr>       <int>\n 1 Houston      9612\n 2 Los Angeles  7771\n 3 Charlotte    7435\n 4 Dallas       6545\n 5 Austin       5832\n 6 Miami        5207\n 7 Raleigh      4420\n 8 Atlanta      3760\n 9 Orlando      3300\n10 Sacramento   3150\n# … with 990 more rows\n\nIntroduction\nThe Fatality Analysis Reporting System indicated that an estimate of 8870 people died in motor vehicle traffic crashes in the second quarter of 2020 (NHTSA, 2020). This analysis is intended to bring light to the main environmental conditions that are associated with the severity of a car accident. For the purpose of this study we defined severity as the accident’s impact on traffic.\nData\nThe data we used has 47 variables and 3 million observations for different car accidents. The data was collected from February 2016 to December 2020 for the 49 states of the US. The data base has been constructed partly by Moosavi, Sobhan, Mohammad Hossein Samavatian, Srinivasan Parthasarathy, and Rajiv Ramnath as “A Countrywide Traffic Accident Dataset” (2019). The other part of the data base was constructed by Moosavi, Sobhan, Mohammad Hossein Samavatian, Srinivasan Parthasarathy, Radu Teodorescu, and Rajiv Ramnath for their database “Accident Risk Prediction based on Heterogeneous Sparse Data: New Dataset and Insights.”\nModels\nUsing this data set, we predicted the Severity of an accident using stacked LASSO, Forest and classification three. Stacking combines predictions from many different models into a “super” predictor. In this case we would be averaging the predictions of the LASSO, Forest and classification three.\nPre-processing\n\n\ncars %>% summarise_all(~ mean(is.na(.))) %>%\n  pivot_longer(1:49, names_to = \"Variables to drop\", values_to = \"NA proportion\") %>%\n  filter(`NA proportion` >= 0.5)\n\n\n# A tibble: 3 x 2\n  `Variables to drop` `NA proportion`\n  <chr>                         <dbl>\n1 End_Lat                       0.636\n2 End_Lng                       0.636\n3 Number                        0.633\n\ndrop_na_cols <- c(\"End_Lat\", \"End_Lng\", \"Number\")\n\nnot_useful <- c(\"ID\", \"Source\", \"Timezone\", \"Airport_Code\", \"Weather_Timestamp\",\"Wind_Direction\", \"Description\", \"Bump\", \"Traffic_Calming\", \"Give_Way\", \"No_Exit\", \"Railway\", \"Roundabout\", \"Station\", \"Stop\", \"Amenity\", \"Street\", \"Zipcode\", \"Country\", \"Turning_Loop\", \"County\", \"TMC\")\n\n\ntraffic <-\n  cars %>%\n  select(-all_of(drop_na_cols), -all_of(not_useful))\n\np1 <- ggplot(cars, aes(as.factor(Station), ..prop.., group = Severity)) +\n  geom_bar(aes(fill = Severity), position = \"dodge\") +\n  scale_y_continuous(labels = percent) +\n  theme(axis.text.x = element_text(angle = 60, vjust = 0.6))\n \np2 <-  ggplot(cars, aes(Turning_Loop, ..prop.., group = Severity)) +\n  geom_bar(aes(fill = Severity), position = \"dodge\") +\n  scale_y_continuous(labels = percent) +\n  theme(axis.text.x = element_text(angle = 60, vjust = 0.6))\n\np3 <- ggplot(cars, aes(Country, ..prop.., group = Severity)) +\n  geom_bar(aes(fill = Severity), position = \"dodge\") +\n  scale_y_continuous(labels = percent) +\n  theme(axis.text.x = element_text(angle = 60, vjust = 0.6))\n\np4 <- ggplot(cars, aes(Amenity, ..prop.., group = Severity)) +\n  geom_bar(aes(fill = Severity), position = \"dodge\") +\n  scale_y_continuous(labels = percent) +\n  theme(axis.text.x = element_text(angle = 60, vjust = 0.6))\n\np5 <- ggplot(cars, aes(Stop, ..prop.., group = Severity)) +\n  geom_bar(aes(fill = Severity), position = \"dodge\") +\n  scale_y_continuous(labels = percent) +\n  theme(axis.text.x = element_text(angle = 60, vjust = 0.6))\n\np6 <- ggplot(cars, aes(Station, ..prop.., group = Severity)) +\n  geom_bar(aes(fill = Severity), position = \"dodge\") +\n  scale_y_continuous(labels = percent) +\n  theme(axis.text.x = element_text(angle = 60, vjust = 0.6))\n\np7 <- ggplot(cars, aes(Roundabout, ..prop.., group = Severity)) +\n  geom_bar(aes(fill = Severity), position = \"dodge\") +\n  scale_y_continuous(labels = percent) +\n  theme(axis.text.x = element_text(angle = 60, vjust = 0.6))\n\np8 <- ggplot(cars, aes(Railway, ..prop.., group = Severity)) +\n  geom_bar(aes(fill = Severity), position = \"dodge\") +\n  scale_y_continuous(labels = percent) +\n  theme(axis.text.x = element_text(angle = 60, vjust = 0.6))\n\np9 <- ggplot(cars, aes(No_Exit, ..prop.., group = Severity)) +\n  geom_bar(aes(fill = Severity), position = \"dodge\") +\n  scale_y_continuous(labels = percent) +\n  theme(axis.text.x = element_text(angle = 60, vjust = 0.6))\n\np10 <- ggplot(cars, aes(Give_Way, ..prop.., group = Severity)) +\n  geom_bar(aes(fill = Severity), position = \"dodge\") +\n  scale_y_continuous(labels = percent) +\n  theme(axis.text.x = element_text(angle = 60, vjust = 0.6))\n\np11 <- ggplot(cars, aes(Traffic_Calming, ..prop.., group = Severity)) +\n  geom_bar(aes(fill = Severity), position = \"dodge\") +\n  scale_y_continuous(labels = percent) +\n  theme(axis.text.x = element_text(angle = 60, vjust = 0.6))\n\n\np12 <- ggplot(cars, aes(Bump, ..prop.., group = Severity)) +\n  geom_bar(aes(fill = Severity), position = \"dodge\") +\n  scale_y_continuous(labels = percent) +\n  theme(axis.text.x = element_text(angle = 60, vjust = 0.6))\n\n\np1+ p2+ p3+ p4\n\n\n\np5+ p6+ p7+ p8\n\n\n\np9+ p10+ p11+ p12\n\n\n\n\nWe chose to not use multiple of the 47 variables that we considered weren’t relevant to the analysis we were conducting. As we can see in the table, end latitude, end longitude’s proportion of NA values are higher than 50%. Given the distribution of wind direction through the different severity levels, we decided that the variable is uninformative. The description, bump, traffic calming, give way, no exit, railway, roundabout, station, stop, amenity, street, zip code, country, turning loop, county and TMC code weren’t informative either given the distribution between the severity categories or near-zero variance.\n\n\ntraffic <-  traffic %>%\n  rename(\"Distance\" = `Distance(mi)`, \"Temperature\" = `Temperature(F)`, \"Humidity\" = `Humidity(%)`,\n         \"Pressure\" = `Pressure(in)`, \"Visibility\" = `Visibility(mi)`, \"Wind_Speed\" = `Wind_Speed(mph)`, \"Precipitation\" = `Precipitation(in)`, \"Wind_Chill\" = `Wind_Chill(F)`)\n\ntraffic$Severity <- as.character(traffic$Severity)\n\ntraffic <-\n  traffic %>%\n  add_n_miss() %>%\n  filter(n_miss_all == 0) %>%\n  select(-n_miss_all)\n\ntraffic <- traffic %>%\n  mutate(\"Status\" = factor(ifelse(Severity == \"3\" | Severity == \"4\", \"Severe\", \"Not Severe\"),\n                           levels = c(\"Not Severe\", \"Severe\")))\n\n\n\nIn this section of data cleaning and pre-processing above, we renamed some of the variables that had their units of measurement for ease of use later in our modeling and for our shinyApp. We took out entries of data that had NAs. If this was a smaller dataset, this may have negatively impacted our analysis, but we don’t believe this effected our analysis for this project because even after taking out some data, we had a lot left to work with. Another very important part of this section is that, we took the Severity variable that we are looking at and split it into two to make it into a Categorical variable called Status. Severities 1 and 2 were grouped in to be Not Severe and 3 and 4 were grouped as Severe in the Status variable.\n\n\n\ntraffic_time <- traffic %>%\n  mutate(Duration = (End_Time - Start_Time)) %>%\n  # accident duration should be positive\n  filter(!(Duration < 0)) %>%\n  separate(Start_Time, into = c(\"Date\", \"Time\"), sep = \" \") %>%\n  mutate(\"Year\" = str_sub(Date, 1, 4), \"Month\" = str_sub(Date, 6, 7), \"Day\" = str_sub(Date, 9, 10),\n         \"Wday\" = as.character(wday(Date))) %>%\n  mutate(\"Hour\" = str_sub(Time,1,2)) %>%\n  select(-c(\"Date\", \"Time\", \"End_Time\")) %>%\n  select(Severity, Year, Month, Day, Hour, Wday, Duration, everything())\n\n\n\nIn this section, we used the End_Time and Start_Time variables to come up with several other variables such as Duration, Date, Time, Year, Month, Day, Wday and Hour.\n\n\n#Drop levels that have less than 20 observations\nweather_to_drop <-\n  traffic_time %>%\n    count(Weather_Condition) %>%\n    filter(n < 20) %>%\n    select(Weather_Condition)\n\nweather_to_drop <-\n  weather_to_drop$Weather_Condition %>%\n    unlist()\n\ntraffic_weather <- traffic_time %>%\n  filter(!(Weather_Condition %in% weather_to_drop)) %>%\n  mutate(Weather_Condition = factor(Weather_Condition))\n\ntraffic2 <- traffic_weather\n\ncount_city <- traffic2 %>%\n  group_by(City) %>%\n  summarize(Count=n()) %>%\n  arrange(desc(Count)) %>%\n  head(950)  \n\ntraffic3 <-\n  traffic2 %>%\n    left_join(count_city, by=\"City\")\n\ntraffic_final <-\n  traffic3 %>%\n  add_n_miss() %>%\n  filter(n_miss_all == 0) %>%\n  select(-n_miss_all) %>%\n  select(-Count)\n\n#write.csv(traffic_final, \"traffic_final.csv\", row.names = FALSE)\n\n\n\n\n\nread_csv(\"traffic_final.csv\")\n\n\n# A tibble: 98,088 x 29\n   Severity  Year Month Day   Hour   Wday Duration Start_Lat Start_Lng\n      <dbl> <dbl> <chr> <chr> <chr> <dbl>    <dbl>     <dbl>     <dbl>\n 1        2  2016 12    07    23        4     44.4      38.6     -121.\n 2        2  2016 12    08    09        5     29.6      38.4     -123.\n 3        2  2016 12    23    09        6     29.7      38.3     -123.\n 4        2  2017 01    02    19        2     29.7      39.3     -121.\n 5        2  2017 01    22    15        1     44.6      38.1     -122.\n 6        2  2017 01    23    20        2     29.6      37.2     -122.\n 7        2  2017 01    23    22        2     29.8      37.4     -122.\n 8        2  2017 01    25    06        4     44.6      38.0     -122.\n 9        2  2016 11    30    08        4     70.0      38.7     -122.\n10        2  2016 06    21    16        3     60        34.4     -119.\n# … with 98,078 more rows, and 20 more variables: Distance <dbl>,\n#   Side <chr>, City <chr>, State <chr>, Temperature <dbl>,\n#   Wind_Chill <dbl>, Humidity <dbl>, Pressure <dbl>,\n#   Visibility <dbl>, Wind_Speed <dbl>, Precipitation <dbl>,\n#   Weather_Condition <chr>, Crossing <lgl>, Junction <lgl>,\n#   Traffic_Signal <lgl>, Sunrise_Sunset <chr>, Civil_Twilight <chr>,\n#   Nautical_Twilight <chr>, Astronomical_Twilight <chr>,\n#   Status <chr>\n\nUpon close examination of the Weather_Conditions variable, we realized that it had a lot of different observations and we wanted to narrow it down. So, we filtered and dropped levels that have less than 20 observations. We had to narrow down the City variable as well. We have over 4000 distinct cities under the top 12 states. Shiny only allows a thousand different observations and so in order for all the top cities to fit in our shiny app, we had to narrow down the list of cities to the top 950.\n\n\n#modeling pre-process for traffic_final\n\ntraffic_mod <- traffic_final %>%\n  mutate(Status = as.factor(Status)) %>%\n  mutate(across(where(is.character), as.factor)) %>%\n  select(-c(State, Severity, Year, Day)) %>%\n  # select(-arrival_date_year,\n  #        -reservation_status,\n  #        -reservation_status_date) %>%\n  add_n_miss() %>%\n  filter(n_miss_all == 0) %>%\n  select(-n_miss_all)\n\n\ntraffic_mod$Crossing <- as.factor(traffic_mod$Crossing)\ntraffic_mod$Month <- as.numeric(traffic_mod$Month)\ntraffic_mod$Wday <- as.numeric(traffic_mod$Wday)\ntraffic_mod$Hour <- as.numeric(traffic_mod$Hour)\ntraffic_mod$Duration <- as.numeric(traffic_mod$Duration)\ntraffic_mod$Junction <- as.factor(traffic_mod$Junction)\ntraffic_mod$Traffic_Signal <- as.factor(traffic_mod$Traffic_Signal)\n \n\nset.seed(494) #for reproducibility\n\n# Randomly assigns 75% of the data to training.\ntraffic_split <- initial_split(traffic_mod,\n                             prop = .50)\ntraffic_split\n\n\n<Analysis/Assess/Total>\n<49044/49044/98088>\n\ntraffic_training <- training(traffic_split)\ntraffic_testing <- testing(traffic_split)\n\n\n\nHere, we get the data ready for the modeling part by taking the non-predictive variables out of our data set, and converting the predictors’ data type into the correct type. After that, we split our data into testing and training data according to a 50 percentage split.\n\n\n#lasso\nset.seed(494)\n\nlasso_recipe <-\n  recipe(Status ~ .,\n         data = traffic_training) %>%\n  # step_mutate(County,\n  #              County = fct_lump_n(County, n = 5)) %>%\n   step_mutate(City,\n               City = fct_lump_n(City, n = 5)) %>%\n  step_normalize(all_predictors(),\n                 -all_nominal(),\n                 -all_outcomes()) %>%\n  step_dummy(all_nominal(),\n             -all_outcomes())\n\nlasso_recipe %>%\n  prep() %>%\n  juice()\n\n\n# A tibble: 49,044 x 64\n    Month    Hour    Wday Duration Start_Lat Start_Lng Distance\n    <dbl>   <dbl>   <dbl>    <dbl>     <dbl>     <dbl>    <dbl>\n 1  1.21   1.86   -0.0289  -0.0345     0.715     -1.31   -0.252\n 2  1.21  -0.527   0.536   -0.0399     0.682     -1.38   -0.252\n 3  1.21  -0.527   1.10    -0.0399     0.665     -1.38   -0.260\n 4 -1.93   0.495  -1.72    -0.0344     0.621     -1.36   -0.252\n 5 -1.93   1.69   -1.16    -0.0398     0.464     -1.34   -0.252\n 6 -0.504 -0.527  -1.16    -0.0366    -0.161     -1.15   -0.260\n 7 -0.219 -1.04    1.67    -0.0342    -0.155     -1.15   -0.260\n 8 -0.219 -1.89   -0.594   -0.0287    -0.141     -1.16   -0.260\n 9 -0.219 -0.0158 -0.0289  -0.0368    -0.178     -1.17   -0.260\n10 -0.219  1.35   -0.0289  -0.0398    -0.179     -1.16   -0.260\n# … with 49,034 more rows, and 57 more variables: Temperature <dbl>,\n#   Wind_Chill <dbl>, Humidity <dbl>, Pressure <dbl>,\n#   Visibility <dbl>, Wind_Speed <dbl>, Precipitation <dbl>,\n#   Status <fct>, Side_R <dbl>, City_Dallas <dbl>,\n#   City_Houston <dbl>, City_Los.Angeles <dbl>, City_Miami <dbl>,\n#   City_Other <dbl>, Weather_Condition_Cloudy <dbl>,\n#   Weather_Condition_Cloudy...Windy <dbl>,\n#   Weather_Condition_Drizzle <dbl>, Weather_Condition_Fair <dbl>,\n#   Weather_Condition_Fair...Windy <dbl>,\n#   Weather_Condition_Fog <dbl>, Weather_Condition_Haze <dbl>,\n#   Weather_Condition_Heavy.Rain <dbl>,\n#   Weather_Condition_Heavy.Rain...Windy <dbl>,\n#   Weather_Condition_Heavy.Snow <dbl>,\n#   Weather_Condition_Heavy.T.Storm <dbl>,\n#   Weather_Condition_Light.Drizzle <dbl>,\n#   Weather_Condition_Light.Freezing.Rain <dbl>,\n#   Weather_Condition_Light.Rain <dbl>,\n#   Weather_Condition_Light.Rain...Windy <dbl>,\n#   Weather_Condition_Light.Rain.with.Thunder <dbl>,\n#   Weather_Condition_Light.Snow <dbl>,\n#   Weather_Condition_Light.Snow...Windy <dbl>,\n#   Weather_Condition_Mist <dbl>,\n#   Weather_Condition_Mostly.Cloudy <dbl>,\n#   Weather_Condition_Mostly.Cloudy...Windy <dbl>,\n#   Weather_Condition_N.A.Precipitation <dbl>,\n#   Weather_Condition_Overcast <dbl>,\n#   Weather_Condition_Partly.Cloudy <dbl>,\n#   Weather_Condition_Partly.Cloudy...Windy <dbl>,\n#   Weather_Condition_Patches.of.Fog <dbl>,\n#   Weather_Condition_Rain <dbl>,\n#   Weather_Condition_Rain...Windy <dbl>,\n#   Weather_Condition_Scattered.Clouds <dbl>,\n#   Weather_Condition_Shallow.Fog <dbl>,\n#   Weather_Condition_Smoke <dbl>, Weather_Condition_Snow <dbl>,\n#   Weather_Condition_T.Storm <dbl>, Weather_Condition_Thunder <dbl>,\n#   Weather_Condition_Thunder.in.the.Vicinity <dbl>,\n#   Weather_Condition_Wintry.Mix <dbl>, Crossing_TRUE. <dbl>,\n#   Junction_TRUE. <dbl>, Traffic_Signal_TRUE. <dbl>,\n#   Sunrise_Sunset_Night <dbl>, Civil_Twilight_Night <dbl>,\n#   Nautical_Twilight_Night <dbl>, Astronomical_Twilight_Night <dbl>\n\nlasso_mod <-\n  logistic_reg(mixture = 1) %>%\n  set_engine(\"glmnet\") %>%\n  set_args(penalty = tune()) %>%\n  set_mode(\"classification\")\n\nlasso_wf <-\n  workflow() %>%\n  add_recipe(lasso_recipe) %>%\n  add_model(lasso_mod)\n\nset.seed(494) #for reproducible 5-fold\ntraffic_cv <- vfold_cv(traffic_training,\n                       v = 5)\n\npenalty_grid <- grid_regular(penalty(),\n                             levels = 10)\n\n# add ctrl_grid - assures predictions and workflows are saved\nctrl_grid <- control_stack_resamples()\n\nmetric <- metric_set(accuracy)\n\n# tune the model\nlasso_tune <-\n  lasso_wf %>%\n  tune_grid(\n    resamples = traffic_cv,\n    grid = penalty_grid,\n    control = ctrl_grid\n    )\n\nlasso_tune %>%\n  collect_metrics()\n\n\n# A tibble: 20 x 7\n       penalty .metric  .estimator  mean     n std_err .config        \n         <dbl> <chr>    <chr>      <dbl> <int>   <dbl> <chr>          \n 1    1.00e-10 accuracy binary     0.808     5 1.52e-3 Preprocessor1_…\n 2    1.00e-10 roc_auc  binary     0.738     5 7.72e-4 Preprocessor1_…\n 3    1.29e- 9 accuracy binary     0.808     5 1.52e-3 Preprocessor1_…\n 4    1.29e- 9 roc_auc  binary     0.738     5 7.72e-4 Preprocessor1_…\n 5    1.67e- 8 accuracy binary     0.808     5 1.52e-3 Preprocessor1_…\n 6    1.67e- 8 roc_auc  binary     0.738     5 7.72e-4 Preprocessor1_…\n 7    2.15e- 7 accuracy binary     0.808     5 1.52e-3 Preprocessor1_…\n 8    2.15e- 7 roc_auc  binary     0.738     5 7.72e-4 Preprocessor1_…\n 9    2.78e- 6 accuracy binary     0.808     5 1.52e-3 Preprocessor1_…\n10    2.78e- 6 roc_auc  binary     0.738     5 7.72e-4 Preprocessor1_…\n11    3.59e- 5 accuracy binary     0.808     5 1.53e-3 Preprocessor1_…\n12    3.59e- 5 roc_auc  binary     0.738     5 7.75e-4 Preprocessor1_…\n13    4.64e- 4 accuracy binary     0.808     5 1.69e-3 Preprocessor1_…\n14    4.64e- 4 roc_auc  binary     0.737     5 9.58e-4 Preprocessor1_…\n15    5.99e- 3 accuracy binary     0.807     5 2.46e-3 Preprocessor1_…\n16    5.99e- 3 roc_auc  binary     0.730     5 2.33e-3 Preprocessor1_…\n17    7.74e- 2 accuracy binary     0.806     5 2.34e-3 Preprocessor1_…\n18    7.74e- 2 roc_auc  binary     0.5       5 0.      Preprocessor1_…\n19    1.00e+ 0 accuracy binary     0.806     5 2.34e-3 Preprocessor1_…\n20    1.00e+ 0 roc_auc  binary     0.5       5 0.      Preprocessor1_…\n\nbest_param <- lasso_tune %>%\n  select_best(metric = \"accuracy\")\nbest_param\n\n\n# A tibble: 1 x 2\n   penalty .config              \n     <dbl> <chr>                \n1 0.000464 Preprocessor1_Model07\n\nfinal_lasso <- lasso_wf %>%\n  finalize_workflow(best_param) %>%\n  fit(data = traffic_training)\n\nfinal_lasso %>%\n  pull_workflow_fit() %>%\n  tidy()\n\n\n# A tibble: 64 x 3\n   term         estimate  penalty\n   <chr>           <dbl>    <dbl>\n 1 (Intercept) -3.56     0.000464\n 2 Month       -0.353    0.000464\n 3 Hour         0.0587   0.000464\n 4 Wday        -0.000337 0.000464\n 5 Duration     0        0.000464\n 6 Start_Lat    0.135    0.000464\n 7 Start_Lng    0.462    0.000464\n 8 Distance     0.0946   0.000464\n 9 Temperature  0.224    0.000464\n10 Wind_Chill  -0.0209   0.000464\n# … with 54 more rows\n\nThe first model we build is a classification LASSO model, which selects the variables based on the magnitude of their coefficients. We tuned the LASSO model using a level 10 panelty grid, and selected the tuning parameter with the best prediction accuracy as the parameter for the final model. The accuracy for the best LASSO model is 80.956%, which means that the LASSO model predicts the right severity level 80.956% of the times.\n\n\n#classification rf\nset.seed(494)\n\nrf_recipe <-\n  recipe(Status ~ .,\n         data = traffic_training) %>%\n  step_mutate_at(all_numeric(),\n                 fn = ~as.numeric(.))\n\n\nrf_recipe %>%\n  prep() %>%\n  juice()\n\n\n# A tibble: 49,044 x 25\n   Month  Hour  Wday Duration Start_Lat Start_Lng Distance Side  City \n   <dbl> <dbl> <dbl>    <dbl>     <dbl>     <dbl>    <dbl> <fct> <fct>\n 1    12    24     4     44.4      38.6     -121.     0.01 R     Sacr…\n 2    12    10     5     29.6      38.4     -123.     0.01 R     Sant…\n 3    12    10     6     29.7      38.3     -123.     0    L     Seba…\n 4     1    16     1     44.6      38.1     -122.     0.01 R     Peta…\n 5     1    23     2     29.8      37.4     -122.     0.01 L     Sant…\n 6     6    10     2     38.6      34.4     -119.     0    R     Newh…\n 7     7     7     7     45        34.4     -119.     0    R     Vale…\n 8     7     2     3     60        34.5     -119.     0    L     Cast…\n 9     7    13     4     38.1      34.3     -119.     0    R     Simi…\n10     7    21     4     30        34.3     -119.     0    R     Simi…\n# … with 49,034 more rows, and 16 more variables: Temperature <dbl>,\n#   Wind_Chill <dbl>, Humidity <dbl>, Pressure <dbl>,\n#   Visibility <dbl>, Wind_Speed <dbl>, Precipitation <dbl>,\n#   Weather_Condition <fct>, Crossing <fct>, Junction <fct>,\n#   Traffic_Signal <fct>, Sunrise_Sunset <fct>, Civil_Twilight <fct>,\n#   Nautical_Twilight <fct>, Astronomical_Twilight <fct>,\n#   Status <fct>\n\nrf_model <-\n  rand_forest(mtry = tune(),\n              min_n = tune(),\n              trees = 10) %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"ranger\")\n\n\nrf_workflow <-\n  workflow() %>%\n  add_recipe(rf_recipe) %>%\n  add_model(rf_model)\n\n\nrf_penalty_grid <-\n  grid_regular(finalize(mtry(),\n                        traffic_training %>%\n                          select(-Status)),\n               min_n(),\n               levels = 3)\n\n\n# traffic_cv <- vfold_cv(traffic_training,\n#                        v = 5)\n\nrf_tune <-\n  rf_workflow %>%\n  tune_grid(\n    resamples = traffic_cv,\n    grid = rf_penalty_grid,\n    control = control_stack_grid()\n  )\n\nrf_tune %>%\n  collect_metrics()\n\n\n# A tibble: 18 x 8\n    mtry min_n .metric  .estimator  mean     n std_err .config        \n   <int> <int> <chr>    <chr>      <dbl> <int>   <dbl> <chr>          \n 1     1     2 accuracy binary     0.806     5 0.00225 Preprocessor1_…\n 2     1     2 roc_auc  binary     0.786     5 0.00307 Preprocessor1_…\n 3    12     2 accuracy binary     0.842     5 0.00184 Preprocessor1_…\n 4    12     2 roc_auc  binary     0.851     5 0.00291 Preprocessor1_…\n 5    24     2 accuracy binary     0.841     5 0.00267 Preprocessor1_…\n 6    24     2 roc_auc  binary     0.848     5 0.00259 Preprocessor1_…\n 7     1    21 accuracy binary     0.807     5 0.00210 Preprocessor1_…\n 8     1    21 roc_auc  binary     0.796     5 0.00191 Preprocessor1_…\n 9    12    21 accuracy binary     0.846     5 0.00172 Preprocessor1_…\n10    12    21 roc_auc  binary     0.868     5 0.00193 Preprocessor1_…\n11    24    21 accuracy binary     0.844     5 0.00125 Preprocessor1_…\n12    24    21 roc_auc  binary     0.862     5 0.00229 Preprocessor1_…\n13     1    40 accuracy binary     0.807     5 0.00210 Preprocessor1_…\n14     1    40 roc_auc  binary     0.788     5 0.00353 Preprocessor1_…\n15    12    40 accuracy binary     0.848     5 0.00105 Preprocessor1_…\n16    12    40 roc_auc  binary     0.872     5 0.00170 Preprocessor1_…\n17    24    40 accuracy binary     0.844     5 0.00189 Preprocessor1_…\n18    24    40 roc_auc  binary     0.866     5 0.00212 Preprocessor1_…\n\nAfter conducting the LASSO model, we also build the random forest model, which builds 10 trees and gives out the mode of the predictions of these 10 trees. We thought that this model might be more accurate than the LASSO model although it is also more computationally inefficient. We used a panelty grid of level 3 to tune our random forest model, and the tuning parameter with the largest accuracy is with mtry = 12 and min_n = 40. The largest accuracy is 84.693%, which is higher than the LASSO model.\n\n\n#decision trees\nset.seed(494)\n\ntree_model <-\n  decision_tree() %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"rpart\")\n\ntree_workflow <-\n  workflow() %>%\n  add_recipe(rf_recipe) %>%  \n  add_model(tree_model)\n\ntree_fit <-\n  tree_workflow %>%\n  fit_resamples(traffic_cv,\n                # metrics = metric,\n                control = control_stack_resamples()\n  )\n\ncollect_metrics(tree_fit)\n\n\n# A tibble: 2 x 6\n  .metric  .estimator  mean     n std_err .config             \n  <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n1 accuracy binary     0.824     5 0.00143 Preprocessor1_Model1\n2 roc_auc  binary     0.686     5 0.00309 Preprocessor1_Model1\n\nFinally, in order to create a stacked model, we build a third model which is just a simple classification decision tree. The accuracy for the decision tree model is 82.559%, which is also higher than the LASSO.\n\n\n# model stacking\nlasso_tune %>%\n  collect_metrics()\n\n\n# A tibble: 20 x 7\n       penalty .metric  .estimator  mean     n std_err .config        \n         <dbl> <chr>    <chr>      <dbl> <int>   <dbl> <chr>          \n 1    1.00e-10 accuracy binary     0.808     5 1.52e-3 Preprocessor1_…\n 2    1.00e-10 roc_auc  binary     0.738     5 7.72e-4 Preprocessor1_…\n 3    1.29e- 9 accuracy binary     0.808     5 1.52e-3 Preprocessor1_…\n 4    1.29e- 9 roc_auc  binary     0.738     5 7.72e-4 Preprocessor1_…\n 5    1.67e- 8 accuracy binary     0.808     5 1.52e-3 Preprocessor1_…\n 6    1.67e- 8 roc_auc  binary     0.738     5 7.72e-4 Preprocessor1_…\n 7    2.15e- 7 accuracy binary     0.808     5 1.52e-3 Preprocessor1_…\n 8    2.15e- 7 roc_auc  binary     0.738     5 7.72e-4 Preprocessor1_…\n 9    2.78e- 6 accuracy binary     0.808     5 1.52e-3 Preprocessor1_…\n10    2.78e- 6 roc_auc  binary     0.738     5 7.72e-4 Preprocessor1_…\n11    3.59e- 5 accuracy binary     0.808     5 1.53e-3 Preprocessor1_…\n12    3.59e- 5 roc_auc  binary     0.738     5 7.75e-4 Preprocessor1_…\n13    4.64e- 4 accuracy binary     0.808     5 1.69e-3 Preprocessor1_…\n14    4.64e- 4 roc_auc  binary     0.737     5 9.58e-4 Preprocessor1_…\n15    5.99e- 3 accuracy binary     0.807     5 2.46e-3 Preprocessor1_…\n16    5.99e- 3 roc_auc  binary     0.730     5 2.33e-3 Preprocessor1_…\n17    7.74e- 2 accuracy binary     0.806     5 2.34e-3 Preprocessor1_…\n18    7.74e- 2 roc_auc  binary     0.5       5 0.      Preprocessor1_…\n19    1.00e+ 0 accuracy binary     0.806     5 2.34e-3 Preprocessor1_…\n20    1.00e+ 0 roc_auc  binary     0.5       5 0.      Preprocessor1_…\n\nrf_tune %>%\n  collect_metrics()\n\n\n# A tibble: 18 x 8\n    mtry min_n .metric  .estimator  mean     n std_err .config        \n   <int> <int> <chr>    <chr>      <dbl> <int>   <dbl> <chr>          \n 1     1     2 accuracy binary     0.806     5 0.00225 Preprocessor1_…\n 2     1     2 roc_auc  binary     0.786     5 0.00307 Preprocessor1_…\n 3    12     2 accuracy binary     0.842     5 0.00184 Preprocessor1_…\n 4    12     2 roc_auc  binary     0.851     5 0.00291 Preprocessor1_…\n 5    24     2 accuracy binary     0.841     5 0.00267 Preprocessor1_…\n 6    24     2 roc_auc  binary     0.848     5 0.00259 Preprocessor1_…\n 7     1    21 accuracy binary     0.807     5 0.00210 Preprocessor1_…\n 8     1    21 roc_auc  binary     0.796     5 0.00191 Preprocessor1_…\n 9    12    21 accuracy binary     0.846     5 0.00172 Preprocessor1_…\n10    12    21 roc_auc  binary     0.868     5 0.00193 Preprocessor1_…\n11    24    21 accuracy binary     0.844     5 0.00125 Preprocessor1_…\n12    24    21 roc_auc  binary     0.862     5 0.00229 Preprocessor1_…\n13     1    40 accuracy binary     0.807     5 0.00210 Preprocessor1_…\n14     1    40 roc_auc  binary     0.788     5 0.00353 Preprocessor1_…\n15    12    40 accuracy binary     0.848     5 0.00105 Preprocessor1_…\n16    12    40 roc_auc  binary     0.872     5 0.00170 Preprocessor1_…\n17    24    40 accuracy binary     0.844     5 0.00189 Preprocessor1_…\n18    24    40 roc_auc  binary     0.866     5 0.00212 Preprocessor1_…\n\ntree_fit %>%\n  collect_metrics()\n\n\n# A tibble: 2 x 6\n  .metric  .estimator  mean     n std_err .config             \n  <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n1 accuracy binary     0.824     5 0.00143 Preprocessor1_Model1\n2 roc_auc  binary     0.686     5 0.00309 Preprocessor1_Model1\n\n\n\ntraffic_stack <-\n  stacks() %>%\n  add_candidates(lasso_tune) %>%\n  add_candidates(rf_tune) %>%\n  add_candidates(tree_fit)\n\n\n\n\n\ntraffic_blend <-\n  traffic_stack %>%\n  blend_predictions()\ntraffic_blend\n\n\n# A tibble: 9 x 3\n  member                    type          weight\n  <chr>                     <chr>          <dbl>\n1 .pred_Severe_rf_tune_1_8  rand_forest    1.80 \n2 .pred_Severe_rf_tune_1_5  rand_forest    1.47 \n3 .pred_Severe_rf_tune_1_2  rand_forest    0.871\n4 .pred_Severe_tree_fit_1_1 decision_tree  0.783\n5 .pred_Severe_rf_tune_1_9  rand_forest    0.642\n6 .pred_Severe_rf_tune_1_4  rand_forest    0.639\n7 .pred_Severe_rf_tune_1_3  rand_forest    0.604\n8 .pred_Severe_rf_tune_1_6  rand_forest    0.578\n9 .pred_Severe_rf_tune_1_7  rand_forest    0.225\n\n\n\ntraffic_final_stack <- traffic_blend %>%\n  fit_members()\n\n#saveRDS(traffic_final_stack, \"traffic_final_stacked.rds\")\n\n\n\nShiny App\nUser Interface (UI)\nWe also developed a shiny app that included all of the relevant variables of our analysis. The purpose of the app was to allow the user to plug different values for environmental conditions or locations to see how the severity predicted changes. When creating the UI, we decided to use sliders for each of the numerical variables. The sliders referenced minimum and maximum variables that I had defined previously in line 553. For variables with multiple levels like City and Weather condition, we decided to create a list of levels that we referenced later in the selectInput function. This saved us the time of having to type the name of each of the variables’ levels. We also formatted the app using the bslib package, we included this in the theme argument in line 566. We used the package to define a font, the primary, secondary and bootswatch colors. Given the large number of variables included, we also added a scrollable side panel in lineS 577 to 580.\nServer\nWe then defined the input variables for the server. We run into errors regarding incoherence between the variables used in the UI and the server. We decided to assign arbitrary values to the input variables and one by one we tested what variables were not being recognized. This also allowed us to detect certain variables that we no longer considered relevant, like county. We also had to pre-processing to change the names of variables like “wind_chill(F)” which were for some reason not recognized by the tibble function. Then, in lines 836 - 840 we defined the output by using the stacked model in our data and asked the app to show our prediction.\nEmbedded app\nWe decided to use an embedded app in a different r-markdown file considering that the size of our rsd file (our model) was too big to be deployed. We added a “runtime: shiny” argument in the YAML. Within an r code chunk, we still had to call the lists of variable levels and the minimum and maximum variable values before copying the code for our app. We used the shinyApp function in line 564 before plugging the remainder of our code. The app works but it could only be seen by people that have R installed in the computers and that have the project file. This is an issue that we further need to work on. We believe there are ways to reduce the size of our rsd file.\nShiny App Code\n\n\n# traffic_mod <- readRDS(\"traffic_final_stacked.rds\")\n# traffic_mod <- readRDS(\"traffic_final_stacked.rds\")\n# \n# Cities <-\n#   traffic_mod$train  %>%\n#   select(City) %>%\n#   distinct(City) %>%\n#   arrange(City) %>%\n#   pull(City)\n# \n# Weather <-\n#   traffic_mod$train  %>%\n#   select(Weather_Condition) %>%\n#   distinct(Weather_Condition) %>%\n#   arrange(Weather_Condition) %>%\n#   pull(Weather_Condition)\n# \n# \n# # Find min's, max's, and median's for quantitative vars:\n# \n# stats_num <-\n#   traffic_mod$train  %>%\n#   select(where(is.numeric)) %>%\n#   pivot_longer(cols = everything(),\n#                names_to = \"variable\",\n#                values_to = \"value\") %>%\n#   group_by(variable) %>%\n#   summarize(min_val = min(value),\n#             max_val = max(value),\n#             med_val = median(value))\n# \n# shinyApp(\n#   ui <- fluidPage(\n#   theme = bs_theme(primary = \"#123B60\",\n#                    secondary = \"#D44420\",\n#                    base_font = list(font_google(\"Raleway\"), \"-apple-system\",\n#                                     \"BlinkMacSystemFont\", \"Segoe UI\", \"Helvetica Neue\", \"Arial\",\n#                                     \"sans-serif\", \"Apple Color Emoji\", \"Segoe UI Emoji\",\n#                                     \"Segoe UI Symbol\"),\n#                    bootswatch = \"sandstone\"),\n#   # Application title\n#   sidebarLayout(\n#     sidebarPanel(\n#       # added this for scrollable side panel:\n#       tags$head(tags$style(\n#         type = 'text/css',\n#         'form.well { max-height: 600px; overflow-y: auto; }'\n#       )),\n#       sliderInput(inputId = \"Hour\",\n#               label = \"Hour of Accident\",\n#               min = stats_num %>%\n#                 filter(variable ==\"Hour\") %>%\n#                 pull(min_val),\n#               max = stats_num %>%\n#                 filter(variable ==\"Hour\") %>%\n#                 pull(max_val),\n#               value = stats_num %>%\n#                 filter(variable ==\"Hour\") %>%\n#                 pull(med_val),\n#               step = 1,\n#               round = TRUE),\n#       sliderInput(inputId = \"Month\",\n#               label = \"Month of Accident\",\n#               min = stats_num %>%\n#                 filter(variable ==\"Month\") %>%\n#                 pull(min_val),\n#               max = stats_num %>%\n#                 filter(variable ==\"Month\") %>%\n#                 pull(max_val),\n#               value = stats_num %>%\n#                 filter(variable ==\"Month\") %>%\n#                 pull(med_val),\n#               step = 1,\n#               round = TRUE),\n#       sliderInput(inputId = \"Wday\",\n#               label = \"Week day of Accident\",\n#               min = stats_num %>%\n#                 filter(variable ==\"Wday\") %>%\n#                 pull(min_val),\n#               max = stats_num %>%\n#                 filter(variable ==\"Wday\") %>%\n#                 pull(max_val),\n#               value = stats_num %>%\n#                 filter(variable ==\"Wday\") %>%\n#                 pull(med_val),\n#               step = 1,\n#               round = TRUE),\n#       sliderInput(inputId = \"Duration\",\n#               label = \"Duration of Accident in seconds\",\n#               min = stats_num %>%\n#                 filter(variable ==\"Duration\") %>%\n#                 pull(min_val),\n#               max = stats_num %>%\n#                 filter(variable ==\"Duration\") %>%\n#                 pull(max_val),\n#               value = stats_num %>%\n#                 filter(variable ==\"Duration\") %>%\n#                 pull(med_val),\n#               step = 1,\n#               round = TRUE),\n#       sliderInput(inputId = \"Start_Lat\",\n#               label = \"Starting latitude of the Accident\",\n#               min = stats_num %>%\n#                 filter(variable ==\"Start_Lat\") %>%\n#                 pull(min_val),\n#               max = stats_num %>%\n#                 filter(variable ==\"Start_Lat\") %>%\n#                 pull(max_val),\n#               value = stats_num %>%\n#                 filter(variable ==\"Start_Lat\") %>%\n#                 pull(med_val),\n#               step = 1,\n#               round = TRUE),\n#       sliderInput(inputId = \"Start_Lng\",\n#               label = \"Starting longitude of the Accident\",\n#               min = stats_num %>%\n#                 filter(variable ==\"Start_Lng\") %>%\n#                 pull(min_val),\n#               max = stats_num %>%\n#                 filter(variable ==\"Start_Lng\") %>%\n#                 pull(max_val),\n#               value = stats_num %>%\n#                 filter(variable ==\"Start_Lng\") %>%\n#                 pull(med_val),\n#               step = 1,\n#               round = TRUE),\n#       sliderInput(inputId = \"Distance\",\n#               label = \"Distance of the Accident\",\n#               min = stats_num %>%\n#                 filter(variable ==\"Distance\") %>%\n#                 pull(min_val),\n#               max = stats_num %>%\n#                 filter(variable ==\"Distance\") %>%\n#                 pull(max_val),\n#               value = stats_num %>%\n#                 filter(variable ==\"Distance\") %>%\n#                 pull(med_val),\n#               step = 1,\n#               round = TRUE),\n#       selectInput(inputId = \"Side\",\n#               label = \"Side of the street where the accident happened\",\n#               choices = list(Right = \"R\",\n#                              Left = \"L\")),\n#       selectInput(inputId = \"City\",\n#                   label = \"City where the accident happened\",\n#                   choices = Cities),\n#       sliderInput(inputId = \"Temperature\",\n#               label = \"Temperature when accident happened\",\n#               min = stats_num %>%\n#                 filter(variable ==\"Temperature\") %>%\n#                 pull(min_val),\n#               max = stats_num %>%\n#                 filter(variable ==\"Temperature\") %>%\n#                 pull(max_val),\n#               value = stats_num %>%\n#                 filter(variable ==\"Temperature\") %>%\n#                 pull(med_val),\n#               step = 1,\n#               round = TRUE),\n#       sliderInput(inputId = \"Wind_Chill\",\n#               label = \"Wind chill in degrees Farenheit when accident happened\",\n#               min = stats_num %>%\n#                 filter(variable ==\"Wind_Chill\") %>%\n#                 pull(min_val),\n#               max = stats_num %>%\n#                 filter(variable ==\"Wind_Chill\") %>%\n#                 pull(max_val),\n#               value = stats_num %>%\n#                 filter(variable ==\"Wind_Chill\") %>%\n#                 pull(med_val),\n#               step = 1,\n#               round = TRUE),\n#       sliderInput(inputId = \"Humidity\",\n#               label = \"Humidity when accident happened\",\n#               min = stats_num %>%\n#                 filter(variable ==\"Humidity\") %>%\n#                 pull(min_val),\n#               max = stats_num %>%\n#                 filter(variable ==\"Humidity\") %>%\n#                 pull(max_val),\n#               value = stats_num %>%\n#                 filter(variable ==\"Humidity\") %>%\n#                 pull(med_val),\n#               step = 1,\n#               round = TRUE),\n#       sliderInput(inputId = \"Pressure\",\n#               label = \"Pressure when accident happened\",\n#               min = stats_num %>%\n#                 filter(variable ==\"Pressure\") %>%\n#                 pull(min_val),\n#               max = stats_num %>%\n#                 filter(variable ==\"Pressure\") %>%\n#                 pull(max_val),\n#               value = stats_num %>%\n#                 filter(variable ==\"Pressure\") %>%\n#                 pull(med_val),\n#               step = 1,\n#               round = TRUE),\n#       sliderInput(inputId = \"Visibility\",\n#               label = \"Visibility when accident happened\",\n#               min = stats_num %>%\n#                 filter(variable ==\"Visibility\") %>%\n#                 pull(min_val),\n#               max = stats_num %>%\n#                 filter(variable ==\"Visibility\") %>%\n#                 pull(max_val),\n#               value = stats_num %>%\n#                 filter(variable ==\"Visibility\") %>%\n#                 pull(med_val),\n#               step = 1,\n#               round = TRUE),\n#       sliderInput(inputId = \"Wind_Speed\",\n#               label = \"Wind speed when accident happened\",\n#               min = stats_num %>%\n#                 filter(variable ==\"Wind_Speed\") %>%\n#                 pull(min_val),\n#               max = stats_num %>%\n#                 filter(variable ==\"Wind_Speed\") %>%\n#                 pull(max_val),\n#               value = stats_num %>%\n#                 filter(variable ==\"Wind_Speed\") %>%\n#                 pull(med_val),\n#               step = 1,\n#               round = TRUE),\n#       sliderInput(inputId = \"Precipitation\",\n#               label = \"Precipitation when accident happened in inches\",\n#               min = stats_num %>%\n#                 filter(variable ==\"Precipitation\") %>%\n#                 pull(min_val),\n#               max = stats_num %>%\n#                 filter(variable ==\"Precipitation\") %>%\n#                 pull(max_val),\n#               value = stats_num %>%\n#                 filter(variable ==\"Precipitation\") %>%\n#                 pull(med_val),\n#               step = 1,\n#               round = TRUE),\n#       selectInput(inputId = \"Crossing\",\n#               label = \"Is there a crossing where the accident happened?\",\n#               choices = list(Yes = \"TRUE\",\n#                              No = \"FALSE\")),\n#       selectInput(inputId = \"Junction\",\n#               label = \"Is there a junction where the accident happened?\",\n#               choices = list(Yes = \"TRUE\",\n#                              No = \"FALSE\")),\n#       selectInput(inputId = \"Traffic_Signal\",\n#               label = \"Is there a traffic signal where the accident happened?\",\n#               choices = list(Yes = \"TRUE\",\n#                              No = \"FALSE\")),\n#       selectInput(inputId = \"Sunrise_Sunset\",\n#               label = \"Is it night or day?\",\n#               choices = list(Night = \"Night\",\n#                              Day = \"Day\")),\n#       selectInput(inputId = \"Civil_Twilight\",\n#               label = \"Is there enough natural light to be day?\",\n#               choices = list(Yes = \"Day\",\n#                              No = \"Night\")),\n#       selectInput(inputId = \"Nautical_Twilight\",\n#               label = \"Is it nautical day or night?\",\n#               choices = list(\"Day\",\"Night\")),\n#       selectInput(inputId = \"Astronomical_Twilight\",\n#               label = \"Was the sky illuminated by the sun?\",\n#               choices = list(Yes = \"Day\",\n#                              No = \"Night\")),\n#       selectInput(inputId = \"Weather_Condition\",\n#               label = \"Weather condition when accident happened\",\n#               choices = Weather),\n#       submitButton(text = \"Get the Prediction\"),\n#     ),\n#       mainPanel(\n#         verbatimTextOutput(\"Pred\")\n#       )\n#    )\n# ),\n# server = function (input,output) {\n#   output$Pred <- renderPrint({\n#     data <- tibble(\n#       # TMC=input$TMC,\n#       Month=input$Month,\n#       Hour=input$Hour,\n#       Wday=input$Wday,\n#       Duration=input$Duration,\n#       Start_Lat=input$Start_Lat,\n#       Start_Lng=input$Start_Lng,\n#       Distance=input$Distance,\n#       Side=input$Side,\n#       City=input$City,\n#       Temperature=input$Temperature,\n#       Wind_Chill=input$Wind_Chill,\n#       Humidity=input$Humidity,\n#       Pressure=input$Pressure,\n#       Visibility=input$Visibility,\n#       Wind_Speed=input$Wind_Speed,\n#       Precipitation=input$Precipitation,\n#       Crossing=input$Crossing,\n#       Junction=input$Junction,\n#       Traffic_Signal=input$Traffic_Signal,\n#       Sunrise_Sunset=input$Sunrise_Sunset,\n#       Civil_Twilight=input$Civil_Twilight,\n#       Nautical_Twilight=input$Nautical_Twilight,\n#       Astronomical_Twilight=input$Astronomical_Twilight,\n#       Weather_Condition=input$Weather_Condition\n#     )\n#     pred <-\n#       predict(traffic_mod,data) %>%\n#       pull(.pred_class)\n# \n#     pred}\n#   )\n# },\n# \n#   options = list(height = 500)\n# )\n\n\n\n\n\n\n",
    "preview": "posts/2021-05-07-accidentbehind/behind.jpg",
    "last_modified": "2021-05-07T18:49:46-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-05-07-accidentprediction/",
    "title": "Prediction of the severity of car accidents",
    "description": "Analyze the environmental factors that are more strongly associated with car accident in the United States to create a prediction model.",
    "author": [
      {
        "name": "Juthi Dewan, Coco Li, Franco Salinas",
        "url": {}
      }
    ],
    "date": "2021-05-07",
    "categories": [],
    "contents": "\n\n\n\n\n\n\nIntroduction\nThe Fatality Analysis Reporting System indicated that an estimate of 8870 people died in motor vehicle traffic crashes in the second quarter of 2020 (NHTSA, 2020). This analysis is intended to bring light to the main environmental conditions that are associated with the severity of a car accident. For the purpose of this study we defined severity as the accident’s impact on traffic.\nData\nThe data we used has 47 variables and 3 million observations for different car accidents. The data was collected from February 2016 to December 2020 for the 49 states of the US. The data base has been constructed partly by Moosavi, Sobhan, Mohammad Hossein Samavatian, Srinivasan Parthasarathy, and Rajiv Ramnath as “A Countrywide Traffic Accident Dataset” (2019). The other part of the data base was constructed by Moosavi, Sobhan, Mohammad Hossein Samavatian, Srinivasan Parthasarathy, Radu Teodorescu, and Rajiv Ramnath for their database “Accident Risk Prediction based on Heterogeneous Sparse Data: New Dataset and Insights.”\nModels\nUsing this data set, we predicted the Severity of an accident using stacked LASSO, Forest and classification three. Stacking combines predictions from many different models into a “super” predictor. In this case we would be averaging the predictions of the LASSO, Forest and classification three.\nPre-processing\n\n# A tibble: 3 x 2\n  `Variables to drop` `NA proportion`\n  <chr>                         <dbl>\n1 End_Lat                       0.636\n2 End_Lng                       0.636\n3 Number                        0.633\n\n\n\n\nWe chose not to use multiple of the 47 variables that we considered weren’t relevant to the analysis we were conducting. As we can see in the table, end latitude, end longitude’s proportion of NA values are higher than 50%. Given the distribution of wind direction through the different severity levels, we decided that the variable is uninformative. The description, bump, traffic calming, give way, no exit, railway, roundabout, station, stop, amenity, street, zip code, country, turning loop, county and TMC code weren’t informative either given the distribution between the severity categories or near-zero variance. We also decided to group the four levels of severity status into two categories. Given the large amount of levels for the weather condition variable I decided to use only the levels that had a count higher than 20.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlobal interpretation\nGlobal model interpretations explain the overall relationships between the predictor variables and the response.\nModel performance\n\n\n\n\nPreparation of a new explainer is initiated\n  -> model label       :  rf \n  -> data              :  49044  rows  24  cols \n  -> data              :  tibble converted into a data.frame \n  -> target variable   :  49044  values \n  -> predict function  :  yhat.workflow  will be used ( [33m default [39m )\n  -> predicted values  :  No value for predict function target column. ( [33m default [39m )\n  -> model_info        :  package tidymodels , ver. 0.1.2 , task classification ( [33m default [39m ) \n  -> predicted values  :  numerical, min =  0 , mean =  0.1959684 , max =  1  \n  -> residual function :  difference between y and yhat ( [33m default [39m )\n  -> residuals         :  numerical, min =  -0.8282602 , mean =  -0.001734642 , max =  0.9717742  \n [32m A new explainer has been created! [39m \nPreparation of a new explainer is initiated\n  -> model label       :  lasso \n  -> data              :  49044  rows  24  cols \n  -> data              :  tibble converted into a data.frame \n  -> target variable   :  49044  values \n  -> predict function  :  yhat.workflow  will be used ( [33m default [39m )\n  -> predicted values  :  No value for predict function target column. ( [33m default [39m )\n  -> model_info        :  package tidymodels , ver. 0.1.2 , task classification ( [33m default [39m ) \n  -> predicted values  :  numerical, min =  0.00227835 , mean =  0.1942335 , max =  0.9658464  \n  -> residual function :  difference between y and yhat ( [33m default [39m )\n  -> residuals         :  numerical, min =  -0.8613942 , mean =  2.375737e-07 , max =  0.9934433  \n [32m A new explainer has been created! [39m \nPreparation of a new explainer is initiated\n  -> model label       :  tree \n  -> data              :  49044  rows  24  cols \n  -> data              :  tibble converted into a data.frame \n  -> target variable   :  49044  values \n  -> predict function  :  yhat.workflow  will be used ( [33m default [39m )\n  -> predicted values  :  No value for predict function target column. ( [33m default [39m )\n  -> model_info        :  package tidymodels , ver. 0.1.2 , task classification ( [33m default [39m ) \n  -> predicted values  :  numerical, min =  0.1165841 , mean =  0.1942337 , max =  0.7279736  \n  -> residual function :  difference between y and yhat ( [33m default [39m )\n  -> residuals         :  numerical, min =  -0.7279736 , mean =  3.562719e-18 , max =  0.8834159  \n [32m A new explainer has been created! [39m \n\n\n\n\n\n\n\nAs we can see from the histograms, the majority of the residual values are clustered in values close to 0. The model with the lowest average residual value is lasso, followed by the tree model and the forest model. While the lasso and the forest models are left skewed this is not the case for the classification tree. The residuals for the classification tree are more spread towards the positive and the negative values, showing that this model might be less precise than the other two.\nVariable of importance\n\n\n\n\n\n\n\n\n\nWe can see that there are more variables with greater importance in the lasso and the forest models. This means that, the values that are at the top generate great increases in the performance of the model when permuted relative to the other variables. The length of the bars indicate how much the performance increases when that variable is permuted. Permuting is the process of exchanging the values of a variable between observations. We can see that the lasso and forest model’s performance increase when start longitude is permuted. This is the case for city in the classification tree.\nCeteris-Paribus Profile\nThis profiles show how one variable affects the outcome holding all other variables fixed for one observation.\n\n\n\nIn this graph we can see how changes to the values of the starting longitude affect the probability that the accident is Severe, holding all the other variables constant. As we can see, the probability that an accident is severe changes drastically whit different start longitude values, reflecting the importance of the variable for the forest model.\nPartial Dependence Plots\nRemember the CP profile used only one observation? A partial dependence plot is created by averaging the CP profiles for a sample of observations. The partial dependence profile is the blue line. We can see that overall, changes to the value of the longitude affect the probability that an accident is sever significantly. If we were to analyze the dependence plot for lasso the lines we would see inclined parallel lines given that lasso is additive. This is not very informative, that’s why we decided to focus on the forest model.\n\n\n\nLocal Model Interpretation\nLocal model interpretation helps us understand the impact of variables on individual observations. We will focus on the random forest model given that it is the model with the higher accuracy. We would like to do the interpretation for the stacked model, but this isn’t possible using DALEX and DALEXtra. Considering that out stacked model has few models stacked, we think that doing the analysis of the random forest model should give us a fair idea of what is the local importance of our variables.\nShapley Additive Explanations (SHAP)\nFor Break Down profiles, the contributions of variables would change with the order in which the variables are considered in the random forest model. Therefore we decided to use the SHAP.\n\n\n\nEach bar shows the average contribution of each variable’s value to the predicted severity for this observed accident. We can see that a duration of 44.62 contributes almost an additional 0.15 to the predicted probability of a severe accident, on average. The boxplot shows the variation across permutations of the order of the variable. A large variation would mean that we should be less confident in its exact effect. For example we see that Nautical twilight has a large variation; therefore, we aren’t confident about it’s contribution to the prediction.\nLocal Interpretable Model-agnostic Explanations (LIME)\n\n# A tibble: 1 x 3\n  model_r2 model_prediction prediction\n     <dbl>            <dbl>      <dbl>\n1    0.217            0.155      0.178\n\n\nThe table shows the predicted value from the local model as 0.2624304. and the prediction from the original forest model of 6.17 This graph shows us the predicted value from the original random forest model as “Prediction”. The r-squared value of the model is shown as the “Explanation fit” showing that the model explains 25% of the variance of our sample. The bars show that distance is the most important variable in the local model.\nConclusion:\nWe have explored how changes in the values of variables for an observation can affect the predicted outcome using two methods. Both methods show similar results; however, it’s important to note that while the SHAP plot includes more variables, it is less reliable. We have seen also that starting longitude is the most important variable according to two of our models, and that changing its value for an observation holding the other variables constant changes the predicted value significantly. Even with slight differences all of the models showed similar results for the variables of interest, showing that the results are coherent and reasonable. We can conclude that overall, distance, starting longitude, duration and month are the most important variables for the prediction of the severity of an accident. This means that depending on the month, the weather or the amount of traffic could be associated with the severity of an accident. The length of the road extent affected by the accident also is a good predictor for severity. The time between the start of the accident and the end of the impact on traffic flow also are associated with the severity. Lastly, there is a strong association between starting longitude of the accident and the severity which could be driven by demographic or other non-controlled characteristics that differentiate western to eastern states.\nRepercussions:\nOur analysis only focused on environmental factors that are associated with the severity of an accident. There are multiple other factors that we didn’t include like speed or whether the driver was intoxicated. Therefore, the model and the analysis is not complete and should not be regarded as such. On the other hand, this analysis didn’t account for demographics; therefore, the differences in locations do not respond to anyone’s ethnicity, education, sex or any other identity. This model is meant to further analyze what external factors contribute the most to the severity of an accident.\n\n\n\n",
    "preview": "posts/2021-05-07-accidentprediction/accident.png",
    "last_modified": "2021-05-07T10:38:37-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-19-buildingdistill/",
    "title": "Changes on the stock market during the COVID-19 pandemic",
    "description": "Analysis of the change in stock market prices, cumulative returns, and trading volumes during the COVID-19 pandemic for the biggest companies in each sector.",
    "author": [
      {
        "name": "Franco Salinas, Duc Ngo, Vichearith Meas, and Max Wang",
        "url": {}
      }
    ],
    "date": "2021-03-19",
    "categories": [],
    "contents": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction:\nWe have seen multiple changes around the world after one year. The main reason for that is because of the coronavirus pandemic. Firstly thought of as a common flu in Wuhan, China, the pandemic has spread around the world, with more than 100 million cases and more than 2.5 million deaths. Especially in the US, we have seen around 30 million cases with more than 500,000 deaths resulting from the pandemic (according to Worldometer) . Coronavirus has changed our perspective in multiple areas, however, in this blog post, we focus specifically on its impact on companies and sectors in the US stock market. We will see what are the changes in stock price, in market capitalization as well as how each sector has changed after the pandemic.\nFirst, as we have more than 2,800 companies trading in the New York Stock Exchange (NYSE), it is really difficult to do it in a short period of time. Instead, we used the website Yahoo Finance to find the data for the 3 companies with highest market cap in different 11 sectors: Communication Discretionary, Communication Services, Consumer Staples, Energy, Financials, Health Care, Industrials, Material, Real Estate, Technology, Utilities. Then, to compare the results from different sectors to the wider market, we used the SPY, which is an ETF for the S&P 500 that tracks the changes in the value of the biggest 500 companies in the market. We classified the SPY data as “All fields”. We tracked the data from 2020-01-02 to 2021-03-04, using the volume and the closing price. We assembled the data in a time series format creating dummy variables for each sector, then we chose to use the closing price given that it shows the final value that market participants attribute to a share after trading, providing a more realistic depiction of its value. Then, we calculated the Cumulative Rate of Return everyday by using the following formula:\n\\[\nreturn = \\frac{Price_i - Price_1}{Price_1}* 100\n\\]\nWe also obtained the market cap by multiplying the number of market shares of each company by the Closing price each day. We retrieved the number of shares from the SEC filing website.\nAfter we have found the way to conduct research with our data, we have created three main questions:\na. Which companies and which sectors have performed the best during the period? What is the proportion of negative and positive daily average return for each sector?\nb. What are the changes in market capitalization?\nc. Which companies have the highest trading volume? Which sectors have the highest number of trading?\nThe performance of companies and the sectors:\nReturn by sectors:\n\n\n\n\n\n\n\n\n\n\n\n\nOverall, it seems that the return for the Communication Discretionary is consistently growing, almost in a positively linear way. The lowest return is still not below 0. Changes in return of Communication Services and Technology follow a similar trend to that of Communication Discretionary in shape, but have a smaller magnitude (-20~ 60 for CS, -25~100 for T, compared to 0~300 for CD). Consumer Staples fluctuated mildly, falling sharply first to -10 and rebounding quickly, then growing steadily to finally decrease back to 0 return. The situation for the Energy and Financials sectors is the worst. Since the first day, Energy’s cumulative return is negative. Only the last month is positive for the Financials sector. Health Care, Industrials, Material, Real Estate, and Utilities followed a similar trend, but these sectors’ conditions are better than that of Energy and Financials. Health Care, Industrials, Material, Real Estate, and Utilities have similar trends compared to the overall market. In general, the return sharply dropped from 2020-1-1 to April 2020 and then rebounded at a lower rate, back to 0 in August 2020. After, it oscillated up and down until 2020 November and then continuously increased slowly. So far, return as a whole is 20%.\n\n\n\nAdditionally, we try to see the proportion of time in which the average of the cumulative return of the best stocks per sector have been over or under the base price. In here, it comes as no surprise as Communication Discretionary was the sector with the highest proportion of positive returns relative to the negative returns. This meaning, that 97% of the time the Communication Discretionary sector had positive returns relative to the initial price. Had a person bought shares of the companies in this sector on January 2nd, they would have slept peacefully 97% of the time. On the other hand, the Energy sector was the sector with the highest proportion of negative returns relative to positive returns. As we can see, the pandemic affected different sectors in a different way. The least affected sectors are Communication Discretionary and Technology, one of the reasons being that their consumption doesn’t require in person presence. In the case of energy, a drop in the demand of petroleum as a consequence of the lockdown in different countries decreased their prices. Given that oil companies’ revenue depends on price, the investors sold their shares expecting losses on the oil industry. The Financials sector was also drastically affected, as the FED lowered the interest rates and there were negative interest rates in Europe, understanding that banks are the suppliers of capital, lower (or negative) interest rates affect banks’ revenue.\nReturn by companies:\n\n\n\n\n\n\nWhen we go deeper into each company in the field, it will be Tesla that grows the most during the period. Tesla’s stock price has increased more than 9 times for the period, followed by Nvidia as the company tripled in value. After that, we can see Apple, Amazon, Microsoft or Facebook increased quite heavily during the period, all increasing by 100-150%. Given that the Energy and Financial sectors did not see an increase in the stock price, it comes as no surprise that we can’t see any companies in those sectors in the top ten list.\nWhen we look closer at the shorter period, in March or April, we can see Walmart (one of the companies in the Consumer Staples section) that was in top 10 companies with the greatest return. However, after this period, it was still companies in Communication Discretionary and Technology that have the highest return such as Tesla, Nvidia, Microsoft, Amazon or Apple.\nThe change in market capitalization for each sector and for the company:\n\n\n\n\n\n\nLooking at the distribution of market cap by sector during the Covid-19 period, it is seen clearly that Communication Discretionary and Technology have consistently been the top first and second largest sectors respectively with combined shares at more than 60%. The two sectors remain the leading sectors throughout the period from January 2020 to March 2021.\nConsumer Discretionary is the biggest sector which accounts for around 46% of the entire market cap. The highest percentage of this sector was 49.7888% in late October 2020. This increasing trend started in January and hit the peak in October 2020 before having a decreasing trend from October to March the next year. The next sector, Technology, also had an increasing percentage over time as the area in the graph increased. This percentage of market cap for this sector started from 19.85% in January 2020, at 21.48% in the 1st Quarter in April 2020 and at 24.17% in November 2020. The max for this sector was 26.84%.\n\n\n\n\n\n\n\n\n\nThe distribution of market cap by company graph shows the top 20 companies that have the greatest amount of market cap proportion. Apple (AAPL), Amazon (AMZN), and Microsoft (MSFT) are the top three companies that are from the top two sectors: Communication Discretionary and Technology. From this we can see how the two sectors are the dominant by looking which companies have the greatest market cap.\nThe change in trading volume for each sector.\n\n\n\nThe bar graph shows the sum of the monthly transactions for the three biggest companies in each sector. We can see that the sector with the highest volume is Technology, and the sector with the lowest volume is the Real Estate sector. The high volumes in the Technology sector potentially reflect that investors preferred large technology companies over the pandemic.\n\n\n\n\n\n\nThese companies have large market caps, and given that the 5 largest companies in the Technology sector account for a large fraction of the S&P 500, these “blue chip” companies are expected to have larger volumes. One of the reasons is their liquidity. They are very liquid because many people are trading them; therefore, you can find buyers and sellers quickly. As a result there are no price distortions as a consequence of delays in transactions. On the other hand, this could be just a continuation of past trends, given that the technology sector has been the most popular in the last years.\nAnother potential explanation is that Technology stocks are always more risky given the larger returns; therefore, there are major price swings that cause people to increase their transactions. Also, larger volumes reflect the strength of price changes during the pandemic. In the case of Real Estate, low volumes reflect the lack of strength in price changes, reflected on subtle changes in the cumulative returns graphed. As we can see in the graph for Real Estate companies, the trend is flatter and the changes are of a smaller magnitude, proving that there weren’t significant changes in return. Therefore,there weren’t many changes in the demand or supply of Real Estate stock.\nWe can also observe that in periods with low prices, the volume tends to increase. In this case, it was in March given the declaration of sanitary emergency and the first COVID-19 cases in the country. This is because many people attempt to sell their shares quickly while other investors seek to buy shares at lower prices.\nConclusion:\nThe following post has shown some of the key changes within the period. It has shown how each sector has changed and how each company reacts to the pandemic. Some companies thrived, like Tesla, however, some took lots of time to recover, especially in the Energy and Finance sector. For the next steps, we could try and improve our information by scraping and getting all of the data in every company to illustrate and show the best picture for these sectors. This will be the improvement that we might need to do to see the wider picture of the market.\nOverall, our analysis has shown the changes in stock prices, volume, market capitalization before and after the pandemic. The pandemic and its consequences on the economy make it difficult to predict which companies will thrive and succeed in the future. However, we hope that some of our findings will shed light on some information about the general market trends observed when dealing with a global pandemic.\n\n\n\n",
    "preview": "posts/2021-03-19-buildingdistill/final_distill_foto.png",
    "last_modified": "2021-03-20T11:59:24-07:00",
    "input_file": {}
  }
]
