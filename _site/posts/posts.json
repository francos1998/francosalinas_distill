[
  {
    "path": "posts/2022-06-16-gibbssampling/",
    "title": "Gibbs Sampling",
    "description": "Literature review and explanation of concepts and computational methods related to Gibbs Sampling.",
    "author": [
      {
        "name": "Thu Dang, Franco Salinas",
        "url": {}
      }
    ],
    "date": "2022-06-16",
    "categories": [],
    "contents": "\nProject mathstats\n\n\n\n",
    "preview": "posts/2022-06-16-gibbssampling/MC.png",
    "last_modified": "2022-06-16T21:08:01-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-06-16-housevalues/",
    "title": "Real State Project",
    "description": "My capstone.",
    "author": [
      {
        "name": "Thu Dang, Franco Salinas",
        "url": {}
      }
    ],
    "date": "2022-06-16",
    "categories": [],
    "contents": "\nI will talk about the project\n\n\n\n",
    "preview": "posts/2022-06-16-housevalues/real-estate.png",
    "last_modified": "2022-06-16T19:27:24-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-05-07-accidentbehind/",
    "title": "Behind Scenes: Prediction of the severity of car accidents",
    "description": "In depth analysis of modelling decisions, coding and recommendations when approaching car accidents prediction.",
    "author": [
      {
        "name": "Juthi Dewan, Coco Li, Franco Salinas",
        "url": {}
      }
    ],
    "date": "2021-05-07",
    "categories": [],
    "contents": "\n\n\nlibrary(data.table)\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tidyverse)         # for reading in data, graphing, and cleaning\nlibrary(tidymodels)        # for modeling ... tidily\nlibrary(glmnet)            # for regularized regression, including LASSO\nlibrary(naniar)            # for examining missing values (NAs)\nlibrary(lubridate)         # for date manipulation\nlibrary(moderndive)        # for King County housing data\nlibrary(vip)               # for variable importance plots\nlibrary(rmarkdown)         # for paged tables\nlibrary(themis)            # for step functions for unbalanced data\nlibrary(stacks)            # for stacking models\nlibrary(DALEX)             # for model interpretation  \nlibrary(DALEXtra)          # for extension of DALEX\nlibrary(patchwork)         # for combining plots nicely\nlibrary(scales)\nlibrary(plotly)\nlibrary(gridExtra)\nlibrary(tidytext)\nlibrary(modelr)\nlibrary(caret)\nlibrary(ROSE)\nlibrary(glmnet)\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(randomForest)\nlibrary(shiny)\nlibrary(bslib)\noptions(warn = -1)\ntheme_set(theme_minimal()) # my favorite ggplot2 theme :)\n\n\n\n\n\ncars <- read_csv(\"small_accidents.csv\", col_types = cols(.default = col_character())) %>%\n  type_convert()\n\ncars %>%\n  group_by(City) %>%\n  summarize(Count=n()) %>%\n  arrange(desc(Count)) %>%\n  head(1000)\n\n\n# A tibble: 1,000 x 2\n   City        Count\n   <chr>       <int>\n 1 Houston      9612\n 2 Los Angeles  7771\n 3 Charlotte    7435\n 4 Dallas       6545\n 5 Austin       5832\n 6 Miami        5207\n 7 Raleigh      4420\n 8 Atlanta      3760\n 9 Orlando      3300\n10 Sacramento   3150\n# … with 990 more rows\n\nIntroduction\nThe Fatality Analysis Reporting System indicated that an estimate of 8870 people died in motor vehicle traffic crashes in the second quarter of 2020 (NHTSA, 2020). This analysis is intended to bring light to the main environmental conditions that are associated with the severity of a car accident. For the purpose of this study we defined severity as the accident’s impact on traffic.\nData\nThe data we used has 47 variables and 3 million observations for different car accidents. The data was collected from February 2016 to December 2020 for the 49 states of the US. The data base has been constructed partly by Moosavi, Sobhan, Mohammad Hossein Samavatian, Srinivasan Parthasarathy, and Rajiv Ramnath as “A Countrywide Traffic Accident Dataset” (2019). The other part of the data base was constructed by Moosavi, Sobhan, Mohammad Hossein Samavatian, Srinivasan Parthasarathy, Radu Teodorescu, and Rajiv Ramnath for their database “Accident Risk Prediction based on Heterogeneous Sparse Data: New Dataset and Insights.”\nModels\nUsing this data set, we predicted the Severity of an accident using stacked LASSO, Forest and classification three. Stacking combines predictions from many different models into a “super” predictor. In this case we would be averaging the predictions of the LASSO, Forest and classification three.\nPre-processing\n\n\ncars %>% summarise_all(~ mean(is.na(.))) %>%\n  pivot_longer(1:49, names_to = \"Variables to drop\", values_to = \"NA proportion\") %>%\n  filter(`NA proportion` >= 0.5)\n\n\n# A tibble: 3 x 2\n  `Variables to drop` `NA proportion`\n  <chr>                         <dbl>\n1 End_Lat                       0.636\n2 End_Lng                       0.636\n3 Number                        0.633\n\ndrop_na_cols <- c(\"End_Lat\", \"End_Lng\", \"Number\")\n\nnot_useful <- c(\"ID\", \"Source\", \"Timezone\", \"Airport_Code\", \"Weather_Timestamp\",\"Wind_Direction\", \"Description\", \"Bump\", \"Traffic_Calming\", \"Give_Way\", \"No_Exit\", \"Railway\", \"Roundabout\", \"Station\", \"Stop\", \"Amenity\", \"Street\", \"Zipcode\", \"Country\", \"Turning_Loop\", \"County\", \"TMC\")\n\n\ntraffic <-\n  cars %>%\n  select(-all_of(drop_na_cols), -all_of(not_useful))\n\np1 <- ggplot(cars, aes(as.factor(Station), ..prop.., group = Severity)) +\n  geom_bar(aes(fill = Severity), position = \"dodge\") +\n  scale_y_continuous(labels = percent) +\n  theme(axis.text.x = element_text(angle = 60, vjust = 0.6))\n \np2 <-  ggplot(cars, aes(Turning_Loop, ..prop.., group = Severity)) +\n  geom_bar(aes(fill = Severity), position = \"dodge\") +\n  scale_y_continuous(labels = percent) +\n  theme(axis.text.x = element_text(angle = 60, vjust = 0.6))\n\np3 <- ggplot(cars, aes(Country, ..prop.., group = Severity)) +\n  geom_bar(aes(fill = Severity), position = \"dodge\") +\n  scale_y_continuous(labels = percent) +\n  theme(axis.text.x = element_text(angle = 60, vjust = 0.6))\n\np4 <- ggplot(cars, aes(Amenity, ..prop.., group = Severity)) +\n  geom_bar(aes(fill = Severity), position = \"dodge\") +\n  scale_y_continuous(labels = percent) +\n  theme(axis.text.x = element_text(angle = 60, vjust = 0.6))\n\np5 <- ggplot(cars, aes(Stop, ..prop.., group = Severity)) +\n  geom_bar(aes(fill = Severity), position = \"dodge\") +\n  scale_y_continuous(labels = percent) +\n  theme(axis.text.x = element_text(angle = 60, vjust = 0.6))\n\np6 <- ggplot(cars, aes(Station, ..prop.., group = Severity)) +\n  geom_bar(aes(fill = Severity), position = \"dodge\") +\n  scale_y_continuous(labels = percent) +\n  theme(axis.text.x = element_text(angle = 60, vjust = 0.6))\n\np7 <- ggplot(cars, aes(Roundabout, ..prop.., group = Severity)) +\n  geom_bar(aes(fill = Severity), position = \"dodge\") +\n  scale_y_continuous(labels = percent) +\n  theme(axis.text.x = element_text(angle = 60, vjust = 0.6))\n\np8 <- ggplot(cars, aes(Railway, ..prop.., group = Severity)) +\n  geom_bar(aes(fill = Severity), position = \"dodge\") +\n  scale_y_continuous(labels = percent) +\n  theme(axis.text.x = element_text(angle = 60, vjust = 0.6))\n\np9 <- ggplot(cars, aes(No_Exit, ..prop.., group = Severity)) +\n  geom_bar(aes(fill = Severity), position = \"dodge\") +\n  scale_y_continuous(labels = percent) +\n  theme(axis.text.x = element_text(angle = 60, vjust = 0.6))\n\np10 <- ggplot(cars, aes(Give_Way, ..prop.., group = Severity)) +\n  geom_bar(aes(fill = Severity), position = \"dodge\") +\n  scale_y_continuous(labels = percent) +\n  theme(axis.text.x = element_text(angle = 60, vjust = 0.6))\n\np11 <- ggplot(cars, aes(Traffic_Calming, ..prop.., group = Severity)) +\n  geom_bar(aes(fill = Severity), position = \"dodge\") +\n  scale_y_continuous(labels = percent) +\n  theme(axis.text.x = element_text(angle = 60, vjust = 0.6))\n\n\np12 <- ggplot(cars, aes(Bump, ..prop.., group = Severity)) +\n  geom_bar(aes(fill = Severity), position = \"dodge\") +\n  scale_y_continuous(labels = percent) +\n  theme(axis.text.x = element_text(angle = 60, vjust = 0.6))\n\n\np1+ p2+ p3+ p4\n\n\n\np5+ p6+ p7+ p8\n\n\n\np9+ p10+ p11+ p12\n\n\n\n\nWe chose to not use multiple of the 47 variables that we considered weren’t relevant to the analysis we were conducting. As we can see in the table, end latitude, end longitude’s proportion of NA values are higher than 50%. Given the distribution of wind direction through the different severity levels, we decided that the variable is uninformative. The description, bump, traffic calming, give way, no exit, railway, roundabout, station, stop, amenity, street, zip code, country, turning loop, county and TMC code weren’t informative either given the distribution between the severity categories or near-zero variance.\n\n\ntraffic <-  traffic %>%\n  rename(\"Distance\" = `Distance(mi)`, \"Temperature\" = `Temperature(F)`, \"Humidity\" = `Humidity(%)`,\n         \"Pressure\" = `Pressure(in)`, \"Visibility\" = `Visibility(mi)`, \"Wind_Speed\" = `Wind_Speed(mph)`, \"Precipitation\" = `Precipitation(in)`, \"Wind_Chill\" = `Wind_Chill(F)`)\n\ntraffic$Severity <- as.character(traffic$Severity)\n\ntraffic <-\n  traffic %>%\n  add_n_miss() %>%\n  filter(n_miss_all == 0) %>%\n  select(-n_miss_all)\n\ntraffic <- traffic %>%\n  mutate(\"Status\" = factor(ifelse(Severity == \"3\" | Severity == \"4\", \"Severe\", \"Not Severe\"),\n                           levels = c(\"Not Severe\", \"Severe\")))\n\n\n\nIn this section of data cleaning and pre-processing above, we renamed some of the variables that had their units of measurement for ease of use later in our modeling and for our shinyApp. We took out entries of data that had NAs. If this was a smaller dataset, this may have negatively impacted our analysis, but we don’t believe this effected our analysis for this project because even after taking out some data, we had a lot left to work with. Another very important part of this section is that, we took the Severity variable that we are looking at and split it into two to make it into a Categorical variable called Status. Severities 1 and 2 were grouped in to be Not Severe and 3 and 4 were grouped as Severe in the Status variable.\n\n\n\ntraffic_time <- traffic %>%\n  mutate(Duration = (End_Time - Start_Time)) %>%\n  # accident duration should be positive\n  filter(!(Duration < 0)) %>%\n  separate(Start_Time, into = c(\"Date\", \"Time\"), sep = \" \") %>%\n  mutate(\"Year\" = str_sub(Date, 1, 4), \"Month\" = str_sub(Date, 6, 7), \"Day\" = str_sub(Date, 9, 10),\n         \"Wday\" = as.character(wday(Date))) %>%\n  mutate(\"Hour\" = str_sub(Time,1,2)) %>%\n  select(-c(\"Date\", \"Time\", \"End_Time\")) %>%\n  select(Severity, Year, Month, Day, Hour, Wday, Duration, everything())\n\n\n\nIn this section, we used the End_Time and Start_Time variables to come up with several other variables such as Duration, Date, Time, Year, Month, Day, Wday and Hour.\n\n\n#Drop levels that have less than 20 observations\nweather_to_drop <-\n  traffic_time %>%\n    count(Weather_Condition) %>%\n    filter(n < 20) %>%\n    select(Weather_Condition)\n\nweather_to_drop <-\n  weather_to_drop$Weather_Condition %>%\n    unlist()\n\ntraffic_weather <- traffic_time %>%\n  filter(!(Weather_Condition %in% weather_to_drop)) %>%\n  mutate(Weather_Condition = factor(Weather_Condition))\n\ntraffic2 <- traffic_weather\n\ncount_city <- traffic2 %>%\n  group_by(City) %>%\n  summarize(Count=n()) %>%\n  arrange(desc(Count)) %>%\n  head(950)  \n\ntraffic3 <-\n  traffic2 %>%\n    left_join(count_city, by=\"City\")\n\ntraffic_final <-\n  traffic3 %>%\n  add_n_miss() %>%\n  filter(n_miss_all == 0) %>%\n  select(-n_miss_all) %>%\n  select(-Count)\n\n#write.csv(traffic_final, \"traffic_final.csv\", row.names = FALSE)\n\n\n\n\n\nread_csv(\"traffic_final.csv\")\n\n\n# A tibble: 98,088 x 29\n   Severity  Year Month Day   Hour   Wday Duration Start_Lat Start_Lng\n      <dbl> <dbl> <chr> <chr> <chr> <dbl>    <dbl>     <dbl>     <dbl>\n 1        2  2016 12    07    23        4     44.4      38.6     -121.\n 2        2  2016 12    08    09        5     29.6      38.4     -123.\n 3        2  2016 12    23    09        6     29.7      38.3     -123.\n 4        2  2017 01    02    19        2     29.7      39.3     -121.\n 5        2  2017 01    22    15        1     44.6      38.1     -122.\n 6        2  2017 01    23    20        2     29.6      37.2     -122.\n 7        2  2017 01    23    22        2     29.8      37.4     -122.\n 8        2  2017 01    25    06        4     44.6      38.0     -122.\n 9        2  2016 11    30    08        4     70.0      38.7     -122.\n10        2  2016 06    21    16        3     60        34.4     -119.\n# … with 98,078 more rows, and 20 more variables: Distance <dbl>,\n#   Side <chr>, City <chr>, State <chr>, Temperature <dbl>,\n#   Wind_Chill <dbl>, Humidity <dbl>, Pressure <dbl>,\n#   Visibility <dbl>, Wind_Speed <dbl>, Precipitation <dbl>,\n#   Weather_Condition <chr>, Crossing <lgl>, Junction <lgl>,\n#   Traffic_Signal <lgl>, Sunrise_Sunset <chr>, Civil_Twilight <chr>,\n#   Nautical_Twilight <chr>, Astronomical_Twilight <chr>,\n#   Status <chr>\n\nUpon close examination of the Weather_Conditions variable, we realized that it had a lot of different observations and we wanted to narrow it down. So, we filtered and dropped levels that have less than 20 observations. We had to narrow down the City variable as well. We have over 4000 distinct cities under the top 12 states. Shiny only allows a thousand different observations and so in order for all the top cities to fit in our shiny app, we had to narrow down the list of cities to the top 950.\n\n\n#modeling pre-process for traffic_final\n\ntraffic_mod <- traffic_final %>%\n  mutate(Status = as.factor(Status)) %>%\n  mutate(across(where(is.character), as.factor)) %>%\n  select(-c(State, Severity, Year, Day)) %>%\n  # select(-arrival_date_year,\n  #        -reservation_status,\n  #        -reservation_status_date) %>%\n  add_n_miss() %>%\n  filter(n_miss_all == 0) %>%\n  select(-n_miss_all)\n\n\ntraffic_mod$Crossing <- as.factor(traffic_mod$Crossing)\ntraffic_mod$Month <- as.numeric(traffic_mod$Month)\ntraffic_mod$Wday <- as.numeric(traffic_mod$Wday)\ntraffic_mod$Hour <- as.numeric(traffic_mod$Hour)\ntraffic_mod$Duration <- as.numeric(traffic_mod$Duration)\ntraffic_mod$Junction <- as.factor(traffic_mod$Junction)\ntraffic_mod$Traffic_Signal <- as.factor(traffic_mod$Traffic_Signal)\n \n\nset.seed(494) #for reproducibility\n\n# Randomly assigns 75% of the data to training.\ntraffic_split <- initial_split(traffic_mod,\n                             prop = .50)\ntraffic_split\n\n\n<Analysis/Assess/Total>\n<49044/49044/98088>\n\ntraffic_training <- training(traffic_split)\ntraffic_testing <- testing(traffic_split)\n\n\n\nHere, we get the data ready for the modeling part by taking the non-predictive variables out of our data set, and converting the predictors’ data type into the correct type. After that, we split our data into testing and training data according to a 50 percentage split.\n\n\n#lasso\nset.seed(494)\n\nlasso_recipe <-\n  recipe(Status ~ .,\n         data = traffic_training) %>%\n  # step_mutate(County,\n  #              County = fct_lump_n(County, n = 5)) %>%\n   step_mutate(City,\n               City = fct_lump_n(City, n = 5)) %>%\n  step_normalize(all_predictors(),\n                 -all_nominal(),\n                 -all_outcomes()) %>%\n  step_dummy(all_nominal(),\n             -all_outcomes())\n\nlasso_recipe %>%\n  prep() %>%\n  juice()\n\n\n# A tibble: 49,044 x 64\n    Month    Hour    Wday Duration Start_Lat Start_Lng Distance\n    <dbl>   <dbl>   <dbl>    <dbl>     <dbl>     <dbl>    <dbl>\n 1  1.21   1.86   -0.0289  -0.0345     0.715     -1.31   -0.252\n 2  1.21  -0.527   0.536   -0.0399     0.682     -1.38   -0.252\n 3  1.21  -0.527   1.10    -0.0399     0.665     -1.38   -0.260\n 4 -1.93   0.495  -1.72    -0.0344     0.621     -1.36   -0.252\n 5 -1.93   1.69   -1.16    -0.0398     0.464     -1.34   -0.252\n 6 -0.504 -0.527  -1.16    -0.0366    -0.161     -1.15   -0.260\n 7 -0.219 -1.04    1.67    -0.0342    -0.155     -1.15   -0.260\n 8 -0.219 -1.89   -0.594   -0.0287    -0.141     -1.16   -0.260\n 9 -0.219 -0.0158 -0.0289  -0.0368    -0.178     -1.17   -0.260\n10 -0.219  1.35   -0.0289  -0.0398    -0.179     -1.16   -0.260\n# … with 49,034 more rows, and 57 more variables: Temperature <dbl>,\n#   Wind_Chill <dbl>, Humidity <dbl>, Pressure <dbl>,\n#   Visibility <dbl>, Wind_Speed <dbl>, Precipitation <dbl>,\n#   Status <fct>, Side_R <dbl>, City_Dallas <dbl>,\n#   City_Houston <dbl>, City_Los.Angeles <dbl>, City_Miami <dbl>,\n#   City_Other <dbl>, Weather_Condition_Cloudy <dbl>,\n#   Weather_Condition_Cloudy...Windy <dbl>,\n#   Weather_Condition_Drizzle <dbl>, Weather_Condition_Fair <dbl>,\n#   Weather_Condition_Fair...Windy <dbl>,\n#   Weather_Condition_Fog <dbl>, Weather_Condition_Haze <dbl>,\n#   Weather_Condition_Heavy.Rain <dbl>,\n#   Weather_Condition_Heavy.Rain...Windy <dbl>,\n#   Weather_Condition_Heavy.Snow <dbl>,\n#   Weather_Condition_Heavy.T.Storm <dbl>,\n#   Weather_Condition_Light.Drizzle <dbl>,\n#   Weather_Condition_Light.Freezing.Rain <dbl>,\n#   Weather_Condition_Light.Rain <dbl>,\n#   Weather_Condition_Light.Rain...Windy <dbl>,\n#   Weather_Condition_Light.Rain.with.Thunder <dbl>,\n#   Weather_Condition_Light.Snow <dbl>,\n#   Weather_Condition_Light.Snow...Windy <dbl>,\n#   Weather_Condition_Mist <dbl>,\n#   Weather_Condition_Mostly.Cloudy <dbl>,\n#   Weather_Condition_Mostly.Cloudy...Windy <dbl>,\n#   Weather_Condition_N.A.Precipitation <dbl>,\n#   Weather_Condition_Overcast <dbl>,\n#   Weather_Condition_Partly.Cloudy <dbl>,\n#   Weather_Condition_Partly.Cloudy...Windy <dbl>,\n#   Weather_Condition_Patches.of.Fog <dbl>,\n#   Weather_Condition_Rain <dbl>,\n#   Weather_Condition_Rain...Windy <dbl>,\n#   Weather_Condition_Scattered.Clouds <dbl>,\n#   Weather_Condition_Shallow.Fog <dbl>,\n#   Weather_Condition_Smoke <dbl>, Weather_Condition_Snow <dbl>,\n#   Weather_Condition_T.Storm <dbl>, Weather_Condition_Thunder <dbl>,\n#   Weather_Condition_Thunder.in.the.Vicinity <dbl>,\n#   Weather_Condition_Wintry.Mix <dbl>, Crossing_TRUE. <dbl>,\n#   Junction_TRUE. <dbl>, Traffic_Signal_TRUE. <dbl>,\n#   Sunrise_Sunset_Night <dbl>, Civil_Twilight_Night <dbl>,\n#   Nautical_Twilight_Night <dbl>, Astronomical_Twilight_Night <dbl>\n\nlasso_mod <-\n  logistic_reg(mixture = 1) %>%\n  set_engine(\"glmnet\") %>%\n  set_args(penalty = tune()) %>%\n  set_mode(\"classification\")\n\nlasso_wf <-\n  workflow() %>%\n  add_recipe(lasso_recipe) %>%\n  add_model(lasso_mod)\n\nset.seed(494) #for reproducible 5-fold\ntraffic_cv <- vfold_cv(traffic_training,\n                       v = 5)\n\npenalty_grid <- grid_regular(penalty(),\n                             levels = 10)\n\n# add ctrl_grid - assures predictions and workflows are saved\nctrl_grid <- control_stack_resamples()\n\nmetric <- metric_set(accuracy)\n\n# tune the model\nlasso_tune <-\n  lasso_wf %>%\n  tune_grid(\n    resamples = traffic_cv,\n    grid = penalty_grid,\n    control = ctrl_grid\n    )\n\nlasso_tune %>%\n  collect_metrics()\n\n\n# A tibble: 20 x 7\n       penalty .metric  .estimator  mean     n std_err .config        \n         <dbl> <chr>    <chr>      <dbl> <int>   <dbl> <chr>          \n 1    1.00e-10 accuracy binary     0.808     5 1.52e-3 Preprocessor1_…\n 2    1.00e-10 roc_auc  binary     0.738     5 7.72e-4 Preprocessor1_…\n 3    1.29e- 9 accuracy binary     0.808     5 1.52e-3 Preprocessor1_…\n 4    1.29e- 9 roc_auc  binary     0.738     5 7.72e-4 Preprocessor1_…\n 5    1.67e- 8 accuracy binary     0.808     5 1.52e-3 Preprocessor1_…\n 6    1.67e- 8 roc_auc  binary     0.738     5 7.72e-4 Preprocessor1_…\n 7    2.15e- 7 accuracy binary     0.808     5 1.52e-3 Preprocessor1_…\n 8    2.15e- 7 roc_auc  binary     0.738     5 7.72e-4 Preprocessor1_…\n 9    2.78e- 6 accuracy binary     0.808     5 1.52e-3 Preprocessor1_…\n10    2.78e- 6 roc_auc  binary     0.738     5 7.72e-4 Preprocessor1_…\n11    3.59e- 5 accuracy binary     0.808     5 1.53e-3 Preprocessor1_…\n12    3.59e- 5 roc_auc  binary     0.738     5 7.75e-4 Preprocessor1_…\n13    4.64e- 4 accuracy binary     0.808     5 1.69e-3 Preprocessor1_…\n14    4.64e- 4 roc_auc  binary     0.737     5 9.58e-4 Preprocessor1_…\n15    5.99e- 3 accuracy binary     0.807     5 2.46e-3 Preprocessor1_…\n16    5.99e- 3 roc_auc  binary     0.730     5 2.33e-3 Preprocessor1_…\n17    7.74e- 2 accuracy binary     0.806     5 2.34e-3 Preprocessor1_…\n18    7.74e- 2 roc_auc  binary     0.5       5 0.      Preprocessor1_…\n19    1.00e+ 0 accuracy binary     0.806     5 2.34e-3 Preprocessor1_…\n20    1.00e+ 0 roc_auc  binary     0.5       5 0.      Preprocessor1_…\n\nbest_param <- lasso_tune %>%\n  select_best(metric = \"accuracy\")\nbest_param\n\n\n# A tibble: 1 x 2\n   penalty .config              \n     <dbl> <chr>                \n1 0.000464 Preprocessor1_Model07\n\nfinal_lasso <- lasso_wf %>%\n  finalize_workflow(best_param) %>%\n  fit(data = traffic_training)\n\nfinal_lasso %>%\n  pull_workflow_fit() %>%\n  tidy()\n\n\n# A tibble: 64 x 3\n   term         estimate  penalty\n   <chr>           <dbl>    <dbl>\n 1 (Intercept) -3.56     0.000464\n 2 Month       -0.353    0.000464\n 3 Hour         0.0587   0.000464\n 4 Wday        -0.000337 0.000464\n 5 Duration     0        0.000464\n 6 Start_Lat    0.135    0.000464\n 7 Start_Lng    0.462    0.000464\n 8 Distance     0.0946   0.000464\n 9 Temperature  0.224    0.000464\n10 Wind_Chill  -0.0209   0.000464\n# … with 54 more rows\n\nThe first model we build is a classification LASSO model, which selects the variables based on the magnitude of their coefficients. We tuned the LASSO model using a level 10 panelty grid, and selected the tuning parameter with the best prediction accuracy as the parameter for the final model. The accuracy for the best LASSO model is 80.956%, which means that the LASSO model predicts the right severity level 80.956% of the times.\n\n\n#classification rf\nset.seed(494)\n\nrf_recipe <-\n  recipe(Status ~ .,\n         data = traffic_training) %>%\n  step_mutate_at(all_numeric(),\n                 fn = ~as.numeric(.))\n\n\nrf_recipe %>%\n  prep() %>%\n  juice()\n\n\n# A tibble: 49,044 x 25\n   Month  Hour  Wday Duration Start_Lat Start_Lng Distance Side  City \n   <dbl> <dbl> <dbl>    <dbl>     <dbl>     <dbl>    <dbl> <fct> <fct>\n 1    12    24     4     44.4      38.6     -121.     0.01 R     Sacr…\n 2    12    10     5     29.6      38.4     -123.     0.01 R     Sant…\n 3    12    10     6     29.7      38.3     -123.     0    L     Seba…\n 4     1    16     1     44.6      38.1     -122.     0.01 R     Peta…\n 5     1    23     2     29.8      37.4     -122.     0.01 L     Sant…\n 6     6    10     2     38.6      34.4     -119.     0    R     Newh…\n 7     7     7     7     45        34.4     -119.     0    R     Vale…\n 8     7     2     3     60        34.5     -119.     0    L     Cast…\n 9     7    13     4     38.1      34.3     -119.     0    R     Simi…\n10     7    21     4     30        34.3     -119.     0    R     Simi…\n# … with 49,034 more rows, and 16 more variables: Temperature <dbl>,\n#   Wind_Chill <dbl>, Humidity <dbl>, Pressure <dbl>,\n#   Visibility <dbl>, Wind_Speed <dbl>, Precipitation <dbl>,\n#   Weather_Condition <fct>, Crossing <fct>, Junction <fct>,\n#   Traffic_Signal <fct>, Sunrise_Sunset <fct>, Civil_Twilight <fct>,\n#   Nautical_Twilight <fct>, Astronomical_Twilight <fct>,\n#   Status <fct>\n\nrf_model <-\n  rand_forest(mtry = tune(),\n              min_n = tune(),\n              trees = 10) %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"ranger\")\n\n\nrf_workflow <-\n  workflow() %>%\n  add_recipe(rf_recipe) %>%\n  add_model(rf_model)\n\n\nrf_penalty_grid <-\n  grid_regular(finalize(mtry(),\n                        traffic_training %>%\n                          select(-Status)),\n               min_n(),\n               levels = 3)\n\n\n# traffic_cv <- vfold_cv(traffic_training,\n#                        v = 5)\n\nrf_tune <-\n  rf_workflow %>%\n  tune_grid(\n    resamples = traffic_cv,\n    grid = rf_penalty_grid,\n    control = control_stack_grid()\n  )\n\nrf_tune %>%\n  collect_metrics()\n\n\n# A tibble: 18 x 8\n    mtry min_n .metric  .estimator  mean     n std_err .config        \n   <int> <int> <chr>    <chr>      <dbl> <int>   <dbl> <chr>          \n 1     1     2 accuracy binary     0.806     5 0.00225 Preprocessor1_…\n 2     1     2 roc_auc  binary     0.786     5 0.00307 Preprocessor1_…\n 3    12     2 accuracy binary     0.842     5 0.00184 Preprocessor1_…\n 4    12     2 roc_auc  binary     0.851     5 0.00291 Preprocessor1_…\n 5    24     2 accuracy binary     0.841     5 0.00267 Preprocessor1_…\n 6    24     2 roc_auc  binary     0.848     5 0.00259 Preprocessor1_…\n 7     1    21 accuracy binary     0.807     5 0.00210 Preprocessor1_…\n 8     1    21 roc_auc  binary     0.796     5 0.00191 Preprocessor1_…\n 9    12    21 accuracy binary     0.846     5 0.00172 Preprocessor1_…\n10    12    21 roc_auc  binary     0.868     5 0.00193 Preprocessor1_…\n11    24    21 accuracy binary     0.844     5 0.00125 Preprocessor1_…\n12    24    21 roc_auc  binary     0.862     5 0.00229 Preprocessor1_…\n13     1    40 accuracy binary     0.807     5 0.00210 Preprocessor1_…\n14     1    40 roc_auc  binary     0.788     5 0.00353 Preprocessor1_…\n15    12    40 accuracy binary     0.848     5 0.00105 Preprocessor1_…\n16    12    40 roc_auc  binary     0.872     5 0.00170 Preprocessor1_…\n17    24    40 accuracy binary     0.844     5 0.00189 Preprocessor1_…\n18    24    40 roc_auc  binary     0.866     5 0.00212 Preprocessor1_…\n\nAfter conducting the LASSO model, we also build the random forest model, which builds 10 trees and gives out the mode of the predictions of these 10 trees. We thought that this model might be more accurate than the LASSO model although it is also more computationally inefficient. We used a panelty grid of level 3 to tune our random forest model, and the tuning parameter with the largest accuracy is with mtry = 12 and min_n = 40. The largest accuracy is 84.693%, which is higher than the LASSO model.\n\n\n#decision trees\nset.seed(494)\n\ntree_model <-\n  decision_tree() %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"rpart\")\n\ntree_workflow <-\n  workflow() %>%\n  add_recipe(rf_recipe) %>%  \n  add_model(tree_model)\n\ntree_fit <-\n  tree_workflow %>%\n  fit_resamples(traffic_cv,\n                # metrics = metric,\n                control = control_stack_resamples()\n  )\n\ncollect_metrics(tree_fit)\n\n\n# A tibble: 2 x 6\n  .metric  .estimator  mean     n std_err .config             \n  <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n1 accuracy binary     0.824     5 0.00143 Preprocessor1_Model1\n2 roc_auc  binary     0.686     5 0.00309 Preprocessor1_Model1\n\nFinally, in order to create a stacked model, we build a third model which is just a simple classification decision tree. The accuracy for the decision tree model is 82.559%, which is also higher than the LASSO.\n\n\n# model stacking\nlasso_tune %>%\n  collect_metrics()\n\n\n# A tibble: 20 x 7\n       penalty .metric  .estimator  mean     n std_err .config        \n         <dbl> <chr>    <chr>      <dbl> <int>   <dbl> <chr>          \n 1    1.00e-10 accuracy binary     0.808     5 1.52e-3 Preprocessor1_…\n 2    1.00e-10 roc_auc  binary     0.738     5 7.72e-4 Preprocessor1_…\n 3    1.29e- 9 accuracy binary     0.808     5 1.52e-3 Preprocessor1_…\n 4    1.29e- 9 roc_auc  binary     0.738     5 7.72e-4 Preprocessor1_…\n 5    1.67e- 8 accuracy binary     0.808     5 1.52e-3 Preprocessor1_…\n 6    1.67e- 8 roc_auc  binary     0.738     5 7.72e-4 Preprocessor1_…\n 7    2.15e- 7 accuracy binary     0.808     5 1.52e-3 Preprocessor1_…\n 8    2.15e- 7 roc_auc  binary     0.738     5 7.72e-4 Preprocessor1_…\n 9    2.78e- 6 accuracy binary     0.808     5 1.52e-3 Preprocessor1_…\n10    2.78e- 6 roc_auc  binary     0.738     5 7.72e-4 Preprocessor1_…\n11    3.59e- 5 accuracy binary     0.808     5 1.53e-3 Preprocessor1_…\n12    3.59e- 5 roc_auc  binary     0.738     5 7.75e-4 Preprocessor1_…\n13    4.64e- 4 accuracy binary     0.808     5 1.69e-3 Preprocessor1_…\n14    4.64e- 4 roc_auc  binary     0.737     5 9.58e-4 Preprocessor1_…\n15    5.99e- 3 accuracy binary     0.807     5 2.46e-3 Preprocessor1_…\n16    5.99e- 3 roc_auc  binary     0.730     5 2.33e-3 Preprocessor1_…\n17    7.74e- 2 accuracy binary     0.806     5 2.34e-3 Preprocessor1_…\n18    7.74e- 2 roc_auc  binary     0.5       5 0.      Preprocessor1_…\n19    1.00e+ 0 accuracy binary     0.806     5 2.34e-3 Preprocessor1_…\n20    1.00e+ 0 roc_auc  binary     0.5       5 0.      Preprocessor1_…\n\nrf_tune %>%\n  collect_metrics()\n\n\n# A tibble: 18 x 8\n    mtry min_n .metric  .estimator  mean     n std_err .config        \n   <int> <int> <chr>    <chr>      <dbl> <int>   <dbl> <chr>          \n 1     1     2 accuracy binary     0.806     5 0.00225 Preprocessor1_…\n 2     1     2 roc_auc  binary     0.786     5 0.00307 Preprocessor1_…\n 3    12     2 accuracy binary     0.842     5 0.00184 Preprocessor1_…\n 4    12     2 roc_auc  binary     0.851     5 0.00291 Preprocessor1_…\n 5    24     2 accuracy binary     0.841     5 0.00267 Preprocessor1_…\n 6    24     2 roc_auc  binary     0.848     5 0.00259 Preprocessor1_…\n 7     1    21 accuracy binary     0.807     5 0.00210 Preprocessor1_…\n 8     1    21 roc_auc  binary     0.796     5 0.00191 Preprocessor1_…\n 9    12    21 accuracy binary     0.846     5 0.00172 Preprocessor1_…\n10    12    21 roc_auc  binary     0.868     5 0.00193 Preprocessor1_…\n11    24    21 accuracy binary     0.844     5 0.00125 Preprocessor1_…\n12    24    21 roc_auc  binary     0.862     5 0.00229 Preprocessor1_…\n13     1    40 accuracy binary     0.807     5 0.00210 Preprocessor1_…\n14     1    40 roc_auc  binary     0.788     5 0.00353 Preprocessor1_…\n15    12    40 accuracy binary     0.848     5 0.00105 Preprocessor1_…\n16    12    40 roc_auc  binary     0.872     5 0.00170 Preprocessor1_…\n17    24    40 accuracy binary     0.844     5 0.00189 Preprocessor1_…\n18    24    40 roc_auc  binary     0.866     5 0.00212 Preprocessor1_…\n\ntree_fit %>%\n  collect_metrics()\n\n\n# A tibble: 2 x 6\n  .metric  .estimator  mean     n std_err .config             \n  <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n1 accuracy binary     0.824     5 0.00143 Preprocessor1_Model1\n2 roc_auc  binary     0.686     5 0.00309 Preprocessor1_Model1\n\n\n\ntraffic_stack <-\n  stacks() %>%\n  add_candidates(lasso_tune) %>%\n  add_candidates(rf_tune) %>%\n  add_candidates(tree_fit)\n\n\n\n\n\ntraffic_blend <-\n  traffic_stack %>%\n  blend_predictions()\ntraffic_blend\n\n\n# A tibble: 9 x 3\n  member                    type          weight\n  <chr>                     <chr>          <dbl>\n1 .pred_Severe_rf_tune_1_8  rand_forest    1.80 \n2 .pred_Severe_rf_tune_1_5  rand_forest    1.47 \n3 .pred_Severe_rf_tune_1_2  rand_forest    0.871\n4 .pred_Severe_tree_fit_1_1 decision_tree  0.783\n5 .pred_Severe_rf_tune_1_9  rand_forest    0.642\n6 .pred_Severe_rf_tune_1_4  rand_forest    0.639\n7 .pred_Severe_rf_tune_1_3  rand_forest    0.604\n8 .pred_Severe_rf_tune_1_6  rand_forest    0.578\n9 .pred_Severe_rf_tune_1_7  rand_forest    0.225\n\n\n\ntraffic_final_stack <- traffic_blend %>%\n  fit_members()\n\n#saveRDS(traffic_final_stack, \"traffic_final_stacked.rds\")\n\n\n\nShiny App\nUser Interface (UI)\nWe also developed a shiny app that included all of the relevant variables of our analysis. The purpose of the app was to allow the user to plug different values for environmental conditions or locations to see how the severity predicted changes. When creating the UI, we decided to use sliders for each of the numerical variables. The sliders referenced minimum and maximum variables that I had defined previously in line 553. For variables with multiple levels like City and Weather condition, we decided to create a list of levels that we referenced later in the selectInput function. This saved us the time of having to type the name of each of the variables’ levels. We also formatted the app using the bslib package, we included this in the theme argument in line 566. We used the package to define a font, the primary, secondary and bootswatch colors. Given the large number of variables included, we also added a scrollable side panel in lineS 577 to 580.\nServer\nWe then defined the input variables for the server. We run into errors regarding incoherence between the variables used in the UI and the server. We decided to assign arbitrary values to the input variables and one by one we tested what variables were not being recognized. This also allowed us to detect certain variables that we no longer considered relevant, like county. We also had to pre-processing to change the names of variables like “wind_chill(F)” which were for some reason not recognized by the tibble function. Then, in lines 836 - 840 we defined the output by using the stacked model in our data and asked the app to show our prediction.\nEmbedded app\nWe decided to use an embedded app in a different r-markdown file considering that the size of our rsd file (our model) was too big to be deployed. We added a “runtime: shiny” argument in the YAML. Within an r code chunk, we still had to call the lists of variable levels and the minimum and maximum variable values before copying the code for our app. We used the shinyApp function in line 564 before plugging the remainder of our code. The app works but it could only be seen by people that have R installed in the computers and that have the project file. This is an issue that we further need to work on. We believe there are ways to reduce the size of our rsd file.\nShiny App Code\n\n\n# traffic_mod <- readRDS(\"traffic_final_stacked.rds\")\n# traffic_mod <- readRDS(\"traffic_final_stacked.rds\")\n# \n# Cities <-\n#   traffic_mod$train  %>%\n#   select(City) %>%\n#   distinct(City) %>%\n#   arrange(City) %>%\n#   pull(City)\n# \n# Weather <-\n#   traffic_mod$train  %>%\n#   select(Weather_Condition) %>%\n#   distinct(Weather_Condition) %>%\n#   arrange(Weather_Condition) %>%\n#   pull(Weather_Condition)\n# \n# \n# # Find min's, max's, and median's for quantitative vars:\n# \n# stats_num <-\n#   traffic_mod$train  %>%\n#   select(where(is.numeric)) %>%\n#   pivot_longer(cols = everything(),\n#                names_to = \"variable\",\n#                values_to = \"value\") %>%\n#   group_by(variable) %>%\n#   summarize(min_val = min(value),\n#             max_val = max(value),\n#             med_val = median(value))\n# \n# shinyApp(\n#   ui <- fluidPage(\n#   theme = bs_theme(primary = \"#123B60\",\n#                    secondary = \"#D44420\",\n#                    base_font = list(font_google(\"Raleway\"), \"-apple-system\",\n#                                     \"BlinkMacSystemFont\", \"Segoe UI\", \"Helvetica Neue\", \"Arial\",\n#                                     \"sans-serif\", \"Apple Color Emoji\", \"Segoe UI Emoji\",\n#                                     \"Segoe UI Symbol\"),\n#                    bootswatch = \"sandstone\"),\n#   # Application title\n#   sidebarLayout(\n#     sidebarPanel(\n#       # added this for scrollable side panel:\n#       tags$head(tags$style(\n#         type = 'text/css',\n#         'form.well { max-height: 600px; overflow-y: auto; }'\n#       )),\n#       sliderInput(inputId = \"Hour\",\n#               label = \"Hour of Accident\",\n#               min = stats_num %>%\n#                 filter(variable ==\"Hour\") %>%\n#                 pull(min_val),\n#               max = stats_num %>%\n#                 filter(variable ==\"Hour\") %>%\n#                 pull(max_val),\n#               value = stats_num %>%\n#                 filter(variable ==\"Hour\") %>%\n#                 pull(med_val),\n#               step = 1,\n#               round = TRUE),\n#       sliderInput(inputId = \"Month\",\n#               label = \"Month of Accident\",\n#               min = stats_num %>%\n#                 filter(variable ==\"Month\") %>%\n#                 pull(min_val),\n#               max = stats_num %>%\n#                 filter(variable ==\"Month\") %>%\n#                 pull(max_val),\n#               value = stats_num %>%\n#                 filter(variable ==\"Month\") %>%\n#                 pull(med_val),\n#               step = 1,\n#               round = TRUE),\n#       sliderInput(inputId = \"Wday\",\n#               label = \"Week day of Accident\",\n#               min = stats_num %>%\n#                 filter(variable ==\"Wday\") %>%\n#                 pull(min_val),\n#               max = stats_num %>%\n#                 filter(variable ==\"Wday\") %>%\n#                 pull(max_val),\n#               value = stats_num %>%\n#                 filter(variable ==\"Wday\") %>%\n#                 pull(med_val),\n#               step = 1,\n#               round = TRUE),\n#       sliderInput(inputId = \"Duration\",\n#               label = \"Duration of Accident in seconds\",\n#               min = stats_num %>%\n#                 filter(variable ==\"Duration\") %>%\n#                 pull(min_val),\n#               max = stats_num %>%\n#                 filter(variable ==\"Duration\") %>%\n#                 pull(max_val),\n#               value = stats_num %>%\n#                 filter(variable ==\"Duration\") %>%\n#                 pull(med_val),\n#               step = 1,\n#               round = TRUE),\n#       sliderInput(inputId = \"Start_Lat\",\n#               label = \"Starting latitude of the Accident\",\n#               min = stats_num %>%\n#                 filter(variable ==\"Start_Lat\") %>%\n#                 pull(min_val),\n#               max = stats_num %>%\n#                 filter(variable ==\"Start_Lat\") %>%\n#                 pull(max_val),\n#               value = stats_num %>%\n#                 filter(variable ==\"Start_Lat\") %>%\n#                 pull(med_val),\n#               step = 1,\n#               round = TRUE),\n#       sliderInput(inputId = \"Start_Lng\",\n#               label = \"Starting longitude of the Accident\",\n#               min = stats_num %>%\n#                 filter(variable ==\"Start_Lng\") %>%\n#                 pull(min_val),\n#               max = stats_num %>%\n#                 filter(variable ==\"Start_Lng\") %>%\n#                 pull(max_val),\n#               value = stats_num %>%\n#                 filter(variable ==\"Start_Lng\") %>%\n#                 pull(med_val),\n#               step = 1,\n#               round = TRUE),\n#       sliderInput(inputId = \"Distance\",\n#               label = \"Distance of the Accident\",\n#               min = stats_num %>%\n#                 filter(variable ==\"Distance\") %>%\n#                 pull(min_val),\n#               max = stats_num %>%\n#                 filter(variable ==\"Distance\") %>%\n#                 pull(max_val),\n#               value = stats_num %>%\n#                 filter(variable ==\"Distance\") %>%\n#                 pull(med_val),\n#               step = 1,\n#               round = TRUE),\n#       selectInput(inputId = \"Side\",\n#               label = \"Side of the street where the accident happened\",\n#               choices = list(Right = \"R\",\n#                              Left = \"L\")),\n#       selectInput(inputId = \"City\",\n#                   label = \"City where the accident happened\",\n#                   choices = Cities),\n#       sliderInput(inputId = \"Temperature\",\n#               label = \"Temperature when accident happened\",\n#               min = stats_num %>%\n#                 filter(variable ==\"Temperature\") %>%\n#                 pull(min_val),\n#               max = stats_num %>%\n#                 filter(variable ==\"Temperature\") %>%\n#                 pull(max_val),\n#               value = stats_num %>%\n#                 filter(variable ==\"Temperature\") %>%\n#                 pull(med_val),\n#               step = 1,\n#               round = TRUE),\n#       sliderInput(inputId = \"Wind_Chill\",\n#               label = \"Wind chill in degrees Farenheit when accident happened\",\n#               min = stats_num %>%\n#                 filter(variable ==\"Wind_Chill\") %>%\n#                 pull(min_val),\n#               max = stats_num %>%\n#                 filter(variable ==\"Wind_Chill\") %>%\n#                 pull(max_val),\n#               value = stats_num %>%\n#                 filter(variable ==\"Wind_Chill\") %>%\n#                 pull(med_val),\n#               step = 1,\n#               round = TRUE),\n#       sliderInput(inputId = \"Humidity\",\n#               label = \"Humidity when accident happened\",\n#               min = stats_num %>%\n#                 filter(variable ==\"Humidity\") %>%\n#                 pull(min_val),\n#               max = stats_num %>%\n#                 filter(variable ==\"Humidity\") %>%\n#                 pull(max_val),\n#               value = stats_num %>%\n#                 filter(variable ==\"Humidity\") %>%\n#                 pull(med_val),\n#               step = 1,\n#               round = TRUE),\n#       sliderInput(inputId = \"Pressure\",\n#               label = \"Pressure when accident happened\",\n#               min = stats_num %>%\n#                 filter(variable ==\"Pressure\") %>%\n#                 pull(min_val),\n#               max = stats_num %>%\n#                 filter(variable ==\"Pressure\") %>%\n#                 pull(max_val),\n#               value = stats_num %>%\n#                 filter(variable ==\"Pressure\") %>%\n#                 pull(med_val),\n#               step = 1,\n#               round = TRUE),\n#       sliderInput(inputId = \"Visibility\",\n#               label = \"Visibility when accident happened\",\n#               min = stats_num %>%\n#                 filter(variable ==\"Visibility\") %>%\n#                 pull(min_val),\n#               max = stats_num %>%\n#                 filter(variable ==\"Visibility\") %>%\n#                 pull(max_val),\n#               value = stats_num %>%\n#                 filter(variable ==\"Visibility\") %>%\n#                 pull(med_val),\n#               step = 1,\n#               round = TRUE),\n#       sliderInput(inputId = \"Wind_Speed\",\n#               label = \"Wind speed when accident happened\",\n#               min = stats_num %>%\n#                 filter(variable ==\"Wind_Speed\") %>%\n#                 pull(min_val),\n#               max = stats_num %>%\n#                 filter(variable ==\"Wind_Speed\") %>%\n#                 pull(max_val),\n#               value = stats_num %>%\n#                 filter(variable ==\"Wind_Speed\") %>%\n#                 pull(med_val),\n#               step = 1,\n#               round = TRUE),\n#       sliderInput(inputId = \"Precipitation\",\n#               label = \"Precipitation when accident happened in inches\",\n#               min = stats_num %>%\n#                 filter(variable ==\"Precipitation\") %>%\n#                 pull(min_val),\n#               max = stats_num %>%\n#                 filter(variable ==\"Precipitation\") %>%\n#                 pull(max_val),\n#               value = stats_num %>%\n#                 filter(variable ==\"Precipitation\") %>%\n#                 pull(med_val),\n#               step = 1,\n#               round = TRUE),\n#       selectInput(inputId = \"Crossing\",\n#               label = \"Is there a crossing where the accident happened?\",\n#               choices = list(Yes = \"TRUE\",\n#                              No = \"FALSE\")),\n#       selectInput(inputId = \"Junction\",\n#               label = \"Is there a junction where the accident happened?\",\n#               choices = list(Yes = \"TRUE\",\n#                              No = \"FALSE\")),\n#       selectInput(inputId = \"Traffic_Signal\",\n#               label = \"Is there a traffic signal where the accident happened?\",\n#               choices = list(Yes = \"TRUE\",\n#                              No = \"FALSE\")),\n#       selectInput(inputId = \"Sunrise_Sunset\",\n#               label = \"Is it night or day?\",\n#               choices = list(Night = \"Night\",\n#                              Day = \"Day\")),\n#       selectInput(inputId = \"Civil_Twilight\",\n#               label = \"Is there enough natural light to be day?\",\n#               choices = list(Yes = \"Day\",\n#                              No = \"Night\")),\n#       selectInput(inputId = \"Nautical_Twilight\",\n#               label = \"Is it nautical day or night?\",\n#               choices = list(\"Day\",\"Night\")),\n#       selectInput(inputId = \"Astronomical_Twilight\",\n#               label = \"Was the sky illuminated by the sun?\",\n#               choices = list(Yes = \"Day\",\n#                              No = \"Night\")),\n#       selectInput(inputId = \"Weather_Condition\",\n#               label = \"Weather condition when accident happened\",\n#               choices = Weather),\n#       submitButton(text = \"Get the Prediction\"),\n#     ),\n#       mainPanel(\n#         verbatimTextOutput(\"Pred\")\n#       )\n#    )\n# ),\n# server = function (input,output) {\n#   output$Pred <- renderPrint({\n#     data <- tibble(\n#       # TMC=input$TMC,\n#       Month=input$Month,\n#       Hour=input$Hour,\n#       Wday=input$Wday,\n#       Duration=input$Duration,\n#       Start_Lat=input$Start_Lat,\n#       Start_Lng=input$Start_Lng,\n#       Distance=input$Distance,\n#       Side=input$Side,\n#       City=input$City,\n#       Temperature=input$Temperature,\n#       Wind_Chill=input$Wind_Chill,\n#       Humidity=input$Humidity,\n#       Pressure=input$Pressure,\n#       Visibility=input$Visibility,\n#       Wind_Speed=input$Wind_Speed,\n#       Precipitation=input$Precipitation,\n#       Crossing=input$Crossing,\n#       Junction=input$Junction,\n#       Traffic_Signal=input$Traffic_Signal,\n#       Sunrise_Sunset=input$Sunrise_Sunset,\n#       Civil_Twilight=input$Civil_Twilight,\n#       Nautical_Twilight=input$Nautical_Twilight,\n#       Astronomical_Twilight=input$Astronomical_Twilight,\n#       Weather_Condition=input$Weather_Condition\n#     )\n#     pred <-\n#       predict(traffic_mod,data) %>%\n#       pull(.pred_class)\n# \n#     pred}\n#   )\n# },\n# \n#   options = list(height = 500)\n# )\n\n\n\n\n\n\n",
    "preview": "posts/2021-05-07-accidentbehind/behind.jpg",
    "last_modified": "2021-05-07T20:49:46-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-05-07-accidentprediction/",
    "title": "Prediction of the severity of car accidents",
    "description": "Analyze the environmental factors that are more strongly associated with car accident in the United States to create a prediction model.",
    "author": [
      {
        "name": "Juthi Dewan, Coco Li, Franco Salinas",
        "url": {}
      }
    ],
    "date": "2021-05-07",
    "categories": [],
    "contents": "\n\n\n\n\n\n\nIntroduction\nThe Fatality Analysis Reporting System indicated that an estimate of 8870 people died in motor vehicle traffic crashes in the second quarter of 2020 (NHTSA, 2020). This analysis is intended to bring light to the main environmental conditions that are associated with the severity of a car accident. For the purpose of this study we defined severity as the accident’s impact on traffic.\nData\nThe data we used has 47 variables and 3 million observations for different car accidents. The data was collected from February 2016 to December 2020 for the 49 states of the US. The data base has been constructed partly by Moosavi, Sobhan, Mohammad Hossein Samavatian, Srinivasan Parthasarathy, and Rajiv Ramnath as “A Countrywide Traffic Accident Dataset” (2019). The other part of the data base was constructed by Moosavi, Sobhan, Mohammad Hossein Samavatian, Srinivasan Parthasarathy, Radu Teodorescu, and Rajiv Ramnath for their database “Accident Risk Prediction based on Heterogeneous Sparse Data: New Dataset and Insights.”\nModels\nUsing this data set, we predicted the Severity of an accident using stacked LASSO, Forest and classification three. Stacking combines predictions from many different models into a “super” predictor. In this case we would be averaging the predictions of the LASSO, Forest and classification three.\nPre-processing\n\n# A tibble: 3 x 2\n  `Variables to drop` `NA proportion`\n  <chr>                         <dbl>\n1 End_Lat                       0.636\n2 End_Lng                       0.636\n3 Number                        0.633\n\n\n\n\nWe chose not to use multiple of the 47 variables that we considered weren’t relevant to the analysis we were conducting. As we can see in the table, end latitude, end longitude’s proportion of NA values are higher than 50%. Given the distribution of wind direction through the different severity levels, we decided that the variable is uninformative. The description, bump, traffic calming, give way, no exit, railway, roundabout, station, stop, amenity, street, zip code, country, turning loop, county and TMC code weren’t informative either given the distribution between the severity categories or near-zero variance. We also decided to group the four levels of severity status into two categories. Given the large amount of levels for the weather condition variable I decided to use only the levels that had a count higher than 20.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlobal interpretation\nGlobal model interpretations explain the overall relationships between the predictor variables and the response.\nModel performance\n\n\n\n\nPreparation of a new explainer is initiated\n  -> model label       :  rf \n  -> data              :  49044  rows  24  cols \n  -> data              :  tibble converted into a data.frame \n  -> target variable   :  49044  values \n  -> predict function  :  yhat.workflow  will be used ( [33m default [39m )\n  -> predicted values  :  No value for predict function target column. ( [33m default [39m )\n  -> model_info        :  package tidymodels , ver. 0.1.2 , task classification ( [33m default [39m ) \n  -> predicted values  :  numerical, min =  0 , mean =  0.1959684 , max =  1  \n  -> residual function :  difference between y and yhat ( [33m default [39m )\n  -> residuals         :  numerical, min =  -0.8282602 , mean =  -0.001734642 , max =  0.9717742  \n [32m A new explainer has been created! [39m \nPreparation of a new explainer is initiated\n  -> model label       :  lasso \n  -> data              :  49044  rows  24  cols \n  -> data              :  tibble converted into a data.frame \n  -> target variable   :  49044  values \n  -> predict function  :  yhat.workflow  will be used ( [33m default [39m )\n  -> predicted values  :  No value for predict function target column. ( [33m default [39m )\n  -> model_info        :  package tidymodels , ver. 0.1.2 , task classification ( [33m default [39m ) \n  -> predicted values  :  numerical, min =  0.00227835 , mean =  0.1942335 , max =  0.9658464  \n  -> residual function :  difference between y and yhat ( [33m default [39m )\n  -> residuals         :  numerical, min =  -0.8613942 , mean =  2.375737e-07 , max =  0.9934433  \n [32m A new explainer has been created! [39m \nPreparation of a new explainer is initiated\n  -> model label       :  tree \n  -> data              :  49044  rows  24  cols \n  -> data              :  tibble converted into a data.frame \n  -> target variable   :  49044  values \n  -> predict function  :  yhat.workflow  will be used ( [33m default [39m )\n  -> predicted values  :  No value for predict function target column. ( [33m default [39m )\n  -> model_info        :  package tidymodels , ver. 0.1.2 , task classification ( [33m default [39m ) \n  -> predicted values  :  numerical, min =  0.1165841 , mean =  0.1942337 , max =  0.7279736  \n  -> residual function :  difference between y and yhat ( [33m default [39m )\n  -> residuals         :  numerical, min =  -0.7279736 , mean =  3.562719e-18 , max =  0.8834159  \n [32m A new explainer has been created! [39m \n\n\n\n\n\n\n\nAs we can see from the histograms, the majority of the residual values are clustered in values close to 0. The model with the lowest average residual value is lasso, followed by the tree model and the forest model. While the lasso and the forest models are left skewed this is not the case for the classification tree. The residuals for the classification tree are more spread towards the positive and the negative values, showing that this model might be less precise than the other two.\nVariable of importance\n\n\n\n\n\n\n\n\n\nWe can see that there are more variables with greater importance in the lasso and the forest models. This means that, the values that are at the top generate great increases in the performance of the model when permuted relative to the other variables. The length of the bars indicate how much the performance increases when that variable is permuted. Permuting is the process of exchanging the values of a variable between observations. We can see that the lasso and forest model’s performance increase when start longitude is permuted. This is the case for city in the classification tree.\nCeteris-Paribus Profile\nThis profiles show how one variable affects the outcome holding all other variables fixed for one observation.\n\n\n\nIn this graph we can see how changes to the values of the starting longitude affect the probability that the accident is Severe, holding all the other variables constant. As we can see, the probability that an accident is severe changes drastically whit different start longitude values, reflecting the importance of the variable for the forest model.\nPartial Dependence Plots\nRemember the CP profile used only one observation? A partial dependence plot is created by averaging the CP profiles for a sample of observations. The partial dependence profile is the blue line. We can see that overall, changes to the value of the longitude affect the probability that an accident is sever significantly. If we were to analyze the dependence plot for lasso the lines we would see inclined parallel lines given that lasso is additive. This is not very informative, that’s why we decided to focus on the forest model.\n\n\n\nLocal Model Interpretation\nLocal model interpretation helps us understand the impact of variables on individual observations. We will focus on the random forest model given that it is the model with the higher accuracy. We would like to do the interpretation for the stacked model, but this isn’t possible using DALEX and DALEXtra. Considering that out stacked model has few models stacked, we think that doing the analysis of the random forest model should give us a fair idea of what is the local importance of our variables.\nShapley Additive Explanations (SHAP)\nFor Break Down profiles, the contributions of variables would change with the order in which the variables are considered in the random forest model. Therefore we decided to use the SHAP.\n\n\n\nEach bar shows the average contribution of each variable’s value to the predicted severity for this observed accident. We can see that a duration of 44.62 contributes almost an additional 0.15 to the predicted probability of a severe accident, on average. The boxplot shows the variation across permutations of the order of the variable. A large variation would mean that we should be less confident in its exact effect. For example we see that Nautical twilight has a large variation; therefore, we aren’t confident about it’s contribution to the prediction.\nLocal Interpretable Model-agnostic Explanations (LIME)\n\n# A tibble: 1 x 3\n  model_r2 model_prediction prediction\n     <dbl>            <dbl>      <dbl>\n1    0.217            0.155      0.178\n\n\nThe table shows the predicted value from the local model as 0.2624304. and the prediction from the original forest model of 6.17 This graph shows us the predicted value from the original random forest model as “Prediction”. The r-squared value of the model is shown as the “Explanation fit” showing that the model explains 25% of the variance of our sample. The bars show that distance is the most important variable in the local model.\nConclusion:\nWe have explored how changes in the values of variables for an observation can affect the predicted outcome using two methods. Both methods show similar results; however, it’s important to note that while the SHAP plot includes more variables, it is less reliable. We have seen also that starting longitude is the most important variable according to two of our models, and that changing its value for an observation holding the other variables constant changes the predicted value significantly. Even with slight differences all of the models showed similar results for the variables of interest, showing that the results are coherent and reasonable. We can conclude that overall, distance, starting longitude, duration and month are the most important variables for the prediction of the severity of an accident. This means that depending on the month, the weather or the amount of traffic could be associated with the severity of an accident. The length of the road extent affected by the accident also is a good predictor for severity. The time between the start of the accident and the end of the impact on traffic flow also are associated with the severity. Lastly, there is a strong association between starting longitude of the accident and the severity which could be driven by demographic or other non-controlled characteristics that differentiate western to eastern states.\nRepercussions:\nOur analysis only focused on environmental factors that are associated with the severity of an accident. There are multiple other factors that we didn’t include like speed or whether the driver was intoxicated. Therefore, the model and the analysis is not complete and should not be regarded as such. On the other hand, this analysis didn’t account for demographics; therefore, the differences in locations do not respond to anyone’s ethnicity, education, sex or any other identity. This model is meant to further analyze what external factors contribute the most to the severity of an accident.\n\n\n\n",
    "preview": "posts/2021-05-07-accidentprediction/accident.png",
    "last_modified": "2021-05-07T12:38:37-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-19-buildingdistill/",
    "title": "Changes on the stock market during the COVID-19 pandemic",
    "description": "Analysis of the change in stock market prices, cumulative returns, and trading volumes during the COVID-19 pandemic for the biggest companies in each sector.",
    "author": [
      {
        "name": "Franco Salinas, Duc Ngo, Vichearith Meas, and Max Wang",
        "url": {}
      }
    ],
    "date": "2021-03-19",
    "categories": [],
    "contents": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction:\nWe have seen multiple changes around the world after one year. The main reason for that is because of the coronavirus pandemic. Firstly thought of as a common flu in Wuhan, China, the pandemic has spread around the world, with more than 100 million cases and more than 2.5 million deaths. Especially in the US, we have seen around 30 million cases with more than 500,000 deaths resulting from the pandemic (according to Worldometer) . Coronavirus has changed our perspective in multiple areas, however, in this blog post, we focus specifically on its impact on companies and sectors in the US stock market. We will see what are the changes in stock price, in market capitalization as well as how each sector has changed after the pandemic.\nFirst, as we have more than 2,800 companies trading in the New York Stock Exchange (NYSE), it is really difficult to do it in a short period of time. Instead, we used the website Yahoo Finance to find the data for the 3 companies with highest market cap in different 11 sectors: Communication Discretionary, Communication Services, Consumer Staples, Energy, Financials, Health Care, Industrials, Material, Real Estate, Technology, Utilities. Then, to compare the results from different sectors to the wider market, we used the SPY, which is an ETF for the S&P 500 that tracks the changes in the value of the biggest 500 companies in the market. We classified the SPY data as “All fields”. We tracked the data from 2020-01-02 to 2021-03-04, using the volume and the closing price. We assembled the data in a time series format creating dummy variables for each sector, then we chose to use the closing price given that it shows the final value that market participants attribute to a share after trading, providing a more realistic depiction of its value. Then, we calculated the Cumulative Rate of Return everyday by using the following formula:\n\\[\nreturn = \\frac{Price_i - Price_1}{Price_1}* 100\n\\]\nWe also obtained the market cap by multiplying the number of market shares of each company by the Closing price each day. We retrieved the number of shares from the SEC filing website.\nAfter we have found the way to conduct research with our data, we have created three main questions:\na. Which companies and which sectors have performed the best during the period? What is the proportion of negative and positive daily average return for each sector?\nb. What are the changes in market capitalization?\nc. Which companies have the highest trading volume? Which sectors have the highest number of trading?\nThe performance of companies and the sectors:\nReturn by sectors:\n\n\n\n\n\n\n\n\n\n\n\n\nOverall, it seems that the return for the Communication Discretionary is consistently growing, almost in a positively linear way. The lowest return is still not below 0. Changes in return of Communication Services and Technology follow a similar trend to that of Communication Discretionary in shape, but have a smaller magnitude (-20~ 60 for CS, -25~100 for T, compared to 0~300 for CD). Consumer Staples fluctuated mildly, falling sharply first to -10 and rebounding quickly, then growing steadily to finally decrease back to 0 return. The situation for the Energy and Financials sectors is the worst. Since the first day, Energy’s cumulative return is negative. Only the last month is positive for the Financials sector. Health Care, Industrials, Material, Real Estate, and Utilities followed a similar trend, but these sectors’ conditions are better than that of Energy and Financials. Health Care, Industrials, Material, Real Estate, and Utilities have similar trends compared to the overall market. In general, the return sharply dropped from 2020-1-1 to April 2020 and then rebounded at a lower rate, back to 0 in August 2020. After, it oscillated up and down until 2020 November and then continuously increased slowly. So far, return as a whole is 20%.\n\n\n\nAdditionally, we try to see the proportion of time in which the average of the cumulative return of the best stocks per sector have been over or under the base price. In here, it comes as no surprise as Communication Discretionary was the sector with the highest proportion of positive returns relative to the negative returns. This meaning, that 97% of the time the Communication Discretionary sector had positive returns relative to the initial price. Had a person bought shares of the companies in this sector on January 2nd, they would have slept peacefully 97% of the time. On the other hand, the Energy sector was the sector with the highest proportion of negative returns relative to positive returns. As we can see, the pandemic affected different sectors in a different way. The least affected sectors are Communication Discretionary and Technology, one of the reasons being that their consumption doesn’t require in person presence. In the case of energy, a drop in the demand of petroleum as a consequence of the lockdown in different countries decreased their prices. Given that oil companies’ revenue depends on price, the investors sold their shares expecting losses on the oil industry. The Financials sector was also drastically affected, as the FED lowered the interest rates and there were negative interest rates in Europe, understanding that banks are the suppliers of capital, lower (or negative) interest rates affect banks’ revenue.\nReturn by companies:\n\n\n\n\n\n\nWhen we go deeper into each company in the field, it will be Tesla that grows the most during the period. Tesla’s stock price has increased more than 9 times for the period, followed by Nvidia as the company tripled in value. After that, we can see Apple, Amazon, Microsoft or Facebook increased quite heavily during the period, all increasing by 100-150%. Given that the Energy and Financial sectors did not see an increase in the stock price, it comes as no surprise that we can’t see any companies in those sectors in the top ten list.\nWhen we look closer at the shorter period, in March or April, we can see Walmart (one of the companies in the Consumer Staples section) that was in top 10 companies with the greatest return. However, after this period, it was still companies in Communication Discretionary and Technology that have the highest return such as Tesla, Nvidia, Microsoft, Amazon or Apple.\nThe change in market capitalization for each sector and for the company:\n\n\n\n\n\n\nLooking at the distribution of market cap by sector during the Covid-19 period, it is seen clearly that Communication Discretionary and Technology have consistently been the top first and second largest sectors respectively with combined shares at more than 60%. The two sectors remain the leading sectors throughout the period from January 2020 to March 2021.\nConsumer Discretionary is the biggest sector which accounts for around 46% of the entire market cap. The highest percentage of this sector was 49.7888% in late October 2020. This increasing trend started in January and hit the peak in October 2020 before having a decreasing trend from October to March the next year. The next sector, Technology, also had an increasing percentage over time as the area in the graph increased. This percentage of market cap for this sector started from 19.85% in January 2020, at 21.48% in the 1st Quarter in April 2020 and at 24.17% in November 2020. The max for this sector was 26.84%.\n\n\n\n\n\n\n\n\n\nThe distribution of market cap by company graph shows the top 20 companies that have the greatest amount of market cap proportion. Apple (AAPL), Amazon (AMZN), and Microsoft (MSFT) are the top three companies that are from the top two sectors: Communication Discretionary and Technology. From this we can see how the two sectors are the dominant by looking which companies have the greatest market cap.\nThe change in trading volume for each sector.\n\n\n\nThe bar graph shows the sum of the monthly transactions for the three biggest companies in each sector. We can see that the sector with the highest volume is Technology, and the sector with the lowest volume is the Real Estate sector. The high volumes in the Technology sector potentially reflect that investors preferred large technology companies over the pandemic.\n\n\n\n\n\n\nThese companies have large market caps, and given that the 5 largest companies in the Technology sector account for a large fraction of the S&P 500, these “blue chip” companies are expected to have larger volumes. One of the reasons is their liquidity. They are very liquid because many people are trading them; therefore, you can find buyers and sellers quickly. As a result there are no price distortions as a consequence of delays in transactions. On the other hand, this could be just a continuation of past trends, given that the technology sector has been the most popular in the last years.\nAnother potential explanation is that Technology stocks are always more risky given the larger returns; therefore, there are major price swings that cause people to increase their transactions. Also, larger volumes reflect the strength of price changes during the pandemic. In the case of Real Estate, low volumes reflect the lack of strength in price changes, reflected on subtle changes in the cumulative returns graphed. As we can see in the graph for Real Estate companies, the trend is flatter and the changes are of a smaller magnitude, proving that there weren’t significant changes in return. Therefore,there weren’t many changes in the demand or supply of Real Estate stock.\nWe can also observe that in periods with low prices, the volume tends to increase. In this case, it was in March given the declaration of sanitary emergency and the first COVID-19 cases in the country. This is because many people attempt to sell their shares quickly while other investors seek to buy shares at lower prices.\nConclusion:\nThe following post has shown some of the key changes within the period. It has shown how each sector has changed and how each company reacts to the pandemic. Some companies thrived, like Tesla, however, some took lots of time to recover, especially in the Energy and Finance sector. For the next steps, we could try and improve our information by scraping and getting all of the data in every company to illustrate and show the best picture for these sectors. This will be the improvement that we might need to do to see the wider picture of the market.\nOverall, our analysis has shown the changes in stock prices, volume, market capitalization before and after the pandemic. The pandemic and its consequences on the economy make it difficult to predict which companies will thrive and succeed in the future. However, we hope that some of our findings will shed light on some information about the general market trends observed when dealing with a global pandemic.\n\n\n\n",
    "preview": "posts/2021-03-19-buildingdistill/final_distill_foto.png",
    "last_modified": "2021-03-20T13:59:24-05:00",
    "input_file": {}
  }
]
